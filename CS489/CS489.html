<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <title>CS489 | Anthony Zhang</title>
  <link rel="stylesheet" href="../css/base.css" type="text/css">
  <link rel="stylesheet" href="../css/note.css" type="text/css">
  <link rel="stylesheet" href="../highlight/styles/default.css">
  <link rel="stylesheet" href="../highlight/styles/paraiso-light.css">
  <script src="../highlight/highlight.pack.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>
  <style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
  </style>
  <script src="../katex/katex.min.js" type="text/javascript"></script>
  <link rel="stylesheet" href="../katex/katex.min.css" />
  <script type="text/javascript">
  window.onload = function() {
    document.getElementsByClassName("status-banner")[0].style.display = "block";
    setTimeout(function() {
      renderMathElements(document.getElementsByClassName("math"));
      document.getElementsByClassName("status-banner")[0].style.display = "none";
    }, 50); // delay to allow status banner to show
  }

  function renderMathElements(mathElements) {
    var mathOptions = {
      macros: {
        "\\set": "\\left\\{ #1 \\right\\}",
        "\\tup": "\\left\\langle #1 \\right\\rangle",
        "\\abs": "\\left\\lvert #1 \\right\\rvert",
        "\\floor": "\\left\\lfloor #1 \\right\\rfloor",
        "\\ceil": "\\left\\lceil#1 \\right\\rceil",
        "\\mb": "\\mathbb{#1}",
        "\\rem": "\\operatorname{rem}",
        "\\ord": "\\operatorname{ord}",
        "\\sign": "\\operatorname{sign}",
        "\\imag": "\\bm{i}",
        "\\dee": "\\mathop{}\\!\\mathrm{d}",
        "\\lH": "\\overset{\\text{l'H}}{=}",
        "\\evalat": "\\left.\\left(#1\\right)\\right|",
        "\\sech": "\\operatorname{sech}",
        "\\spn": "\\operatorname{Span}",
        "\\proj": "\\operatorname{proj}",
        "\\prp": "\\operatorname{perp}",
        "\\refl": "\\operatorname{refl}",
        "\\magn": "\\left\\lVert #1 \\right\\rVert",
        "\\rank": "\\operatorname{rank}",
        "\\sys": "\\left[ #1 \\mid #2\\space \\right]",
        "\\range": "\\operatorname{Range}",
        "\\adj": "\\operatorname{adj}",
        "\\cof": "\\operatorname{cof}",
        "\\coord": "{\\left\\lbrack #1 \\right\\rbrack}_{#2}",
        "\\diag": "\\operatorname{diag}",
        "\\formlp": "\\operatorname{Form}(\\mathcal{L}^P)",
        "\\argmin": "\operatorname{argmin}}",
        "\\argmax": "\operatorname{argmax}}",
        "\\sgn": "\operatorname{sgn}}",

        // not yet available in KaTeX
        "\\operatorname": "\\mathop{\\text{#1}}\\nolimits", //wip: spacing is slightly off
        "\\not": "\\rlap{\\kern{7.5mu}/}", //wip: slash angle is slightly off
        "\\bm": "\\mathbf", //wip: should be italic, but isn't
      },
      throwOnError: false,
    };
    for (var i=0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      katex.render(texText.data, mathElements[i], mathOptions);
    }
  }
  </script>
</head>
<body>
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-68271407-1', 'auto');
    ga('send', 'pageview');

  </script>
  <h1>Lecture Notes by <a href="/">Anthony Zhang</a>.</h1>
  <ul class="site_links">
    <li><a href="/blog/" class="page">blog</a></li>
    <span class="divider"></span>
    <li><a href="http://uberi.github.io/University-Notes" class="page">notes</a></li>
    <span class="divider"></span>
    <li><a href="/resume.pdf" class="page">résumé</a></li>
    <span class="divider"></span>
    <li><a href="https://github.com/Uberi" class="contact">github</a></li>
    <span class="divider"></span>
    <li><a href="https://www.linkedin.com/in/uberi/" class="contact">linkedin</a></li>
    <span class="divider"></span>
    <li><a href="mailto:me@anthonyz.ca" class="contact">email</a></li>
    <span class="divider"></span>
    <li><a href="https://www.facebook.com/anthony.zhang.user" class="contact">facebook</a></li>
    <span class="divider"></span>
    <li><a href="https://twitter.com/anthony926535" class="contact">twitter</a></li>
    <span class="divider"></span>
    <li><a href="https://keybase.io/uberi" class="info">public key</a></li>
  </ul>
<h1 id="cs489">CS489</h1>
<p>Special Topics in Computer Science - Introduction to Machine Learning.</p>
<pre><code>Yaoliang Yu
Section 001
Email: yaoliang.yu@uwaterloo.ca
Website: http://cs.uwaterloo.ca/~y328yu/mycourses/489
Office Hours: Tuesdays/Thursdays 2:40pm-3:40pm in DC-3617
Tuesdays/Thursdays 4:00pm-5:20pm</code></pre>
<h1 id="section">7/9/17</h1>
<p>Course is potentially going to be collaboration between professor and Focal Systems - guest lectures by deep learning engineers from Focal and assignments from real-world systems. Contact agastya@focal.systems for questions and comments.</p>
<p>Course questions on Piazza, content on LEARN. Course will use MATLAB, Python, or Julia, and requires CS341 concepts.</p>
<p>No required textbooks. 5 semi-weekly assignments worth 50% total, submittable via LEARN, remaining 50% from an open book final exam. There's also a 5% bonus project, consisting of a 1 page proposal and 8 page report on a machine-learning-related project.</p>
<p>Machine learning is about giving computers the ability to learn things that they aren't explicitly programmed to do. More specifically, for a machine to <strong>learn</strong> is to, as experience <span class="math inline">E</span> increases, improve a performance measure <span class="math inline">P</span> for some class of tasks <span class="math inline">T</span>. Essentially, a program learns if it <strong>gets better at solving a problem as it gains more experience</strong>.</p>
<p>Machine learning falls into three categories:</p>
<ul>
<li>Supervised learning: classification/regression/ranking - there's a source of truth that the machine can use to determine the true answer for at least some of the problem instances.
<ul>
<li>Example: the Not Hotdog app - images labelled hotdogs/not-hotdogs are used to train the model, which is then used to make predictions about new images.</li>
<li>Given a training set of pairs <span class="math inline">\tup{x, y}</span>, find a function <span class="math inline">f: X \to Y</span> such that <span class="math inline">f(x)</span> has good performance on values of <span class="math inline">x</span> that haven't been seen in the training set.</li>
<li>We don't actually care that much about performance in the training set - too-high performance in the training set is overfitting</li>
</ul></li>
<li>Reinforcement learning: control/pricing/gaming - there's no explicit source of truth, but doing something gives feedback, like how good the previous output was.
<ul>
<li>Example: AlphaGo uses a reinforcement learning model to guide monte-carlo tree search - wins give positive feedback, losses give negative feedback.</li>
</ul></li>
<li>Unsupervised learning: clustering - there's no explicit source of truth.
<ul>
<li>Example: Google Youtube clustering 9-layer network from 2012 was trained to cluster objects, and managed to learn to detect faces by itself.</li>
</ul></li>
</ul>
<p>Modern ML research focuses on representation of data (e.g., feature engineering), interpretation of results, generalizing models to different domains (e.g., applying image classifiers to video), time/space complexity, learning efficiency (how many samples do we need? how big does the training set need to be?), and real-world applications.</p>
<p>New notation: <span class="math inline">A_i</span> is the <span class="math inline">i</span>-th 1-indexed row of the matrix <span class="math inline">A</span>, and <span class="math inline">A_{:j}</span> is the <span class="math inline">j</span>-th 1-indexed column of the matrix <span class="math inline">A</span>.</p>
<p>New notation: <span class="math inline">\sign x = \begin{cases} 1 &amp;\text{if } x &gt; 0 \\ -1 &amp;\text{if } x &lt; 0 \\ \text{undefined} &amp;\text{if } x = 0 \end{cases}</span>.</p>
<p>New notation: derivative of a function is <span class="math inline">Df(x) = \lim_{\delta \to 0} \frac{f(x + \delta) - f(x)}{\delta}</span>.</p>
<p>New notation: <span class="math inline">\min_{a: f(a), b: g(b), \ldots} f(a, b, c, \ldots)</span> is the minimum value of <span class="math inline">f(a, b, c, \ldots)</span> such that <span class="math inline">f(a), g(b), \ldots</span> are all true. The <span class="math inline">a: f(a)</span> part might also be written as just <span class="math inline">a</span> if there's no constraints.</p>
<p>New notation: <span class="math inline">\argmin_{a: f(a), b: g(b), \ldots} f(a, b, c, \ldots)</span> is the values of <span class="math inline">a, b, c, \ldots</span> such that <span class="math inline">f(a, b, c, \ldots)</span> is minimised and <span class="math inline">f(a), g(b), \ldots</span> are all true. The <span class="math inline">a: f(a)</span> part might also be written as just <span class="math inline">a</span> if there's no constraints.</p>
<h1 id="section-1">12/9/17</h1>
<p>Consider the problem of filtering out spam emails. The training set would be a set <span class="math inline">X</span> of emails (e.g., a vector where each dimension represents a feature, like whether word <span class="math inline">i</span> appears in the email) and a set <span class="math inline">Y</span> representing the spamminess of those emails (e.g., real number between -1 and 1). One of the most important parts of this task is making sure we have a good representation for features in our emails. In a bag of words model, for example, we might make <span class="math inline">X</span> a 10000-dimensional vector where each element represents whether one of 10000 words appears in the email's subject.</p>
<p>In <strong>batch learning</strong>, we care about performance on the testing set <span class="math inline">X&#39;</span>, and the training set is just the means by which we get there, by performing statistics on <span class="math inline">X</span> and assuming things about <span class="math inline">X&#39;</span>. In <strong>online learning</strong>, data is received in a streaming fashion - we need to product the value of <span class="math inline">y</span> without knowing its true value.</p>
<p>In this course, we'll use <span class="math inline">&lt;a, b&gt;</span> to represent the inner product <span class="math inline">a \cdot b = a^T b</span>. Also, <span class="math inline">\sign(x)</span> is 1 when <span class="math inline">x &gt; 0</span>, -1 when <span class="math inline">x &lt; 0</span>, and undefined when <span class="math inline">x = 0</span> (some other courses will instead define it to be 0).</p>
<h2 id="perceptrons">Perceptrons</h2>
<p>The perceptron is a machine learning model based on a highly simplified model of a neuron. It takes in activation from neighboring neurons, takes their weighted sum, and then applies the activation function to them, the <span class="math inline">\sign</span> function, which is the neuron's output. We'll study Rosenblatt's original design from 1958, along with several additions and improvements made since then.</p>
<p>Perceptrons are used for <strong>binary classification problems</strong>. We are given a training set <span class="math inline">\set{\tup{\vec x_1, y_1}, \tup{\vec x_2, y_2}, \ldots}</span> and a testing set <span class="math inline">\set{\vec t_1, \vec t_2, \ldots}</span> where <span class="math inline">\vec x_i, \vec t_i</span> are feature vectors, and <span class="math inline">y_i</span> is the binary <strong>category</strong>, either -1 or 1. Using the training set, we want to train the perceptron to determine the category <span class="math inline">y_i</span> for each <span class="math inline">\vec t_i</span>.</p>
<p>A perceptron is simply <span class="math inline">y = \sign(\vec w \cdot \vec x + b)</span>, where <span class="math inline">\vec w</span> is the perceptron's <strong>weights vector</strong>, <span class="math inline">b</span> is the perceptron's <strong>bias</strong>, <span class="math inline">\vec x</span> is the <strong>input</strong>, and <span class="math inline">y \in \set{-1, 1}</span> is the <strong>prediction</strong>. Note that <span class="math inline">\vec w + b</span> should be a <strong>hyperplane</strong> separating the positive values of <span class="math inline">y_i</span> from the negative values of <span class="math inline">y_i</span>, and the sign of <span class="math inline">\vec w \cdot \vec x + b</span> determines which side of the hyperplace the point <span class="math inline">\vec x</span> is on (the positive predictions side, or the negative predictions side). For now, let's assume that for the training set, there exists a hyperplane that separates all of the positives from all of the negatives - that the data is <strong>separable</strong>.</p>
<p>Now we'll try to simplify the perceptron formula to make it easier to work with. First, let's get rid of the <span class="math inline">\sign</span> by multiplying both sides of the perceptron formula by <span class="math inline">y</span>: <span class="math inline">y^2 = y \sign(\vec w \cdot \vec x + b)</span>, and since <span class="math inline">y</span> is either -1 or 1, <span class="math inline">y^2 = 1</span>, so <span class="math inline">y \sign(\vec w \cdot \vec x + b) = 1</span>, or in other words, <span class="math inline">y (\vec w \cdot \vec x + b) &gt; 0</span>. Expand to get <span class="math inline">\vec w \cdot (y\vec x) + by &gt; 0</span></p>
<p>Let <span class="math inline">\vec w&#39; = \begin{bmatrix} \vec w \\ b \end{bmatrix}</span> and <span class="math inline">a = \begin{bmatrix} y \vec x \\ y \end{bmatrix}</span> - we've chosen these definitions specifically so that <span class="math inline">\vec w \cdot (y\vec x) + by &gt; 0</span> is equivalent to <span class="math inline">a \cdot w&#39; &gt; 0</span>, and so that the value of <span class="math inline">\vec w&#39;</span> represents the perceptron parameters exactly.</p>
<p>When training the perceptron, our goal is to fit the hyperplane to our training set. That means we'll want to make perceptron predictions in bulk, so it would be nice to be able to represent that in a compact way. To do this, we'll let <span class="math inline">A = \begin{bmatrix} \vec a_1 &amp; \vec a_2 &amp; \ldots \end{bmatrix}</span>, where <span class="math inline">\vec a_i = \begin{bmatrix} y_i \vec x_i \\ y_i \end{bmatrix}</span> - columns of <span class="math inline">A</span> are values of <span class="math inline">\vec a</span> corresponding to each value of <span class="math inline">\vec x_i</span>. Written out fully, that's <span class="math inline">A = \begin{bmatrix} y_1 \vec x_1 &amp; y_2 \vec x_2 &amp; \ldots \\ y_1 &amp; y_2 &amp; \ldots \end{bmatrix}</span>.</p>
<p>Clearly, <span class="math inline">A^T \vec w&#39; &gt; 0</span> is equivalent to <span class="math inline">\forall \vec x_i, \sign(\vec w&#39; \cdot \vec x_i + b) = y</span>. We've now simplified the perceptron problem down to a single matrix multiplication and a comparison! Now, <span class="math inline">\vec w&#39;</span> contains all the perceptron parameters, and the columns of <span class="math inline">A</span> are the data points (each with a trailing 1 element), premultiplied by the label.</p>
<p>Now the problem becomes: given premultiplied data in a matrix <span class="math inline">A</span>, find <span class="math inline">\vec w&#39;</span> such that <span class="math inline">A^T \vec w&#39; &gt; 0</span>. The <strong>perceptron training algorithm</strong> does this, and works as follows: repeatedly choose a column <span class="math inline">\vec a_i</span> of <span class="math inline">A</span>, and if <span class="math inline">\vec a_i \cdot \vec w&#39; \le 0</span>, change <span class="math inline">\vec w&#39;</span> by adding <span class="math inline">\vec a_i</span> to it. Stop when <span class="math inline">\vec a_i \cdot \vec w&#39; &gt; 0</span> for all <span class="math inline">\vec a_i</span> in <span class="math inline">A</span>, or when we reach an iteration/passes limit.</p>
<p>Why do we correct the weights when <span class="math inline">\vec a_i \cdot \vec w&#39; \le 0</span> by adding <span class="math inline">\vec a_i</span> to <span class="math inline">\vec w&#39;</span>? Well, the next time we choose the <span class="math inline">\vec a_i</span> column, we'll get <span class="math inline">\vec a_i \cdot (\vec w&#39; + \vec a_i) = \vec a_i \cdot \vec w&#39; + \magn{\vec a_i}^2</span>. Since <span class="math inline">\magn{\vec a_i}^2 &gt; 0</span>, <span class="math inline">\vec a_i \cdot (\vec w&#39; + \vec a_i) &gt; \vec a_i \cdot \vec w&#39;</span>, so <span class="math inline">\vec a_i \cdot (\vec w&#39; + \vec a_i)</span> is closer to being positive.</p>
<p>(Python implementation not included, since this is a question in assignment 1)</p>
<p>After training, we can make predictions for any given input <span class="math inline">\vec x</span> with the usual formula, <span class="math inline">y = \sign\left(\vec w&#39; \cdot \begin{bmatrix} \vec x \\ 1 \end{bmatrix}\right)</span>.</p>
<p>This algorithm is very simple to implement, yet works quite well in practice. Also, the fact that its formula is a linear combination is interesting. If we look at the weights, we notice that large positive weights mean that the corresponding feature strongly suggests that the prediction should be positive, whereas large negative weights strongly suggest that the prediction should be negative.</p>
<p>How well does a perceptron converge when running the training algorithm described above? <strong>Block's perceptron convergence theorem</strong> gives us an idea. If <span class="math inline">A</span> is separable (i.e., a hyperplane exists that separates positive cateogry points from negative category points), then <span class="math inline">\vec w&#39;</span> will converge to some <span class="math inline">\vec w^*</span>. If every column of <span class="math inline">A</span> is selected indefinitely often, then <span class="math inline">A^T \vec w^* &gt; 0</span>. Furthermore, if <span class="math inline">\vec w&#39; = \vec 0</span> initially, then the perceptron converges after at most <span class="math inline">(R / \gamma)^2</span> iterations, where <span class="math inline">R = \max\set{\magn{a_1}, \magn{a_2}, \ldots}</span> and <span class="math inline">\gamma = \max\set{\min\set{\vec w \cdot \vec a_1, \vec w \cdot \vec a_2, \ldots} : \magn{\vec w} \le 1}</span> (the margin - the minimum distance between the convex hull of the positive points and the negative points). Essentially, the margin represents the distance between the &quot;hardest&quot; two datapoints to classify.</p>
<p>Note that these values of <span class="math inline">R</span> and <span class="math inline">\gamma</span> are purely functions of the dataset, and that they don't directly depend on the size of <span class="math inline">A</span> and the number of dimensions <span class="math inline">d</span>. In other words, the number of mistakes the perceptron makes would be independent of the dataset size and number of dimensions! The larger the margin is, the faster the perceptron converges. Block's perceptron convergence theorem gives us a worst case bound, but in many practical situations the perceptron will perform a lot better.</p>
<p>Also, the perceptron stops at an arbitrary linear separator that correctly separates the points, not necessarily the one that most cleanly separates the positive and negative points (with the largest possible minimum distance from the hyperplane to positive/negative predictions). In fact, the resulting hyperplane will even depend on the order we feed in the data. We can use support vector machines instead to find that hyperplane (which also happens to be unique for each dataset!). This is the main disadvantage of perceptrons - they might only barely separate the training data, so they're less robust to unseen data than those that find a linear separator with a larger margin.</p>
<p>If the data is <strong>not separable</strong>, Block's perceptron convergence theorem doesn't apply anymore. The <strong>perceptron boundedness theorem</strong> says that convergence is only guaranteed if such a hyperplane exists, but if it doesn't, then the iterations are still bounded, because the perceptron's state will start cycling after a certain number of iterations. In practice, this means we would specify a time or iteration limit when doing training, or when the training/validation error stops changing, or even if weights stop changing much when using diminishing step sizes.</p>
<p>If we end up with non-separable data, we might want to find a better feature representation, use a deeper model, or use a <strong>soft margin</strong> - instead of a hyperplane that perfectly separates positive/negative values of <span class="math inline">y</span>, we can allow a few mistakes.</p>
<p>There are many ways to extend perceptrons to classify things into more than two categories (positive/negative). One way is <strong>one vs. all</strong>, where we have one perceptron per category, perceptron with highest activation level wins - <span class="math inline">\max_c(w_c \cdot x)</span>. The issue with this is that it's imbalanced - each perceptron has to give negative predictions far more often than positive ones, since it only gives positive prediction for its own category and otherwise must give a negative prediction. Another is <strong>one vs. one</strong>, where we have one perceptron for every pair of categories, where a positive prediction means the datapoint is in the first category and negative means the other category, and then take a vote to find the most commonly predicted category as the final answer.</p>
<p>An example of applying perceptrons online is pricing - selling a product to the user at a price <span class="math inline">y</span>, and updating weights if the price is too high and the user doesn't buy the product.</p>
<h1 id="section-2">14/9/17</h1>
<p>Assignment 1 now available, due in two weeks.</p>
<p>A <strong>pass</strong> is a run through all of the training data - 100 passes means we go through the training data 100 times. An <strong>iteration</strong> is a run through a single data point in our training data.</p>
<h2 id="linear-regression">Linear Regression</h2>
<p>Consider a scatter plot of house market value vs. square footage. We'd expect that these two are pretty well correlated. A linear regression over these two variables can be used to give us a line of best fit.</p>
<p>Regression problems are about fitting models to match datasets as closely as possible. Linear regression problems try to fit linear models to datasets. When we're doing regression problems, we have to consider whether to use linear/nonlinear models, and whether we'll be using it to interpolate or extrapolate (choosing the perfect model is much more important for extrapolation)</p>
<p>Formally, a regression problem is: find <span class="math inline">f(\vec x) \approxeq \vec y</span> given <span class="math inline">\vec x</span> (the <strong>feature vector</strong> a real vector) and <span class="math inline">y</span> (the <strong>response value</strong>, a real number). The hard part of this is that <span class="math inline">\vec x</span> and <span class="math inline">y</span> are drawn from unknown distributions, which makes it hard to interpolate/extrapolate. Additionally, we need a way to express how much error there is in our model predictions - a <strong>loss function</strong>.</p>
<p>One family of regression algorithms is <strong>risk minimizers</strong> (expected loss minimizers): algorithms that try to find <span class="math inline">f</span> such that <span class="math inline">\min_{f: \vec x \to y} E[\ell(f(\vec x), y)]</span>.</p>
<p>A common loss function is <strong>least squares</strong>: <span class="math inline">\min_{f: \vec x \to y} E[\magn{f(\vec x) - y}^2]</span>. The correct loss function for a given situation is often hard to determine, so we use one that's simple and efficient to compute - least squares works well enough for most situations. Additionally, of all the minimizers of $_W _F, <span class="math inline">W = A^+ CB^+</span>$ is the one with the smallest F-norm, where <span class="math inline">A^+</span> is the pseudo-inverse of <span class="math inline">A</span> (Sondermann '86, Yu &amp; Shuurmans '11) - this is mostly a theoretical result, but gives us another good reason to use least squares loss.</p>
<p>Clearly, <span class="math inline">E[\magn{f(\vec x) - y}^2] = E[\magn{f(\vec x) - E(y \mid \vec x)}^2] + E[\magn{E(y \mid \vec x) - y}^2]</span>. Note that the second term doesn't really depend on <span class="math inline">f</span> - it's the <strong>inherent noise variance</strong>, the noise that we can't get rid of no matter how good our regression function is. Also, the first term gives us the problem in a much simpler form: we want to find an <span class="math inline">f(\vec x)</span> that approximates <span class="math inline">E(y \mid \vec x)</span> well, to make this term smaller.</p>
<p>One way to make this optimization process easier is to assume that <span class="math inline">f(\vec x)</span> is linear, so <span class="math inline">f(\vec x) = E(\vec y \mid \vec x) = A \vec x + \vec b</span> for some matrix <span class="math inline">A</span>. If we make this assumption, then with risk minimization we're trying to find <span class="math inline">\min_{f: \vec x \to \vec y} E[A \vec x + \vec b - \vec y]</span>. We can't minimize this directly because we don't know the true distribution of the variables, but using the law of large numbers, <span class="math inline">\frac 1 n \sum Z_i = E(Z)</span> for any <span class="math inline">Z = \set{Z_1, Z_2, \ldots}</span>. So if we assume the model is linear, and the sample is large, then the risk minimization can be approximated by <span class="math inline">\min_{\vec a, \vec b} \frac 1 n \sum \magn{A \vec x + \vec b - \vec y}^2</span> (this approximation is called the <strong>empirical risk</strong>).</p>
<p>Let's simplify the <span class="math inline">\min_{\vec a, \vec b} \frac 1 n \sum \magn{A \vec x + \vec b - \vec y}^2</span> approximation, using something very similar to what we did for perceptrons. First, let's define <span class="math inline">W = \begin{bmatrix} A^T \\ {\vec b}^T \end{bmatrix}</span> and <span class="math inline">\vec x&#39; = \begin{bmatrix} \vec x \\ 1 \end{bmatrix}</span>. Now we have <span class="math inline">\min_W \frac 1 n \sum \magn{W^T \vec x&#39; - \vec y}^2</span>, which is slightly shorter/cleaner.</p>
<p>Let <span class="math inline">\vec x_i</span> be the <span class="math inline">i</span>th value of <span class="math inline">\vec x</span> in our training set. Just like for the perceptrons simplifications above, we also want to include all of the training set data points in a single expression, to make our minimization problem simpler. To do this, let <span class="math inline">X = \begin{bmatrix} {\vec x_1&#39;}^T \\ {\vec x_2&#39;}^T \\ \vdots \end{bmatrix}, Y = \begin{bmatrix} {\vec y_1}^T \\ {\vec y_2}^T \\ \vdots \end{bmatrix}</span>. Now, we can write this as <span class="math inline">\min_W \magn{XW - Y}_F^2</span> where <span class="math inline">\magn{A}_F = \sum_{i, j} A_ij</span> is the <strong>Frobenius norm</strong> - each element simply gets squared and the squares are all summed together to get the result, like the Euclidean norm, but extended for any matrix.</p>
<p>The <strong>least squares problem</strong> is now writeable as <span class="math inline">\min_W \magn{XW - Y}_F^2</span>, and we're minimizing the <strong>sum of square residuals</strong> <span class="math inline">XW - Y</span> (sum of square distances between the predicted values and true values). Here, <span class="math inline">Y</span> is a matrix with columns as the true responses, and the residuals are the distances between each true response in <span class="math inline">Y</span> and the point that the hyperplane would predict given <span class="math inline">X</span>.</p>
<p>Note that the Frobenius norm can be defined as: <span class="math inline">\magn{A}_F^2 = \trace{A^T A}</span>. Additionally, the following are identities: <span class="math inline">\trace(A + B) = \trace(A) + \trace(B)</span>, <span class="math inline">\trace(AB) = \trace(BA)</span>, <span class="math inline">\trace(A) = \trace(A^T)</span>, and <span class="math inline">\trace(cA) = c \trace(A)</span>.</p>
<p>Therefore, <span class="math inline">\magn{XW - Y}_F^2 = \trace((XW - Y)^T (XW - Y)) = \trace((W^T X^T - Y^T) (XW - Y)) = \trace(W^T X^T X W - Y^T X W - W^T X^T Y + Y^T Y) = \trace(W^T X^T X W) - \trace((Y^T X W)^T) - \trace(W^T X^T Y) + \trace(Y^T Y) = \trace(W^T X^T X W) - \trace(W^T X^T Y) - \trace(W^T X^T Y) + \trace(Y^T Y) = \trace(W^T X^T X W - 2 W^T X^T Y + Y^T Y)</span>. Clearly, this is a quadratic equation with respect to <span class="math inline">W</span>, and we want to find its minimum.</p>
<p>Consider <span class="math inline">\min_x f(x)</span>. Fermat's theorem says that at the minimum <span class="math inline">x</span>, the derivative of <span class="math inline">f(x)</span> must be 0. Consider a general quadratic function <span class="math inline">f(x) = \vec x^T A \vec x + \vec x^T \vec b + c</span>. The derivative is then <span class="math inline">\frac{\dee f(x)}{\dee x} = (A + A^T)\vec x + \vec b</span>.</p>
<p>Note that <span class="math inline">\magn{XW - Y}_F^2 = W^T(X^T X) W - 2W^T X^T Y + Y^T Y</span> (a quadratic equation), and if set the derivative of this to 0 and solve we get <span class="math inline">X^T X W = X^T Y</span> as a solution, which is just a linear system - we have <span class="math inline">X</span> and <span class="math inline">Y</span>, so we can solve for <span class="math inline">W</span>. Note that <span class="math inline">X^T X</span> might be invertible, but we should still never solve for <span class="math inline">W</span> by using <span class="math inline">W = (X^T X)^{-1} X^T Y</span>, since this involves solving <span class="math inline">n</span> linear systems, whereas we can solve it by solving only 1 linear system (in practice, we should almost never actually compute matrix inverses).</p>
<p>Once we have <span class="math inline">W</span>, we can make predictions for any given <span class="math inline">X</span> using <span class="math inline">\hat Y = XW</span>, or evaluate those predictions with <span class="math inline">(Y - \hat Y)^2</span>. We can also evaluate using a different loss function, a technique often used in calibration theory.</p>
<p>Linear regression is disproportionally affected by large outliers. To mitigate this, we sometimes use Huber loss, which is linear for large differences and quadratic for smaller ones, where &quot;larger&quot; and &quot;smaller&quot; are defined by a threshold <span class="math inline">\delta</span>. This ensures overly large outliers don't impact the result too much. Huber's loss function is defined as <span class="math inline">H(\hat y, y) = \begin{cases} \frac 1 2 (\hat y - y)^2 &amp;\text{if } \abs{\hat y - y} \le \delta \\ \delta(\abs{\hat y - y} - \frac{\delta}{2}) &amp;\text{otherwise} \end{cases}</span>.</p>
<p>When we're performing training, the end result is a model. Often, we want the simplest possible model that still makes good predictions, because these tend to generalize the best (Occam's Razor). Regularization techniques are used to make training prefer simpler models over slightly more complex but better models, where &quot;simpler&quot; is defined based on the technique.</p>
<p>Regularization also allows us to solve <strong>ill-posed problems</strong> - those that don't have exactly one solution (zero or more than one solution) or don't have their solutions change continuously with respect to the problem initial conditions (i.e., derivative of solution with respect to initial condition doesn't always exist).</p>
<p>Suppose we have an ill-posed linear regression problem <span class="math inline">\min_W \magn{XW - Y}_F^2</span>, so we might have many solutions to <span class="math inline">W</span>. One thing we could say is that linear regressions that use &quot;smaller&quot; values of <span class="math inline">W</span> are &quot;simpler&quot; than those that use &quot;larger&quot; values. To represent this, we just add a term to the least squares problem: <span class="math inline">\min_W \magn{XW - Y}_F^2 + \lambda \magn{W}_F^2</span>, or equivlaently, <span class="math inline">(X^T X + \lambda I)W = X^T Y</span>. A small positive lambda ensures that instead of a small change in the input resulting in a huge difference in the output, it would result in a difference proportional to <span class="math inline">\frac{1}{\lambda}</span> instead. Adding the <span class="math inline">\lambda I</span> value is a technique known as <strong>Tikhonov regularization</strong>, and the resulting modified least squares problem is known as the <strong>ridge regression problem</strong>.</p>
<p>The hyperparameter <span class="math inline">\lambda</span> affects how much we penalize higher weights in the resulting model - higher values force the resulting model to have a smaller Frobenius norm, and smaller values allow us it to &quot;trade off&quot; more weight in return for closer predictions. How do we choose hyperparameters like <span class="math inline">\lambda</span>? We have a training set (for model training), testing set (which we don't see until the end), and sometimes a small validation set (for tuning parameters), and on the training set, we can apply <span class="math inline">n</span>-fold <strong>cross-validation</strong>:</p>
<ol type="1">
<li>Suppose we have <span class="math inline">k</span> different values of <span class="math inline">\lambda</span> we want to consider.</li>
<li>Split the training set into <span class="math inline">n</span> roughly-equal sized chunks.</li>
<li>For each value of <span class="math inline">\lambda</span> we want to cross-validate:
<ol type="1">
<li>For each chunk <span class="math inline">i</span>:
<ol type="1">
<li>Train the model on the dataset formed by combining the <span class="math inline">n - 1</span> chunks that are not chunk <span class="math inline">i</span>.</li>
<li>Evaluate the model against the training data in chunk <span class="math inline">i</span>.</li>
</ol></li>
<li>Average the evaluation scores from each chunk to get the average cross-validation score for the value of <span class="math inline">\lambda</span>.</li>
</ol></li>
<li>Pick the value of <span class="math inline">\lambda</span> that has the best average cross-validation score.</li>
</ol>
<h1 id="section-3">19/9/17</h1>
<p>Guest lecture by Francois from Focal Systems (francois@focal.systems).</p>
<p>Almost any ML problem falls into regression or classification.</p>
<p>For linear regression, we're assuming that the response variable <span class="math inline">y</span> is approximated by <span class="math inline">\vec \theta \cdot \vec x + N(0, \sigma)</span>, where <span class="math inline">N(0, \sigma)</span> is a normal distribution centered around 0 with standard deviation <span class="math inline">\sigma</span>. Further overview of some real-world details in implementing linear regression.</p>
<p>Though linear regression is simplistic, it turns out that it works very well in practice, since more complex models require more advanced ways to do regularization and get decent weight vectors. Tools like SVM are used a lot in the real world to model real phenomena, even when they aren't necessarily linear, because it works well enough for most purposes.</p>
<p>Most modern ML problems use SGD - stochastic gradient descent.</p>
<p>A Bernoulli model predicts <span class="math inline">y = P(Y_1 = y_1, \ldots, Y_n = y_n \mid X_1 = x_1, \ldots, X_n = x_n)</span>. If we assume that the distribution of the variables are a Bernoulli distribution, so they're independent, we can then write this as <span class="math inline">y = \prod P(Y_i = y_i \mid X_i = x_i) = \prod p(x_i; w)^{y_i} (1 - p(x_i; w))</span> ;wip: get the formula for this</p>
<p>Logistic regression tries to predict the value of <span class="math inline">0 \le y \le 1</span> given <span class="math inline">\vec x</span> by fitting the formula <span class="math inline">\frac 1 {1 + \exp(\vec w \cdot x)}</span>, where <span class="math inline">\vec w</span> is the thing that we're trying to fit.</p>
<p>We use a sigmoid rather than, say, a step function, because the gradient doesn't have any signal - if we differentiate it, the derivative is just 0 everywhere, so gradient descent wouldn't be able to get closer to the solution at every step. Instead, the sigmoid formula has a gentle curve, so its derivative is more suitable for performing gradient descent on.</p>
<p>Our loss function is then <span class="math inline">f(y&#39;, y) = \ln(y&#39;) * y + \ln(1 - y&#39;) (1 - y)</span>, where <span class="math inline">y&#39;</span> is the model's prediction and <span class="math inline">y</span> is the true value. ;wip: why??? look at slides</p>
<p>Tensorflow example, implementing logistic regression using the built in gradient descent optimizer to minimize the loss function. When doing gradient descent, we want the largest learning rate that still converges.</p>
<p>;wip: logistic regression</p>
<h1 id="section-4">21/9/17</h1>
<p>Guest lecture by Aghastya from Focal Systems (aghastya@focal.systems).</p>
<p>A perceptron tells you which side of a hyperplane a point is, and also includes an algorithm to separate two classes with a hyperplane - a binary classifier. Logistic regression finds a line of best fit, as well as a measure of confidence that our prediction is correct, because the prediction's value is between -1 and 1 rather than exactly -1 or 1 - a binary classifier as well. The logistic regression gives a higher-magnitude prediction the farther it is from the logistic regression's hyperplane.</p>
<p>Perceptrons aren't very good binary classifiers overall for reasons weve previously discussed, but they're computationally cheap to run and easy to reason about. One example of the this is the XOR problem - a hyperplane cannot separate an XOR function: two classes <span class="math inline">\set{\tup{0, 1}, \tup{1, 0}}</span> and <span class="math inline">\set{\tup{0, 0}, \tup{1, 1}}</span>.</p>
<p>The goal of deep learning classification is to learn a representation of the input into something that's linearly separable, so we could then use classifier techniques like logistic regression.</p>
<p>Consider a <strong>two-payer perceptron</strong>. We have parameters <span class="math inline">U, \vec c, \vec w, b</span>. Let <span class="math inline">\vec z = U\vec x + \vec b</span> and <span class="math inline">h = f(\vec z)</span>, where <span class="math inline">f</span> is a nonlinear function like <span class="math inline">x^2</span> or <span class="math inline">\arctan(x)</span>. Then the output of the two-layer perceptron is then <span class="math inline">\hat y = \vec h \cdot \vec w + b</span>. There are three layers here:</p>
<ol type="1">
<li>The <strong>linear layer</strong> takes the model input <span class="math inline">\vec x</span> and transforms it into another linear space via weights <span class="math inline">U</span> and the bias term <span class="math inline">\vec c</span>, to get the hidden layer inputs <span class="math inline">\vec z</span>.
<ul>
<li>This is the part of the perceptron before the <span class="math inline">\sgn</span> function.</li>
</ul></li>
<li>The <strong>hidden layer</strong> takes the hidden input <span class="math inline">\vec z</span> and applies a non-linear function to it, so we can represent non-linearity in the input, to get the linear layer inputs <span class="math inline">\vec h</span>.
<ul>
<li>The function used in these hidden layers is known as an <strong>activation function</strong>.</li>
<li>Common activation functions are sigmoid <span class="math inline">\frac{1}{1 + \exp(x)}</span>, inverse tangent <span class="math inline">\arctan(x)</span>, and rectified linear unit (ReLU) <span class="math inline">\max(0, x)</span>.</li>
</ul></li>
<li>The <strong>linear layer</strong> takes the hidden layer output <span class="math inline">\vec h</span> and transforms it into another linear space via weights <span class="math inline">\vec w</span>.</li>
</ol>
<p>Essentially, we're weighting the input <span class="math inline">\vec x</span> by the weight matrix <span class="math inline">U</span> and the bias term <span class="math inline">\vec c</span> (a value unaffected of input that influences the output in a particular direction) in a <strong>linear layer</strong> to get the <strong>hidden layer inputs</strong> <span class="math inline">\vec z</span>. Then, we apply the non-linearity <span class="math inline">\vec h = f(\vec z)</span> to those to get the <strong>hidden layer output</strong> <span class="math inline">\vec h</span>. The hidden layer outputs <span class="math inline">\vec h</span> are then used as the inputs to the linear classifier <span class="math inline">\hat y = \vec h \cdot \vec w + b</span>, which is the output of the two-layer perceptron.</p>
<p>The idea is to compose a bunch of non-linear functions to approximate the actual, unknown function of <span class="math inline">\vec x</span> that generates <span class="math inline">y</span>. The non-linear functions add complexity to the model, and by adding enough of those non-linear functions, we can represent some pretty complex things.</p>
<p>The <strong>width</strong> of a multi-layer perceptron is the number of dimensions of <span class="math inline">\vec x</span>. Note that this doesn't include the bias term <span class="math inline">\vec c</span>.</p>
<p>We can't just use linear functions because composing linear functions just makes another linear function - if we applied linear functions on a dataset that wasn't linearly separable, it would still not be linearly separable, but if we apply non-linear functions, there's a chance it can be transformed into something linearly separable.</p>
<p>Consider now an example: <span class="math inline">U = \begin{bmatrix} 1 &amp; 1 \\ 1 &amp; 1 \end{bmatrix}, \vec c = \begin{bmatrix} 0 \\ -1 \end{bmatrix}, \vec w = \begin{bmatrix} 2 \\ -4 \end{bmatrix}, \vec b = -1, f(x) = \max(t, 0)</span>. The activation function is ReLU (rectified linear unit). As it turns out, this can correctly classify the output of the XOR function, something that a plain perceptron can't do.</p>
<p>We can stack these two-layer perceptrons - feeding the output of one into the input of another - to build <span class="math inline">k</span>-layer perceptrons! It's actually been proven that if the activation function is not a polynomial, and there exists at least 1 hidden layer, the resulting model can theoretically learn any function that has values between 0 and 1! This is called the <strong>universal approximation theorem</strong>.</p>
<p>The multi-layer perceptron learns a <strong>hierarchical non-linear feature representation</strong>. Generally, we then feed the outputs of the neural net into our existing classification/regression tools.</p>
<p>As we go from levels closer to the input to levels farther away, we start being able to classify higher-and-higher-level features. For example, an image classifier network might start with learning lines/circles in the lower levels, and then shapes like trees and cars and faces in the higher levels.</p>
<p>Choosing the right activation function and architecture is currently more of an art than a science - there's a lot of nuances that we'll go over in later lectures. Usually, we'll just add more layers than we need, and regularize afterward.</p>
<h2 id="backpropagation">Backpropagation</h2>
<p>The multilayer perceptron's output is clearly a function of the input, <span class="math inline">\hat y = q(\vec x; \Theta)</span>, where <span class="math inline">\Theta</span> is the parameters of the model. We usually don't know the true function <span class="math inline">q^*</span> that we're trying to approximate, but deep learning samples a lot of values of <span class="math inline">q^*</span> and then tries to approximate that by composing lots of nonlinear functions to get <span class="math inline">q</span>.</p>
<p>To do machine learning, we need to be able to talk about the difference between the prediction <span class="math inline">\hat y</span> and the truth <span class="math inline">y</span>. We can represent this as a <strong>loss function</strong> <span class="math inline">\ell(\Theta; X, \vec y)</span> a function of the model parameters (e.g., the <span class="math inline">\vec w</span> values in a linear regressor), as well as some input features <span class="math inline">\vec x</span> and truths <span class="math inline">y</span>. Usually, we omit the X and Y values to write this as <span class="math inline">\ell(\Theta)</span>.</p>
<p>One really common loss function is <strong>cross entropy/logistic loss</strong>: <span class="math inline">\ell(\Theta) = -\sum_{i = 1}^n \left(y_i \ln \hat y_i - (1 - y_i) \ln(1 - \hat y_i)\right)</span> where <span class="math inline">y_i \in \set{0, 1}</span> is the true value and <span class="math inline">\hat y_i \in \set{0, 1}</span> is the predicted value. For <span class="math inline">k</span> classes, we can write this as <span class="math inline">\ell(\Theta) = -\sum_{i = 1}^n \sum_{c = 1}^k \vec y_{i, c} \ln \vec \hat y_{i, c}</span> where <span class="math inline">\vec y</span> and <span class="math inline">\vec \hat y</span> are one-hot vectors. Another really common loss function is mean-squared-error, which is just <span class="math inline">\sum_{i = 1}^n (y_i - \hat y_i)^2</span>.</p>
<p>Gradient descent is a method of minimizing any function by iteratively taking steps toward minima.</p>
<p>We want to minimize our loss function using gradient descent - minimizing <span class="math inline">\ell(\Theta) = \frac 1 n \sum \ell(q(x_i; \Theta), y_i)</span> by iteratively computing <span class="math inline">\Theta_{t + 1} = \Theta_t - \eta_t \frac{\dee}{\dee \Theta_t} \ell(\Theta_t)</span>. Here, <span class="math inline">\Delta \ell(\Theta)</span> is the <strong>gradient</strong>, and <span class="math inline">\eta_t</span> is the <strong>learning rate</strong>.</p>
<p>The loss function should ideally get smaller as we get closer to a good answer, but really we just need a function <span class="math inline">\ell(x&#39;, x)</span> (<span class="math inline">x&#39;</span> is the prediction, <span class="math inline">x</span> is the true value) that's 0 when we have a correct answer, and positive otherwise.</p>
<p>If the function is continuous, the gradient is the derivative of the function. Note that since we're subtracting the gradient, the gradient actually points away from the direction we're optimizing toward.</p>
<p>The gradient at a particular node in the computation graph is known as a local gradient.</p>
<p>;wip: rewrite this section more clearly, talk about forward and backward pass</p>
<p>Backpropagation is a method of computing the gradient of <span class="math inline">\Theta</span> with respect to <span class="math inline">\ell(\Theta)</span>. Suppose we have <span class="math inline">r = f(q(x))</span>. By the chain rule, <span class="math inline">\frac{\dee r}{\dee x} = \frac{r}{\dee f} \frac{f}{\dee q} \frac{q}{\dee x}</span>, so we can easily recursively differentiate to get the gradient.</p>
<p>Backpropagation is implemented by building a computation graph, differentiating at the output layer, and working our way back to the input layer. The innovation is that it's much faster than the naive way of computing the gradient, where we hold all but one variable constant and find the partial derivative.</p>
<p>Backpropagation is an expensive algorithm, and was the reason neural networks weren't practical for a long time, until we can cheap GPU-based computation.</p>
<h1 id="section-5">26/9/17</h1>
<p>Assignment 1 deadline extended to Thursday. In-class overview of the assignment.</p>
<p>For the winnow algorithm, normalizing is pretty important, since the step size depends on the maximum absolute value of the data, unlike the perceptron, where those initial parameters don't matter as much.</p>
<p>The winnow algorithm will always converge, regardless of the step size. This is because of the normalization step in the loop, where we divide by the sum of the weights and biases. However, certain step size values will minimize the number of passes before we get convergence.</p>
<p>Closed-form solution for alternating minimization on ridge regression:</p>
<blockquote>
<p>We want to solve <span class="math inline">z^* = \argmin_z \frac 1 2 \magn{\vec X_{:j} z + \sum_{k \ne j} \vec X_{:k} w_k - \vec y}^2 + \lambda z^2</span>.<br />
Let <span class="math inline">\vec u = \vec X_{:j}</span> and <span class="math inline">\vec v = \sum_{k \ne j} \vec X_{:k} w_k - \vec y</span>, so <span class="math inline">z^* = \argmin_z \frac 1 2 \magn{\vec u z + \vec v}^2 + \lambda z^2 = \argmin_z \frac 1 2 \magn{\vec u}^2 z^2 + \vec u \cdot \vec v z + \frac 1 2 \magn{v}^2 + \lambda z^2 = \argmin_z (\frac 1 2 \magn{\vec u}^2 + \lambda) z^2 + \vec u \cdot \vec v z + \frac 1 2 \magn{v}^2</span>.<br />
Since <span class="math inline">\frac 1 2 \magn{v}^2</span> doesn't depend on <span class="math inline">z</span>, it doesn't affect the value of <span class="math inline">\argmin_z</span>. So <span class="math inline">z^* = \argmin_z (\frac 1 2 \magn{\vec u}^2 + \lambda) z^2 + \vec u \cdot \vec v z</span>.<br />
Clearly, <span class="math inline">\frac{\dee}{\dee z} \left(\left(\frac 1 2 \magn{\vec u}^2 + \lambda\right) z^2 + \vec u \cdot \vec v z\right) = \left(\magn{\vec u}^2 + 2\lambda\right)z + \vec u \cdot \vec v</span>.<br />
If we set the derivative to 0, then <span class="math inline">z = -\frac{\vec u \cdot \vec v}{\magn{\vec u}^2 + 2\lambda}</span>. Since the equation was quadratic, this is the only local optima, so it is also the global minimum. Therefore, <span class="math inline">z</span> is minimized at <span class="math inline">z = -\frac{\vec u \cdot \vec v}{\magn{\vec u}^2 + 2\lambda}</span>.</p>
</blockquote>
<h2 id="nearest-neighbor-rule">Nearest neighbor rule</h2>
<p>Supose we have a perceptron classifier <span class="math inline">\hat y = \sgn(\vec x \cdot \vec w + b)</span>. The <strong>decision boundary</strong> <span class="math inline">\vec x \cdot \vec w + b = 0</span> is the decision boundary here, and we can do a lot with it. This is a <strong>parametric</strong> classifier, because it uses a finite-dimensional set of parameters <span class="math inline">\vec w</span>. In contrast, a <strong>non-parametric</strong> classifier has an arbitrarily large number of parameters. The nearest-neighbor rule is non-parametric, because</p>
<p>The <strong>nearest-neighbor rule</strong> is a way to predict <span class="math inline">y</span> given a feature vector <span class="math inline">\vec x</span> and a training set. Given a feature vector <span class="math inline">\vec x</span> (the <strong>query point</strong>), find the training set entry <span class="math inline">\tup{\vec x&#39;, y&#39;}</span> such that <span class="math inline">\vec x&#39;</span> is the nearest to <span class="math inline">\vec x</span> by some distance metric, then predict that <span class="math inline">y = y&#39;</span>. Essentially, we take the nearest point to the given one, and simply take that point's value as the prediction.</p>
<p>The distance metric <span class="math inline">d(x_1, x_2)</span> must be symmetic (<span class="math inline">d(x_1, x_2) = d(x_2, x_1)</span>), definite (<span class="math inline">d(x, x) = 0</span>), and satisfy the triangle equality (<span class="math inline">d(x_1, x_3) \le d(x_1, x_2) + d(x_2, x_3)</span>). Some examples of distance metrics are the <span class="math inline">L_2</span> norm (Euclidean distance, square root of sum of squares), <span class="math inline">L_1</span> norm (Manhattan distance, sum of absolute values), and <span class="math inline">L_\infty</span> norm (Chebyshev distance, the max value). In general, the <span class="math inline">L_p</span> norm of a vector <span class="math inline">\vec x</span> is <span class="math inline">(\sum_i \abs{x_i}^p)^{\frac 1 n}</span>.</p>
<p>The nearest-neighbor rule doesn't need any training, but in general takes <span class="math inline">O(nd)</span> space to store the entire training set (<span class="math inline">n</span> is number of points, <span class="math inline">d</span> is number of dimensions).</p>
<p>If we implement nearest-neighbor naively, it takes <span class="math inline">O(nd)</span> time to get a prediction. We can do better by constructing a Voronoi diagram - a diagram where the space is partitioned into <span class="math inline">n</span> areas, each of those <span class="math inline">n</span> areas contains exactly one point, and the point is always the nearest neighbor within its associated area. In 2D, this takes <span class="math inline">O(n \log n)</span> time and <span class="math inline">O(n)</span> space. In general, this takes <span class="math inline">n^{O(d)}</span> space and <span class="math inline">O(d \log n)</span> query time, which is still rather bad. In the real world, we often use faster, approximate nearest-neighbor algorithms that take advantage of hashing and similar techniques.</p>
<p>Nearest-neighbor is affected by normalization if it distorts the distance metric, such as if we scale one dimension but not another. Usually, we normalize every feature vector by subtracting the mean of all the feature vectors and then dividing by the standard deviation of all the feature vectors.</p>
<p>Nearest-neighbor depends heavily on the chosen distance metric. There's actually a way to determine a good metric to use based on the training set, based on tweaking a parameterized distance function known as the Mahalanobias function.</p>
<p>Consider some linear transformation <span class="math inline">L</span> of <span class="math inline">\vec x_1, \vec x_2</span>: <span class="math inline">L\vec x_1, L\vec x_2</span> that transforms <span class="math inline">\vec x_1, \vec x_2</span> into a lower-dimensional space. Clearly, the Euclidean distance between them is <span class="math inline">d(\vec x_1, \vec x_2) = \sqrt{(L(\vec x_1 - \vec x_2))^T L(\vec x_1 - \vec x_2)} = \sqrt{(\vec x_1 - \vec x_2)^T L^T L (\vec x_1 - \vec x_2)}</span>. Let <span class="math inline">M = L^T L</span>, then <span class="math inline">d(\vec x_1, \vec x_2) = \sqrt{(\vec x_1 - \vec x_2)^T M (\vec x_1 - \vec x_2)}</span>. This is called the <strong>Mahalanobias function</strong>, and is parameterized by a matrix <span class="math inline">M</span>. It's essentially the Euclidean distance of the lower-dimensional transformations of the vectors.</p>
<p>We can then optimize <span class="math inline">M</span> such that it forms a good distance metric (one that makes the loss function small). Specifically, that means making the distance <span class="math inline">d_M(\vec x_1, \vec x_2)</span> small if <span class="math inline">\vec x_1</span> and <span class="math inline">\vec x_2</span> are in the same class, and large if they are not.</p>
<p>The <span class="math inline">k</span>-nearest-neighbor is the neighbor rule, but instead of just finding the nearest neighbor, we find the <span class="math inline">k</span> nearest neighbors in the training set, and then take their results and combine them, usually either taking the mode (majority vote) or mean of all those <span class="math inline">y</span> values (usually <span class="math inline">k</span> is odd, so for boolean <span class="math inline">y</span> there is never a tie).</p>
<p>The <span class="math inline">k</span> in <span class="math inline">k</span>-nearest-neighbors sort of acts like a regularization parameter - the higher <span class="math inline">k</span> is, the less complicated the resulting model is (the decision boundaries in the voronoi diagram are smoother). Intuitively, the higher <span class="math inline">k</span> is, the more points we have to change to get the prediction for a query point to change.</p>
<p>We want to pick a <span class="math inline">k</span> that avoids overfitting, but doesn't make the resulting predictions overly simple. Usually this is done by just trying a bunch of <span class="math inline">k</span> values and evaluating each value using cross-validation.</p>
<p><span class="math inline">k</span>-nearest-neighbors works surprisingly well on problems with a low number of dimensions and a large training set, and is often used in real-world problems. It works less well with higher dimensionality or smaller training sets.</p>
<p>As the size of the training set tends to infinity, <span class="math inline">P(Y_1 \ne Y \mid X) \le 2P^* - \frac{c}{c - 1} (P^*)^2</span> (Cover &amp; Hart, 1967). <span class="math inline">P(Y_1 \ne Y \mid X)</span> is the probability of a misprediction, <span class="math inline">c</span> is the number of categories/classes, and <span class="math inline">P^*</span> is the Bayes error (the theoretical minimum error, for any machine learning model). Essentially, this is saying that 1-nearest-neighbor will have an error within twice the theoretical minimum error - a very useful result for bounding the error!</p>
<h1 id="section-6">28/9/17</h1>
<p>Review of the last class.</p>
<p>New notation: <span class="math inline">1_{a = b}</span> is an <strong>indicator function</strong> - a function that is 1 if <span class="math inline">a = b</span>, and 0 otherwise. An indicator function can be used inside of expectations to replace probabilities: <span class="math inline">P(a = b) = E(1_{a = b})</span>. We often use this trick to simplify probabilities.</p>
<p>The <strong>Bayes error</strong> is defined by <span class="math inline">P^* = \min_{f : \mathcal{X} \to \set{-1, 1}} P(f(X) \ne Y)</span>. Here, <span class="math inline">X</span> is a feature vector, <span class="math inline">f(X)</span> is the classifier's prediction, and <span class="math inline">Y</span> is the true value. Essentially, it's the minimum error rate for any possible classifier <span class="math inline">f(X)</span>, regardless of how that classifier is implemented. For a binary classifier, we can write this as <span class="math inline">E(\eta(X) + 1_{f(X) = 1}(1 - 2 \eta(X)))</span>.</p>
<p>The <strong>Bayes rule</strong> is the best possible binary classifier, since it achieves a prediction error equivalent to the Bayes error. This can be written as <span class="math inline">f^*(X) = \begin{cases} 1 &amp;\text{if } \eta(X) \ge \frac 1 2 \\ -1 &amp;\text{otherwise} \end{cases}</span> where <span class="math inline">\eta(X) = P(Y = 1 \mid X)</span>.</p>
<p>Proof of optimality:</p>
<blockquote>
<p>Clearly, <span class="math inline">P(f(X) \ne Y) = 1 - P(f(X) = Y)</span>. We don't know <span class="math inline">Y</span>, but we do know it's either -1 or 1, so <span class="math inline">P(f(X) \ne Y) = 1 - P(f(X) = 1 \land Y = 1) - P(f(X) = -1 \land Y = -1)</span>.<br />
We now have unknown <span class="math inline">X</span> and unknown <span class="math inline">Y</span>. We're going to fix <span class="math inline">X</span> and look at the probability of <span class="math inline">Y</span>, a technique known as <strong>conditioning</strong>. This works because <span class="math inline">P(f(X) = 1, Y = 1) = \int_{f(x) = 1, y = 1} P(x \land y) \dee x \dee y = \int_{f(x) = 1, y = 1} P(y \mid x) P(x) \dee x \dee y = \int_{f(x) = 1} \left(\int_{y = 1} P(y \mid x) \dee y \right) \dee x = E(1_{f(X) = 1} P(Y = 1 \mid X))</span>.<br />
So <span class="math inline">P(f(X) \ne Y) = 1 - P(f(X) = 1, Y = 1 \mid X) = 1 - E(1_{f(X) = 1} P(Y = 1 \mid X)) - E(1_{f(X) = -1} P(Y = -1 \mid X))</span> - we conditioned on <span class="math inline">X</span>, then took the expectation.<br />
Let <span class="math inline">\eta(X) = P(Y = 1 \mid X)</span>, so <span class="math inline">P(Y = -1 \mid X) = 1 - \eta(X)</span>. Also, <span class="math inline">1_{f(X) = -1} = 1 - 1_{f(X) = -1}</span>.<br />
So <span class="math inline">P(f(X) \ne Y) = 1 - E(1_{f(X) = 1} \eta(X)) - E((1 - 1_{f(X) = 1}) (1 - \eta(X))) = E(-1_{f(X) = 1} \eta(X) + \eta(X) + 1_{f(X) = 1} - 1_{f(X) = 1} \eta(X))</span>.<br />
So <span class="math inline">P(f(X) \ne Y) = E(\eta(X) + 1_{f(X) = 1}(1 - 2 \eta(X)))</span>.</p>
</blockquote>
<p>Note that <span class="math inline">\eta(X)</span> is already defined, but we can choose our own <span class="math inline">1_{f(X) = 1}</span>. Since we want to minimize <span class="math inline">P(f(X) \ne Y)</span>, we want <span class="math inline">1_{f(X) = 1}</span> to be 1 exactly when <span class="math inline">1 - 2 \eta(X) \le 0</span>. This tells us that the optimal classifier <span class="math inline">f^*</span> is <span class="math inline">f^* = \begin{cases} 1 &amp;\text{if } \eta(X) \ge \frac 1 2 \\ -1 &amp;\text{otherwise} \end{cases}</span>, which is intuitively obvious - say true if it's more than 50% likely to be true!</p>
<p>Since we won't have <span class="math inline">\eta(X) = P(Y = 1 \mid X)</span> in most real-world problems, it's generally not possible to implement this in practice.</p>
<p>For more than two classes, we can do something similar to get <span class="math inline">f^*(X) = \argmax_{1 \le m \le c} P(Y = m \mid X)</span>, and the new Bayes rule would be <span class="math inline">P^* = E(1 - \max_{1 \le m \le c} P(Y = m \mid X))</span>. This is the best we can possibly do assuming the data is sampled independently and identically from <span class="math inline">\tup{X, Y}</span>.</p>
<p>Note that in the worst case, <span class="math inline">P^* = \frac{c - 1}{c}</span>. So the more classes we have, the larger the maximum possible Bayes error is.</p>
<p>So looking back at the formula we saw last class, as the training set size goes to infinity for 1-nearest-neighbor, <span class="math inline">P(Y_1 \ne Y \mid X) \le 2P^* - \frac{c}{c - 1} (P^*)^2</span>. So in the worst case, 1-nearest-neighbor will be less than twice as worse at the Bayes error. For this to be roughly true in practice though, <span class="math inline">n</span> must grow exponentially with respect to <span class="math inline">d</span>, which is generally not practical.</p>
<p>The error rate for <span class="math inline">k</span>-nearest-neighbor for some <span class="math inline">k = 2t + 1</span> is <span class="math inline">\frac 1 {2^n} \sum _{0 \le i \le t} {n \choose i}</span> (this simplifies to <span class="math inline">\frac 1 {2^n}</span> for 1-NN). This tells us that a larger <span class="math inline">k</span> is actually going to give us a higher error in the worst case. Consider how this worst case occurs, when all the points are in random classes.</p>
<p>We want to use a smaller <span class="math inline">k</span> when the classes are easier to separate, and a larger <span class="math inline">k</span> when the classes tend to be harder to separate - larger values for &quot;harder&quot; problems, smaller for &quot;easier&quot; problems. We usually choose <span class="math inline">k</span> via cross-validation.</p>
<p>SSBD page 224: For any <span class="math inline">c &gt; 1</span> and any learning algorithm <span class="math inline">L</span>, there exists a distribution <span class="math inline">\set{0, 1}^d \times \set{0, 1}</span> such that the Bayes error is 0 but for sample sizes <span class="math inline">n \le \frac{(c + 1)^d}{2}</span>, the error probability for <span class="math inline">L</span> is greater than <span class="math inline">\frac 1 4</span>. Essentially, there always exists a distribution such that a given learning algorithm will be wrong 25% or more of the time, but an ideal classifier would get it perfectly, with a limited training set size.</p>
<p>This is especially applicable to <span class="math inline">k</span>-NN, and we often try to avoid this by increasing training set size and doing dimensionality reduction (e.g., projecting a higher dimension into a lower dimension while preserving the most important elements of the features). One common way to perform dimensionality reduction is using principal component analysis (PCA).</p>
<p>For regression, we can also use something similar to <span class="math inline">k</span>-nearest-neighbors. Instead of the training set points having discrete classes, they instead have real values. Essentially, given a feature vector <span class="math inline">\vec x</span>, we take the <span class="math inline">k</span>-nearest-neighbors in the training set, and average their <span class="math inline">y</span> values to get the prediction <span class="math inline">y&#39;</span>. We might additionally do things like do a weighted average based on their distance to <span class="math inline">\vec x</span>.</p>
<h2 id="hard-margin-support-vector-machine-svm">Hard-Margin Support Vector Machine (SVM)</h2>
<p>Recall that the naive perceptron algorithm assumes that the data is linearly separable, and requires that to be true to converge. Furthermore, the perceptron will find any separating hyperplane, depending on how we feed in the data, rather than what we might consider the &quot;best&quot; separating hyperplane.</p>
<p>The perceptron is solving the minimization problem &quot;minimize <span class="math inline">0</span> subject to <span class="math inline">y_i(\vec w^T \vec x_i + b) &gt; 0</span> for all <span class="math inline">i</span>&quot; - there's no objective function, so we're just finding a feasible solution. The SVM also finds a feasible solution, but also tries to maximize the <strong>margin</strong> - the minimum distance from the hyperplane to any training set point.</p>
<p>Given a separating hyperplane <span class="math inline">H</span> defined by <span class="math inline">\vec w \cdot \vec x + b = 0</span>, we want to translate and rotate it until the margin is maximized. First we'll normalize the scale: <span class="math inline">\frac{\vec w}{\magn{w}} \cdot \vec x + \frac{b}{\magn{\vec w}} = 0</span>. We translate the hyperplane by changing <span class="math inline">b</span>, and rotate it by changing <span class="math inline">\vec w</span>. Suppose we translate it upward by <span class="math inline">s</span> until it hits a point, and downward by <span class="math inline">t</span> until it hits a point.</p>
<p>Now we have the hyperplanes <span class="math inline">H_1</span> defined by <span class="math inline">\frac{\vec w}{\magn{w}} \cdot \vec x + \frac{b}{\magn{\vec w}} = t</span> and <span class="math inline">H_{-1}</span> defined by <span class="math inline">\frac{\vec w}{\magn{w}} \cdot \vec x + \frac{b}{\magn{\vec w}} = s</span>, both touching points and having an empty space between them.</p>
<h1 id="section-7">3/10/17</h1>
<p>Overview of assignment 2. For question 1, don't print out the distance matrix, since it's too large. Also, the pseudocode isn't very efficient, and you're expected to optimize things, like turning loops into NumPy array operations.</p>
<p>Clearly, the distance between <span class="math inline">H</span> and <span class="math inline">H_1, H_{-1}</span> is <span class="math inline">\min \set{s, t}</span>. Let <span class="math inline">H_0</span> be the hyperplane exactly in between <span class="math inline">H_1</span> and <span class="math inline">H_{-1}</span>. Clearly, if <span class="math inline">d</span> is the distance between the values, then we have <span class="math inline">H_1 = \vec w \cdot \vec x + b = d</span> and <span class="math inline">\vec w \cdot \vec x + b = -d</span>. If we scale <span class="math inline">\vec w</span> and <span class="math inline">b</span> by dividing them by <span class="math inline">d</span>, we then get <span class="math inline">\vec w \cdot \vec x + b = 1</span> and <span class="math inline">\vec w \cdot \vec x + b = -1</span>.</p>
<p>Now let's scale the formulas by dividing by <span class="math inline">\magn{w}</span>, so that we have <span class="math inline">H_0: \frac{\vec w}{\magn{w}} \cdot \vec x + \frac{b}{\magn{\vec w}} = 0</span> and <span class="math inline">H_1: \frac{\vec w}{\magn{w}} \cdot \vec x + \frac{b}{\magn{\vec w}} = \frac 1 {\magn{w}}</span>. Now we've ensured that <span class="math inline">\magn{\vec w} = 1</span>.</p>
<p>Clearly, we can rotate the entire coodinate space without changing the relative distances between hyperplanes and points. Suppose we rotate the coordinate system such that <span class="math inline">\vec w = \begin{bmatrix} 1 \\ 0 \\ \vdots \\ 0 \end{bmatrix}</span> - the only weight component is the rotated first component of <span class="math inline">\vec x</span>. Then we get the minimum distance between <span class="math inline">H_0</span> and either <span class="math inline">H_1</span> or <span class="math inline">H_{-1}</span> as <span class="math inline">x_1 + \frac{b}{\magn{\vec w}} = \frac 1 {\magn{w}}</span>.</p>
<p>Our goal with hard-margin SVM is to compute <span class="math inline">\vec w, b = \argmax_{\vec w, b: \forall i, y_i(\vec w \cdot \vec x_i + b) \ge 1)} \frac 1 {\magn{\vec w}}</span>. Intuitively, we're maximizing the distance between the hyperplane <span class="math inline">H_0</span> and the closest point by changing <span class="math inline">H_0</span>.</p>
<p>Now we're going to make it more like what textbooks show this as, using these identities: <span class="math inline">\max f(\vec w) = -\min -f(\vec w)</span> for any function <span class="math inline">f</span>, <span class="math inline">\max \frac 1 {f(\vec w)} = \frac 1 {\min f(\vec w)}</span> for positive function <span class="math inline">f</span>, and <span class="math inline">\min f(\vec w)</span> is equivalent to <span class="math inline">\min g(f(\vec w))</span> for any function <span class="math inline">f</span> and strictly monotone function <span class="math inline">g</span> (i.e., the minimums occur at the same values of <span class="math inline">\vec w</span>, even though the actual minimum values are different).</p>
<p>Clearly, <span class="math inline">\max_{\vec w,b: \forall i, y_i(\vec w \cdot \vec x_i + b) \ge 1)} \frac 1 {\magn{\vec w}} = \min_{\vec w,b: \forall i, y_i(\vec w \cdot \vec x_i + b) \ge 1)} \frac 1 2 \magn{w}^2</span>, since <span class="math inline">g(x) = x^2</span> is strictly monotonic. Then, the margin is <span class="math inline">\frac{1}{\magn{\vec w}}</span>.</p>
<p>Note that this formulation is essentially: &quot;minimize <span class="math inline">\frac 1 2 \magn{vec w}^2</span> subject to <span class="math inline">y_i(\vec w^T \vec x_i + b) &gt; 0</span> for all <span class="math inline">i</span>&quot;. Additionally, the minimum <span class="math inline">\vec w</span> and <span class="math inline">b</span> always exist, and are always unique - there's exactly one value for each! <span class="math inline">\vec w</span> is unique because <span class="math inline">\magn{\vec w}^2</span> is strongly convex, and <span class="math inline">b</span> is unique because for the unique <span class="math inline">\vec w</span> value, there's only one possible value of <span class="math inline">b</span> that will make <span class="math inline">\vec w \cdot \vec x_i + b = 0</span> for all <span class="math inline">x_i</span>.</p>
<p>The <strong>support vectors</strong> are a subset of the vectors we hit first when we move the hyperplane back and forth - a subset of the points that are on <span class="math inline">H_{-1}</span> and <span class="math inline">H_1</span>. Importantly, the support vectors entirely determine the SVM's solution, and there are usually a very small number of them, compared to how many points there are overall. For 2D SVM, for example, there will usually be only 3 or 4 of them in most problems.</p>
<p>We're going to now look at the Lagrangian dual of the problem - sort of like the dual of a linear program. Consider the primal problem: &quot;minimize <span class="math inline">\frac 1 2 \magn{\vec w}^2</span> subject to <span class="math inline">y_i(\vec w^T \vec x_i + b) &gt; 0</span> for all <span class="math inline">i</span>&quot;. Clearly, this is equivalent to <span class="math inline">\min_{\vec w, b} \max_{\alpha \ge 0} \frac 1 2 \magn{w}^2 - \sum \alpha_i(y_i(\vec w \cdot \vec x_i + b) - 1)</span>. Here, <span class="math inline">\vec w</span> and <span class="math inline">b</span> are the primal variables, and <span class="math inline">\alpha_i</span> are the Lagrangian multipliers/dual variables. This problem is equivalent to the primal problem - the resulting <span class="math inline">\vec w</span> and <span class="math inline">b</span> values are the same.</p>
<p>The Lagrangian variables can be thought of as &quot;adversarial&quot; - the <span class="math inline">\max_{\alpha \ge 0}</span> is trying to make the <span class="math inline">\min_{\vec w, \vec b}</span> result as bad as possible, and it does this by changing <span class="math inline">\alpha</span> depending on whether <span class="math inline">y_i(\vec w \cdot \vec x_i + b) - 1</span> is positive or negative. Ideally we want <span class="math inline">y_i(\vec w \cdot \vec x_i + b) - 1</span> to be non-negative, since then <span class="math inline">\max_{\alpha \ge 0}</span> has an upper bound.</p>
<p><strong>Fermat's theorem</strong> says that at <span class="math inline">\min_x f(x)</span>, <span class="math inline">\frac{\dee}{\dee x} f(x) = 0</span>.</p>
<p>The <strong>minimax theorem</strong> says that given compact convex sets <span class="math inline">X, Y\ subseteq \mb{R}^n</span> and a function <span class="math inline">f: X \times Y \to \mb{R}</span> such that <span class="math inline">f(\vec x, \vec y)</span> is convex if we hold <span class="math inline">\vec y</span> constant and concave if we hold <span class="math inline">\vec x</span> constant, then <span class="math inline">\min_{\vec x \in X} \max_{\vec y \in Y} f(\vec x, \vec y) = \max_{\vec y \in Y} \min_{\vec x \in X} f(\vec x, \vec y)</span>. In other words, under certain conditions we can swap the min and max. Intuitively, this represents the idea that if the minimizer and maximizer are players in a zero-sum game using the minimax strategy, then they are in a Nash equilibrium, so swapping them doesn't change the final result.</p>
<p>By the minimax theorem, <span class="math inline">\min_{\vec w, b} \max_{\alpha \ge 0} \frac 1 2 \magn{w}^2 - \sum \alpha_i(y_i(\vec w \cdot \vec x_i + b) - 1) = \max_{\alpha \ge 0} \min_{\vec w, b} \frac 1 2 \magn{w}^2 - \sum \alpha_i(y_i(\vec w \cdot \vec x_i + b) - 1)</span>. Note that there's no constraints on <span class="math inline">\vec w</span> and <span class="math inline">b</span> like we had for the primal problem (where we had <span class="math inline">\forall i, y_i(\vec w \cdot \vec x_i + b) \ge 1</span> constraint). Therefore, we can easily find the minimum by taking the derivative with respect to <span class="math inline">\vec w</span> and <span class="math inline">\vec b</span>.</p>
<p>Clearly, <span class="math inline">\frac{\dee}{\dee b} \left(\frac 1 2 \magn{w}^2 - \sum \alpha_i(y_i(\vec w \cdot \vec x_i + b) - 1)\right) = \sum \alpha_i y_i</span>. Clearly, <span class="math inline">\frac{\dee}{\dee \vec w} \left(\frac 1 2 \magn{w}^2 - \sum \alpha_i(y_i(\vec w \cdot \vec x_i + b) - 1)\right) = \vec w - \sum \alpha_i y_i \vec x_i</span>.</p>
<p>By Fermat's theorem, we set the derivatives to 0 to find candidates for the minimum value. So let <span class="math inline">\sum \alpha_i y_i = 0</span> and <span class="math inline">\vec w - \sum \alpha_i y_i \vec x_i = 0</span>. Now, we have <span class="math inline">\sum \alpha_i y_i \vec x_i = \vec w</span> and <span class="math inline">\sum \alpha_i y_i = 0</span>.</p>
<p>If we solve this, it gives us now the <strong>Lagrangian dual problem</strong>: <span class="math inline">\max_{\alpha \ge 0} \left(\sum \alpha_i - \frac 1 2 \magn{\sum \alpha_i y_i \vec x_i}^2\right)</span> subject to <span class="math inline">\sum \alpha_i y_i = 0</span>. We can expand this and rearrange to get <span class="math inline">\min_{\alpha \ge 0} \left(\frac 1 2 \sum_i \sum_j \alpha_i \alpha_j y_i y_j \vec x_i \cdot \vec x_j - \sum_k \alpha_k\right)</span> such that <span class="math inline">\sum a_i y_i = 0</span>. This is the <strong>dual hard-margin SVM problem</strong>, and we sometimes also write it as &quot;<span class="math inline">\min_{\alpha \ge 0} \left(\frac 1 2 \magn{\sum \alpha_i y_i \vec x_i}^2 - \sum \alpha_i\right)</span> subject to <span class="math inline">\sum \alpha_i y_i = 0</span>&quot;.</p>
<p>Note that this dual problem depends on the number of data points <span class="math inline">n</span>, rather than the number of dimensions <span class="math inline">d</span>, like the primal problem. So for high-dimensional, low-data-points problems, we generally want to solve the dual problem rather than the primal.</p>
<p>The dual problem also gives us a new interpretation of support vectors - a support vector is any input feature vector <span class="math inline">\vec x_i</span> such that <span class="math inline">\alpha_i &gt; 0</span>. Since <span class="math inline">\vec w = \sum \alpha_i y_i \vec x_i</span> and all non-support vectors will have their corresponding <span class="math inline">\alpha_i = 0</span>, the resulting weight only depends on the values of the support vectors.</p>
<p>A convex set is defined in the same way as for CO250 - a set where any weighted average of any two points in <span class="math inline">C</span> is also in <span class="math inline">C</span> (the set is closed under weighted averaging). The <strong>convex hull</strong> of a set of points <span class="math inline">C</span> is denoted <span class="math inline">\operatorname{ch}(C) = \set{\sum \alpha_i \vec x_i : \vec x_i \in C, \alpha_i \ge 0, \sum_j \alpha_j = 1}</span>. Essentially, it's the set of all linear combinations of the points within the bounds of the set, or the area enclosed if we connect every pair of points in <span class="math inline">C</span> with a line segment.</p>
<p>While the primal hard-margin SVM problem can be thought of as finding the maximum-margin hyperplane, the dual hard-margin SVM problem can be thought of as finding the points on the convex hulls of the two classes that minimize the distance between the convex hulls of the two classes. Let's take a look at why this interpretation makes sense.</p>
<p>We usually regularize minimization problems of the form <span class="math inline">\min_W \ell(W)</span> using something like <span class="math inline">\min_W \ell(W) + r(W)</span>, where <span class="math inline">r(W)</span> is some function that evaluates how not &quot;simple&quot; <span class="math inline">W</span> is. There always exists a <span class="math inline">C \in \mb{R}</span> such that <span class="math inline">\min_W \ell(W) + r(W)</span> is equivalent to the <strong>constrained</strong> version <span class="math inline">\min_{W: r(W) \le C} \ell(w)</span>. The converse isn't always true, so <span class="math inline">\min_{W: r(W) \le C} \ell(w)</span> doesn't necessarily have an equivalent regularized form, but it is usually true for things we'll look at in this course. We'll usually use the regularized version in practice since it's easier to compute, but the constrainted form has statistical properties that are more useful for theoretical analysis.</p>
<p>Note that the dual hard-margin SVM problem &quot;<span class="math inline">\min_{\alpha \ge 0} \left(\frac 1 2 \magn{\sum \alpha_i y_i \vec x_i}^2 - \sum \alpha_i\right)</span> subject to <span class="math inline">\sum \alpha_i y_i = 0</span>&quot; is of the form <span class="math inline">\min_W \ell(W) + r(W)</span> where <span class="math inline">r(\vec \alpha) = -\sum \alpha_i</span>. We can therefore choose a <span class="math inline">C</span> so that the problem becomes equivalent to &quot;<span class="math inline">\min_{\alpha \ge 0} \frac 1 2 \magn{\sum \alpha_i y_i \vec x_i}^2</span> subject to <span class="math inline">\sum \alpha_i y_i = 0</span> and <span class="math inline">\sum \alpha_i = C</span>&quot;.</p>
<p>If we let <span class="math inline">\alpha_i&#39; = \frac 2 C \alpha</span>, the SVM problem then becomes <span class="math inline">\min_{\vec \alpha&#39; \ge 0} \frac 1 2 \magn{\sum \alpha_i&#39; y_i \vec x_i}^2</span> subject to <span class="math inline">\sum \alpha_i&#39; y_i = 0</span> and <span class="math inline">\sum \alpha_i&#39; = 2</span>.</p>
<p>Let <span class="math inline">P = \set{i : y_i = 1}, N = \set{i : y_i = -1}</span>. Then <span class="math inline">\sum \alpha_i&#39; y_i = \sum_{i \in P} \alpha_i&#39; + \sum_{i \in N} \alpha_i&#39;</span>. Since <span class="math inline">\sum \alpha_i&#39; y_i = 0</span> from the constraints, <span class="math inline">\sum_{i \in P} \alpha_i&#39; + \sum_{i \in N} \alpha_i&#39; = 0</span>, so <span class="math inline">\sum_{i \in P} \alpha_i&#39; = \sum_{i \in N} \alpha_i&#39;</span>. Since <span class="math inline">\sum \alpha_i&#39; = 2</span> from the constraints, it must be that <span class="math inline">\sum_{i \in P} \alpha_i&#39; = \sum_{i \in N} \alpha_i&#39; = 1</span>. We then get the SVM problem formulated as &quot;minimize <span class="math inline">\frac 1 2 \magn{\sum_{i \in P} \alpha_i&#39; \vec x_i - \sum_{i \in N} \alpha_i&#39; \vec x_i}</span> (note that we flipped the sign because <span class="math inline">y_i = -1</span> for all <span class="math inline">i \in N</span>) subject to <span class="math inline">\sum_{i \in P} \alpha_i&#39; = 1</span> and <span class="math inline">\sum_{i \in N} \alpha_i&#39; = 1</span>&quot;.</p>
<p>Let <span class="math inline">\vec u</span> be the vector where <span class="math inline">u_i = \alpha_i&#39;</span> for all <span class="math inline">i \in P</span> and <span class="math inline">u_i = 0</span> otherwise, and let <span class="math inline">\vec v</span> be the vector where <span class="math inline">v_i = \alpha_i&#39;</span> for all <span class="math inline">i \in N</span> and <span class="math inline">v_i = 0</span> otherwise. Clearly, <span class="math inline">\sum u_i = \sum_{i \in P} \alpha_i&#39;</span> and <span class="math inline">\sum v_i = \sum_{i \in N} \alpha_i&#39;</span>. We can then write the dual hard-margin SVM problem as &quot;minimize <span class="math inline">\frac 1 2 \magn{\sum u_i \vec x_i - \sum v_i \vec x_i}</span> subject to <span class="math inline">\sum u_i = 1</span> and <span class="math inline">\sum v_i = 1</span>&quot;.</p>
<p>We can think of <span class="math inline">P</span> as the set of all positive-classed points, and <span class="math inline">N</span> as the set of all negative-classed points. We can then think of <span class="math inline">\sum_{i \in P} \alpha_i&#39; \vec x_i</span> where <span class="math inline">\sum_{i \in P} \alpha_i&#39; = 1</span> as an arbitrary point within the convex hull of all positive-classed points, and <span class="math inline">\sum_{i \in N} \alpha_i&#39; \vec x_i</span> where <span class="math inline">\sum_{i \in N} \alpha_i&#39; = 1</span> as an arbitrary point within the convex hull of all negative-classed points.</p>
<p>This also gives us a geometric interpretation of support vectors: the support vectors are a set of points on the surface of each convex hull such that, when we take average of them weighted by <span class="math inline">\vec u</span> (if this is the positive-classed convex hull) or <span class="math inline">\vec v</span> (if this is the negative-classed convex hull), we get the closest point to the other convex hull. That is why not all points on the hyperplanes <span class="math inline">H_1</span> and <span class="math inline">H_{-1}</span> are necessarily support vectors - they might not be needed in the weighted average to form the closest point to the other convex hull.</p>
<p>The distance between those points is then <span class="math inline">\magn{\sum_{i \in P} \alpha_i&#39; \vec x_i - \sum_{i \in N} \alpha_i&#39; \vec x_i}^2</span> where <span class="math inline">\sum_{i \in P} \alpha_i&#39; = 1</span> and <span class="math inline">\sum_{i \in N} \alpha_i&#39; = 1</span>. If we minimize this distance, we get two points, one in each convex hull, of minimum distance apart.</p>
<p>Recall that <span class="math inline">\sum \alpha_i y_i \vec x_i = \vec w</span>, from the derivation of the dual hard-margin SVM problem. This is equivalent to <span class="math inline">\vec w = \frac 2 C \left(\sum_{i \in P} \alpha_i&#39; \vec x_i - \sum_{i \in N} \alpha_i&#39; \vec x_i\right)</span>. In other words, the normal of the hyperplane is the difference between the two points within the convex hull. If we do the same for <span class="math inline">b</span>, we can determine that the resulting hyperplane <span class="math inline">H_0</span> must pass through the midpoint of the points in those two convex hulls, normal to their difference vector (this is a bit more complicated, though).</p>
<p>Note that <span class="math inline">\sum_{i \in P} \alpha_i&#39; \vec x_i</span> and <span class="math inline">\sum_{i \in N} \alpha_i&#39; \vec x_i</span> are not necessarily the extreme points of the convex hulls, so we can't just try all the pairs of extreme points on both convex hulls. This is a common misconception when learning about the dual SVM formulation.</p>
<h1 id="section-8">5/10/17</h1>
<p>More overview of assignment 2. The pseudocode in the assignments is only meant as a hint - we don't have to follow it if we can think of a better way to do the same thing. Question 3.1 is meant to show that we don't actually need to have a margin hyperparameter, and that it doesn't add any flexibility.</p>
<h2 id="soft-margin-svm">Soft-margin SVM</h2>
<p>What if the data isn't linearly separable? If this is the case, any hyperplane will always misclassify at least one of the points. Additionally, some of the data points might end up too close to the hyperplane, so the maximum margin is too small to make a good classifier.</p>
<p>The hard-margin SVM actually came 30 years before soft-margin, which was invented in the 1990s. Originally, they were called soft-margin networks.</p>
<p>For these cases, we can use a <strong>soft-margin SVM</strong>.</p>
<p>To do this, we change the hard constraints into soft constraints: where the primal problem for hard-margin was &quot;minimize <span class="math inline">\frac 1 2 \magn{vec w}^2</span> subject to <span class="math inline">y_i(\vec w^T \vec x_i + b) &gt; 0</span> for all <span class="math inline">i</span>&quot;, the soft-margin problem is now <span class="math inline">\min_{\vec w, b} \frac 1 2 \magn{w}^2 + C \sum (1 - y_i \hat y_i)_+</span>. The lack of hard constraints make this easier to analyse.</p>
<p>New notation: <span class="math inline">(f(x))_+ = \max(f(x), 0)</span>, the subscript plus is called the <strong>hinge loss function</strong>, because the graph looks like a door hinge on its side.</p>
<p>Here, <span class="math inline">C \sum (1 - y_i \hat y_i)_+</span> are the <strong>soft constraints</strong>, <span class="math inline">\sum (1 - y_i \hat y_i)_+</span> is the <strong>training error</strong>, <span class="math inline">C</span> is the cost parameter (which we tune as a hyperparameter), and <span class="math inline">\hat y_i = \vec w \cdot \vec x_i + b</span> is the hyperplane's prediction.</p>
<p>Note that <span class="math inline">(1 - y_i \hat y_i)_+</span> has 3 main behaviours when we plot it with respect to <span class="math inline">y_i \hat y_i</span>. When the prediction <span class="math inline">\hat y_i</span> is on the same side of the hyperplane as the true value <span class="math inline">y_i</span> and far enough from the hyperplane to be outside of the margin, the value is 0. When the prediction is on the same side as the true value, but within the margin (so <span class="math inline">0 \le y_i \hat y_i &lt; 1</span>), we get a little bit of loss (even though the prediction is correct, we still lose a bit because we aren't confident enough in our prediction). Otherwise, we're wrong and get even more loss.</p>
<p>Other loss functions are also in common use besides <span class="math inline">(1 - y_i \hat y_i)_+</span>, such as zero-one <span class="math inline">-\sgn(y_i \hat y_i)</span>, squared-hinge <span class="math inline">((1 - y_i \hat y_i)_+)^2</span> (this is useful because it happens to be differentiable everywhere), exponential <span class="math inline">\exp(-y_i \hat y_i)</span>, and logistic loss <span class="math inline">\ln(1 + \exp(-y_i \hat y_i))</span>. ;wip: advantages/disadvantages of both</p>
<p>Note that <span class="math inline">\frac 1 2 \magn{w}^2</span> is trying to maximize the hyperplane margin, and <span class="math inline">C \sum (1 - y_i \hat y_i)_+</span> is trying to maximize the correct predictions. Since these goals are not always compatible, the <span class="math inline">C</span> parameter adds balance between these two goals. In other words, we can make wrong prediction in return for a larger margin, and we can reduce the margin in return for more wrong predictions.</p>
<p>A convex loss function <span class="math inline">\ell(x)</span> is <strong>classification-calibrated</strong> if and only if it's differentiable at 0 and <span class="math inline">\frac{\dee}{\dee x} \ell(x) &lt; 0</span>. This doesn't tell us what loss function to use in practice, but it does tell us which ones we shouldn't use. Classification calibration means that <span class="math inline">\argmin_a E(\ell(Y a) \mid (X = x))</span> has the same sign as <span class="math inline">2 \eta(x) - 1</span>. If we minimize that, we get the Bayes rule <span class="math inline">\eta(x) \ell(a) + (1 - \eta(x))\ell(a)</span>.</p>
<p>Now let's look at the Lagrangian dual. First, notice that <span class="math inline">\min_x f(x)</span> is the same as <span class="math inline">\min_{x, t} t</span> such that <span class="math inline">f(x) \le t</span>, because we always want to choose a <span class="math inline">t = f(x)</span> - this is a useful trick from optimization. Consider now the primal: <span class="math inline">\min_{\vec w, b} \frac 1 2 \magn{w}^2 + C \sum (1 - y_i \hat y_i)_+</span>. To do the minimization, we want to differentiate, but we can't differentiate the soft constraint since the hinge loss function isn't always differentiable.</p>
<p>Instead, we'll use that trick we just saw to write the primal problem as follows: <span class="math inline">\min_{\vec w, b, } \frac 1 2 \magn{w}^2 + C \sum \xi_i</span> subject to <span class="math inline">(1 - y_i \hat y_i)_+ \le \xi_i</span> for all <span class="math inline">i</span>. We've now written the soft constraints as hard constraints, and <span class="math inline">\xi_i</span> are called <strong>slack variables</strong>, which we're trying to minimize. The <span class="math inline">\le</span> in these constraints is necessary in order to ensure that the problem is still convex (that's out of the score of this couse, though).</p>
<p>Each <span class="math inline">(1 - y_i \hat y_i)_+ \le \xi</span> constraint can actually be written as two constraints, <span class="math inline">1 - y_i \hat y_i \le \xi_i</span> and <span class="math inline">0 \le \xi_i</span>. Note that each <span class="math inline">\xi_i</span> depends on <span class="math inline">\vec w</span> and <span class="math inline">b</span>. Now we'll write this in the form <span class="math inline">f_i(\ldots) \le 0</span>: <span class="math inline">1 - y_i \hat y_i - \xi_i \le 0</span> and <span class="math inline">-\xi_i \le 0</span>. This is because <span class="math inline">f_i(\ldots)</span> must have a maximum value at <span class="math inline">f_i(\ldots) = 0</span>.</p>
<p>Now we'll add the Lagrangian maximizer and move the constraints into the objective function: <span class="math inline">\min_{\vec w, b, \vec \xi} \max_{\alpha \ge 0, \beta \ge 0} \frac 1 2 \magn{\vec w}^2 + \sum \left(C \xi_i + \alpha_i (1 - y_i \hat y_i - \xi_i) + \beta_i (-\xi_i)\right)</span>. By the minimax theorem, we can swap the minimizer and maximizer, so <span class="math inline">\max_{\alpha \ge 0, \beta \ge 0} \min_{\vec w, b, \vec \xi} \frac 1 2 \magn{\vec w}^2 + \sum \left(C \xi_i + \alpha_i (1 - y_i \hat y_i - \xi_i) + \beta_i (-\xi_i)\right)</span>.</p>
<p>Now we take the derivatives of <span class="math inline">\frac 1 2 \magn{\vec w}^2 + \sum \left(C \xi_i + \alpha_i (1 - y_i \hat y_i - \xi_i) + \beta_i (-\xi_i)\right)</span> with respect to <span class="math inline">\vec w, b, \vec \xi</span>: <span class="math inline">\frac{\dee}{\dee b} (\ldots) = \sum \alpha_i y_i</span>, <span class="math inline">\frac{\dee}{\dee \vec w} (\ldots) = \vec w - \sum \alpha_i y_i \vec x_i</span>, and <span class="math inline">\frac{\dee}{\dee \xi_i} (\ldots) = C - \alpha_i - \beta_i</span>. Just like with hard-margin SVM, we'll set the derivatives to 0 to get <span class="math inline">\sum \alpha_i y_i = 0</span>, <span class="math inline">\vec w = \sum \alpha_i y_i \vec x_i</span>, and <span class="math inline">C = \alpha_i + \beta_i</span> (where <span class="math inline">\alpha_i \le C</span> since <span class="math inline">\beta_i \ge 0</span>).</p>
<p>Clearly, <span class="math inline">\min_{\vec w, b, \vec \xi} \frac 1 2 \magn{\vec w}^2 + \sum \left(C \xi_i + \alpha_i (1 - y_i \hat y_i - \xi_i) + \beta_i (-\xi_i)\right) = \frac 1 2 \magn{\sum \alpha_i y_i \vec x_i}^2 + \sum \left((\alpha_i + \beta_i) \xi_i + \alpha_i - \alpha_i y_i \hat y_i - \alpha_i \xi_i - \beta_i \xi_i\right) = \frac 1 2 \magn{\sum \alpha_i y_i \vec x_i}^2 + \sum \alpha_i (1 - y_i (\vec w \cdot \vec x_i + b)) = \frac 1 2 \magn{\sum \alpha_i y_i \vec x_i}^2 + \sum \alpha_i - \sum \alpha_i y_i (\sum \alpha_i y_i \vec x_i) \cdot \vec x_i - b \sum \alpha_i y_i = \frac 1 2 \sum_i \sum_j \alpha_i \alpha_j y_i y_j \vec x_i \cdot \vec x_j + \sum \alpha_i - \sum_i \sum_j \alpha_i \alpha_j y_i y_j \vec x_i \cdot \vec x_j - b \sum \alpha_i y_i = -\frac 1 2 \sum_i \sum_j \alpha_i \alpha_j y_i y_j \vec x_i \cdot \vec x_j + \sum \alpha_i - b \sum \alpha_i y_i</span> where <span class="math inline">\alpha_i \le C</span>.</p>
<p>Since <span class="math inline">\sum \alpha_i y_i = 0</span>, that's equivalent to <span class="math inline">\sum \alpha_i - \frac 1 2 \sum_i \sum_j \alpha_i \alpha_j y_i y_j \vec x_i \cdot \vec x_j</span> where <span class="math inline">\alpha_i \le C</span>. We then get the <strong>dual soft0margin SVM problem</strong> &quot;<span class="math inline">\max_{C \ge \alpha \ge 0, \beta \ge 0} \sum \alpha_i - \frac 1 2 \sum_i \sum_j \alpha_i \alpha_j y_i y_j \vec x_i \cdot \vec x_j</span> subject to <span class="math inline">\sum \alpha_i y_i = 0</span>&quot;.</p>
<p>What is the effect of <span class="math inline">C</span>? If <span class="math inline">C</span> is infinity, we simply get the hard-margin SVM, since the dual formulations become identical. If <span class="math inline">C</span> is 0, <span class="math inline">\vec \alpha = 0</span> and we get <span class="math inline">\vec w = \vec 0</span> and an undefined <span class="math inline">b</span>. Note that the only difference between the Lagrangian dual of hard-margin and soft-margin SVM is that the value of <span class="math inline">\alpha</span> has the upper bound <span class="math inline">C</span> - a very elegant change.</p>
<p>In general, to take the Lagrangian, we rewrite every constraint <span class="math inline">i</span> in the form <span class="math inline">f_i(\ldots) \le 0</span> (for a minimization primal problem) or <span class="math inline">f_i(\ldots) \ge 0</span> (for a maximization primal problem), and then add <span class="math inline">\alpha_i f_i(\ldots)</span> to the objective function. Then, we add a maximizer (for a minimization primal problem) or minimizer (for a maximization primal problem) before the original minimizer/maximizer, swap the new minimizer/maximizer with the original using the minimax theorem. We then take the derivative of the original minimizer/maximizer's value with respect to each of the original minimizer/maximizer variables, set the derivatives to 0, and solve to get a closed-form solution to the original minimizer/maximizer, in terms of <span class="math inline">\alpha</span>. We then substitute that solution for the original minimizer/maximizer to get the Langrangian dual.</p>
<h1 id="section-9">12/10/17</h1>
<p>Clearly, <span class="math inline">P(\sgn(f(X)) \ne Y) = E[1_{-y f(X) \ge 0}]</span> (<span class="math inline">X</span> and <span class="math inline">Y</span> are random variables from unknown distibution) when we're using zero-one hinge loss. Since we can't minimize <span class="math inline">E[1_{-Y f(X) \ge 0}]</span> directly, we use <span class="math inline">\frac 1 n \sum_i 1_{-y_i \hat y_i \ge 0}</span> instead, where <span class="math inline">y_i</span> are the training set Y values and <span class="math inline">\hat y_i</span> are the training set predictions. We're actually interested in the zero-one loss, but we have to use the hinge loss in practice due to needing a good gradient.</p>
<p>According to the Bayes rule, <span class="math inline">\argmin_a E[\ell(Ya) \mid X = x]</span> has the same sign as <span class="math inline">2 \eta(x) - 1</span>. SVM tries to estimate the sign of <span class="math inline">2 \eta(x) - 1</span>, rather than the value of <span class="math inline">2 \eta(x) - 1</span> instead, which is harder because <span class="math inline">\eta(x)</span> is hard to find.</p>
<p>The <strong>KKT conditions</strong>: given primal constraints on <span class="math inline">\vec w, b, \xi</span>, <span class="math inline">(1 - y_i \hat y_i)_+ \le \xi_i</span> and dual constraints <span class="math inline">\alpha \ge 0, \beta \le 0</span>, we want to combine the constraints from both problems. the KKT conditions are the primary constraints, dual constraints, complementary slackness constraints, and stationarity constraints.</p>
<p>The complementary slackness constraints are <span class="math inline">\alpha_i(1 - y_i \hat y_i - \xi_i) = 0</span> and <span class="math inline">\beta_i \xi_i = 0</span>. Essentially, we added variables such that at the optimal values of the Lagrangian maximizer, the complementary slackness constraints are satisfied.</p>
<p>;wip: do partial derivatives: from this we get <span class="math inline">\vec w = \sum \alpha_i y_i \vec x_i</span>. If <span class="math inline">\alpha_i &lt; C</span>, then <span class="math inline">\xi_i = 0</span>, so <span class="math inline">y_i \hat y_i \ge 1</span>, so <span class="math inline">\vec x_i</span> is not within the margin of the hyperplane. If <span class="math inline">\alpha_i &gt; 0</span>, then <span class="math inline">1 - y_i \hat y_i - \xi_i = 0</span> and <span class="math inline">y_i \hat y_i = 1 - \xi_i \le 1</span>, so <span class="math inline">\vec x_i</span> is within the margin of the hyperplane.</p>
<p>Note that the converses are not necessarily true, so it is possible for <span class="math inline">\vec x_i</span> to be on the hyperplane while <span class="math inline">\alpha_i</span> is 0 - <strong>points can be on the hyperplane without actually being support vectors</strong>. Most people will define support vectors to be those that have non-zero <span class="math inline">\alpha_i</span>. For example, this might occur if we have more than enough</p>
<p>In summary: if <span class="math inline">\alpha_i = 0</span>, point is not on hyperplane or within margin. If <span class="math inline">0 &lt; \alpha_i &lt; C</span>, point is on one of the margin's two hyperplanes. If <span class="math inline">\alpha_i = C</span>, point is between margin's two hyperplanes.</p>
<p>To recover <span class="math inline">\vec w, b</span> from <span class="math inline">\alpha_i</span>: we can get <span class="math inline">\vec w</span> using the <span class="math inline">\vec w = \sum \alpha_i \xi_i \vec x_i</span> formula, Take any <span class="math inline">i</span> such that <span class="math inline">C &gt; \alpha_i &gt; 0</span> (these points don't always exist, because for small values of <span class="math inline">C</span> we might be able to shift the hyperplane without changing the objective value function), so <span class="math inline">\vec x_i</span> is on the hyperplane and <span class="math inline">1 - y_i \hat y_i = 0</span>, then use this to solve for <span class="math inline">b</span>. We can also recover <span class="math inline">\xi_i</span> after this by setting <span class="math inline">(1 - y_i \hat y_i)_+ = \xi_i</span>, since those values must be equal at the optimal value.</p>
<p>The primal problem finds the maximum margin, while the dual problem finds the minimum distance between convex hulls.</p>
<p>We can also just solve the primal problem using gradient descent: <span class="math inline">\vec w_{t + 1} = \vec w_t - \eta_t \Delta \ell(\vec w_t)</span>, where <span class="math inline">\eta_t</span> is the learning rate. There's also <strong>stachastic gradient descent</strong>, where we only compute the gradient for a randomly chosen point,</p>
<p>Recall that the hinge loss is <span class="math inline">\ell(x) = \max(1 - x, 0)</span>. Perceptron is more like <span class="math inline">\ell(x) = \max(-x, 0)</span>, which is not classification calibrated - if we have non-linearly-separable data, perceptron is not good for this use case.</p>
<p>In the early days, we didn't know how to solve the soft-margin SVM dual. Initially, we just dropped the <span class="math inline">\vec \alpha \cdot \vec y = 0</span> constraint, which doesn't give us the optimal hyperplane but gives a deccently good result. Modern algorithms exist that can derive a closed-form update for pairs of <span class="math inline">\alpha_p, \alpha_q</span>, sort of like how we optimized one element at a time for the lasso algorithm in assignment 1.</p>
<p>Gradient descent was first mentioned by Cauchy in 1847, and proven to converge by Haskell Curry in 1944. Stochastic Gradient Descent was proposed by Robbins and Monro in 1951,</p>
<p>For multiclass, we have new SVM formulations, such as this one: <span class="math inline">\min_W \frac 1 2 \magn{W}_F^2</span> such that <span class="math inline">\forall i, forall k \ne y_i</span>, <span class="math inline">\vec i \cdot \vec w_{y_i} \ge 1 + \vec x_i \cdot \vec w_k</span>. Essentially, <span class="math inline">\vec i \cdot \vec w_{y_i} \ge 1 + \vec x_i \cdot \vec w_k</span> is saying that we want the prediction for the correct class to be stronger than 1 plus the prediction for each wrong class.</p>
<p>Unlike perceptron, we didn't have to reduce it to binary problems first. The soft-margin version is similar, and requires a lot more calibration theory to understand.</p>
<p>SVM can also be used for regression, my modifying the formula a bit: <span class="math inline">\min_{\vec w, b} \frac 1 2 \magn{\vec w}^2 + C \sum (\abs{y - \hat y} - \epsilon}_+</span>, using the epsilon-insensitive loss function <span class="math inline">(\abs{y - \hat y} - \epsilon}_+</span>, which doesn't penalize small mistakes.</p>
<p>We can also train SVM in parallel, by partitioning the data, training the SVM on each machine, compute the center of each training set partition, and for each test data point, find the nearest center, and use the corresponding SVM to make the prediction. This is sort of like <span class="math inline">k</span>-nearest-neighbors but with the prediction augmented by SVM</p>
<h1 id="section-10">17/10/17</h1>
<p>Final exam on December 15, 12:30pm-3pm. Project proposals due tonight.</p>
<p>Recall the XOR problem - no linear classifier can learn to classify points the same way as the XOR function <span class="math inline">y = \prod \sgn(x_i)</span> for any <span class="math inline">\vec x</span>.</p>
<p>One way we might try to get around this is to use a quadratic classifier: <span class="math inline">\hat y = \sgn(\vec x^T Q \vec x + \sqrt 2 \vec x^T \vec p + \gamma)</span>. We only need to learn the weights <span class="math inline">Q \in \mb{R}^{d \times d}, \vec p \in \mb{R}^d, \gamma \in \mb{R}</span> to train this.</p>
<p>To make training simpler, we're going to map <span class="math inline">\vec x</span> into a simpler space. Let <span class="math inline">\phi(\vec x) = \begin{bmatrix} x_1 \vec x \\ \vdots \\ x_n \vec x \\ \sqrt 2 \vec x \\ 1 \end{bmatrix}</span>, called the <strong>feature transform</strong>. Let <span class="math inline">\vec w = \begin{bmatrix} Q \\ \vec p \\ \gamma \end{bmatrix}</span>. Both of these are square matrices of side length <span class="math inline">d^2 + d + 1</span>. Now, note that <span class="math inline">\hat y = (\vec x^T Q \vec x + \sqrt 2 \vec x^T \vec p + \gamma \ge 0)</span> is equivalent to <span class="math inline">\vec w \cdot \phi(\vec x) \ge 0</span> - we've converted the quadratic classification problem into a linear classifier in a different space!</p>
<p>In fact, most classifiers can be replaced by a linear one. It's a technique known as <strong>lifting</strong> - when the data isn't linearly separable, we can transform the feature vectors into a different space, then using linear classifiers on that other space, since linear classifiers are simple to train and are well-understood. This is very powerful, because linear classifiers are well-understood, and we don't need to study any nonlinear ones.</p>
<p>For example, we might train an ellipsis classifier <span class="math inline">\hat y = \sgn((w_1 x_1)^2 + (w_2 x_2)^2 - r^2)</span> by lifting it into the space <span class="math inline">\phi(x) = \begin{bmatrix} x_1^2 \\ x_2^2 \end{bmatrix}</span>.</p>
<p>The <strong>curse of dimensionality</strong> is the fact that lifting will often dramatically increase the number of dimensions, which makes algorithms like SVM and similar a lot harder. However, note the Lagrangian dual of hard-margin and soft-margin SVM only depends on the dot product of two points. Additionally, <span class="math inline">\phi(\vec x_1) \cdot \phi(\vec x_2) = (\vec x_1 \cdot \vec x_2)^2 + 2\vec x_1 \cdot \vec x_2 + 1 = (\vec x_1 \cdot \vec x_2 + 1)^2</span> - the <span class="math inline">d^2 + d + 1</span>-dimensional dot product can actually be computed using only a <span class="math inline">d</span>-dimensional dot product!</p>
<p>When doing lifting, we were finding <span class="math inline">\phi(\vec x)</span> and then often computing <span class="math inline">\phi(x_1) \cdot \phi(x_2)</span> in algorithms like SVM. Instead of defining a <span class="math inline">\phi</span> and then computing the dot product, we can define a <strong>reproducing kernel function</strong> <span class="math inline">\kappa: \mb{R}^d \times \mb{R}^d \to \mb{R}</span>, usually just called a <strong>kernel</strong>, such that <span class="math inline">\kappa(\vec x_1, \vec x_2) = \phi(\vec x_1) \cdot \phi(\vec x_2)</span>.</p>
<p>As it turns out, it's often a lot easier and efficient to compute <span class="math inline">\kappa</span> than <span class="math inline">\phi</span> - as long as <span class="math inline">\kappa</span> can be computed efficiently, we don't need to care how many dimensions its corresponding <span class="math inline">\phi</span> has, or even whether it can be computed at all. In fact, kernels don't even have to be over vectors - we can define kernels over anything, and the kernels just need to measure the similarity between two objects.</p>
<p>Some common kernels are:</p>
<ul>
<li>Polynomial: <span class="math inline">\kappa(\vec x_1, \vec x_2) = (\vec x_1 \cdot \vec x_2)^p</span> or <span class="math inline">\kappa(\vec x_1, \vec x_2) = (\vec x_1 \cdot \vec x_2 + 1)^p</span> - this is usually used for polynomial regression/classification.</li>
<li>Gaussian: <span class="math inline">\kappa(\vec x_1, \vec x_2) = \exp(-\magn{\vec x_1 - \vec x_2}^2 / \sigma)</span> - this uses the density function of a Gaussian distribution, and is radially symmetric. It doesn't work that well in practice, but people often use it because it's convenient.</li>
<li>Laplace: <span class="math inline">\kappa(\vec x_1, \vec x_2) = \exp(-\magn{\vec x_1 - \vec x_2} / \sigma)</span>.</li>
<li>Matern: <span class="math inline">\kappa(\vec x_1, \vec x_2) = \frac{1}{2^{v - 1} \Gamma(v)} (2 \sqrt{v} \magn{\vec x_1 - \vec x_2} / \theta)^v H_v(2 \sqrt{v} \magn{\vec x_1 - \vec x_2} / \theta)</span> - this is sort of like an interpolation between Gaussian and Laplace, and is seeing a lot of use in modern ML research.</li>
</ul>
<p>We can show that any function is a kernel simply by constructing a <span class="math inline">\phi</span>, but this is often difficult. We can also use the kernel matrix:</p>
<p>Given a dataset <span class="math inline">\set{\vec x_1, \ldots, \vec x_n}</span>, the <strong>kernel matrix</strong> of a kernel is a matrix <span class="math inline">K_{ij} = \kappa(\vec x_i, \vec x_j)</span>. As it turns out, <span class="math inline">\kappa</span> is a kernel if and only if <span class="math inline">K</span> is symmetric and positive semidefinite (<span class="math inline">K \in \mb{S}_+^d</span>) for any dataset. In other words, we have the following properties:</p>
<ul>
<li>Symmetry: <span class="math inline">K_i = K_j</span>. This is needed because if A is similar to B, then B should be equally similar to A.</li>
<li>Positive semidefinite: for all <span class="math inline">\vec \alpha \in \mb{R}^n</span>, <span class="math inline">\vec \alpha^T K \vec \alpha \ge 0</span> (i.e., <span class="math inline">\sum_i \sum_j \alpha_i \alpha_j K_{i, j} \ge 0</span>).</li>
</ul>
<p>In other words, the kernel matrix is symmetric and positive semidefinite if and only if there exists a <span class="math inline">\phi(\vec x)</span> such that <span class="math inline">\kappa(\vec x_1, \vec x_2) = \phi(\vec x_1) \cdot \phi(\vec x_2)</span>.</p>
<p>Given reproducing kernel functions <span class="math inline">\kappa_1, \kappa_2</span> and a constant <span class="math inline">a \ge 0</span>, <span class="math inline">a\kappa_1</span> and <span class="math inline">\kappa_1 + \kappa_2</span> and <span class="math inline">\kappa_1 \kappa_2</span> are all kernels. In other words, kernels are closed under linear combinations.</p>
<p>Consider now the Lagrangian dual of soft-margin SVM with a kernel: <span class="math inline">\min_{C \ge \alpha \ge 0} \frac 1 2 \sum_i \sum_j \alpha_i \alpha_j y_i y_j K_{i, j} - \sum \alpha_i</span> subject to <span class="math inline">\sum \alpha_i y_i = 0</span>. ;wip: soft-margin or hard-margin?</p>
<p>Now we'd managed to use the the kernel without having to compute <span class="math inline">\phi(\vec x)</span>, but to recover the weights we still have to do <span class="math inline">\vec w = \sum \alpha_i y_i \phi(\vec x_i)</span>. However, <span class="math inline">\phi(\vec x)</span> might be infeasible to compute. ;wip</p>
<p>Note that for SVM, <span class="math inline">\vec w \cdot \phi(\vec x) = \sum \alpha_i y_i \phi(\vec x_i) \cdot \phi(\vec x) = \sum \alpha_i y_i \kappa(\vec x_i, \vec x)</span>. Since the classification result is <span class="math inline">\hat y = \sgn(\vec w \cdot \phi(\vec x))</span>, we then have <span class="math inline">\hat y = \sum \alpha_i y_i \kappa(\vec x_i, \vec x)</span>, which is much more feasible to compute. Note that we're never explicitly computing <span class="math inline">\phi</span>, only the dot product of two <span class="math inline">\phi</span> values.</p>
<p>So previously training cost <span class="math inline">O(nd)</span> and testing took <span class="math inline">O(n)</span>. With kernels, training takes <span class="math inline">O(n^2)</span> and testing takes <span class="math inline">O(n)</span>, where <span class="math inline">n</span> is the size of the dataset and <span class="math inline">d</span> is the number of dimensions in <span class="math inline">\phi(\vec x)</span>.</p>
<p>For example, for the XOR problem we might end up with weights <span class="math inline">\vec w = \begin{bmatrix} 0 \\ -\frac 1 \sqrt 2 \end{bmatrix}</span>, and <span class="math inline">\vec w \cdot \phi(\vec x) = -x_1 x_2</span> and <span class="math inline">\phi()</span>. ;wip: what</p>
<h1 id="section-11">19/10/17</h1>
<p>Overview of assignment 3. For the hyperparameters in this one, we don't have to use cross validation, since that would be really time consuming.</p>
<p>Since we don't know <span class="math inline">\phi</span>, we can't really make the test predictions. However, <span class="math inline">\vec w \cdot \phi(\vec x) = \sum \alpha_i y_i \phi(\vec x)</span></p>
<p>We also don't need to pick a kernel at the beginning - the kernel itself can be learned as well. We can start with a non-negative combination of <span class="math inline">t</span> pre-selected kernels, and learn the coefficients <span class="math inline">\zetta_s</span>. The kernel <span class="math inline">K_{i, j}</span> in the SVM dual formulation then becomes <span class="math inline">\sum_s{s = 1}^t \zetta_s K_{i, j}^{(s)}</span>. We also often add a small regularization term to the expression to ensure we select as few kernels as possible ;wip: full SVM formulation with this, from slides</p>
<p>Recall logistic regression, <span class="math inline">\min_{\vec w \in \mb{R}^d} \sum_i \ln(1 + \exp(-y_i \vec w \cdot \vec x_i)) + \lambda \magn{\vec w}^2</span>. Finding the Lagrangian dual of this is difficult, so we want to try something else. Consider <span class="math inline">\vec w \cdot \vec x_i</span> in the logistic regression's expression. Let <span class="math inline">X = \span \set{\vec x_1, \ldots, \vec x_n}</span> - every linear combination of the training set. Let <span class="math inline">\vec w = \vec w_1 + \vec w_1&#39;</span> where we decompose <span class="math inline">\vec w</span> into two vectors, of which <span class="math inline">w_1</span> is entirely in the subspace <span class="math inline">X</span>, while <span class="math inline">w_1&#39;</span> is entirely in the orthogonal complement of <span class="math inline">X</span>. ;wip: <span class="math inline">w_1&#39;</span> is the orthogonal complement</p>
<p>Now, $W x_i = $. Also, since <span class="math inline">w_1</span> and <span class="math inline">w_1&#39;</span> are in subspaces that are orthogonal to each other, <span class="math inline">\magn{w}^2 = \magn{\vec w_1}^2 + \magn{\vec w_1&#39;}</span>. Also <span class="math inline">\vec w \cdot \vec x_i = \vec w_1 \cdot \vec x_1</span>, because ;wip: what</p>
<p>Now we'll map values of <span class="math inline">\vec x_i</span> into a new space. Let <span class="math inline">\phi(\vec x_i)</span> be an arbitrary transformation of the linear space for the</p>
<p>Now we have <span class="math inline">\min_{\vec w \in \mb{R}^h} \sum \ln(1 + \exp(-y_i \vec w \cdot \phi(\vec x_i))) + \lambda \magn{\vec w}^2</span>.</p>
<p>According to the <strong>representer theorem</strong>, the optimal <span class="math inline">\vec w</span> is of the form <span class="math inline">\sum \alpha_i \phi(\vec x_i)</span>. If we substitude this into the logistic regression formula and simplifying a bit, we get the new primal problem <span class="math inline">\min_{\alpha \in \mb{R}^n} \sum \ln(1 + \exp(-y_i \vec \alpha \cdot K_{:i})) + \lambda \vec \alpha^T K \vec \alpha</span> ;wip: do the full derication for this</p>
<p>In the general form (not just for the training set), we have <span class="math inline">\min_{\alpha \in \mb{R}^d} \sum \ln(1 + \exp(-y_i \vec \alpha \cdot K_{:i})) + \lambda \vec w</span>. ;wip: what's the rest of this formula</p>
<p>We'll solve this using Newton's method - <span class="math inline">\vec \alpha_{t + 1} = \vec \alpha_t - \eta_n \frac 1 {\Delta^2 f(\vec \alpha_t)} \Delta f(\vec \alpha_t)</span> where <span class="math inline">\Delta^2</span> is the second order gradient and <span class="math inline">\Delta</span> is the first order gradient. Clearly, <span class="math inline">\Delta</span>^2 f(<em>t) p_i (1 - p_i) K</em>{:i} K_{:i}^T$ where <span class="math inline">p_i = \frac{1}{1 + \exp(-\vec \alpha_t \codt K_{:1})}</span> (the probability of the <span class="math inline">i</span>th training set point being in class 1). Note that <span class="math inline">p_i(1 - p_i)</span> is sort of like a weight determining how &quot;unsure&quot; we are about the category for the point, since it's maximized when <span class="math inline">p_i = 0.5</span> (we have no idea which class it is in either way), so as we do more iterations, we seem to assign more weight to the points we are unsure about the categories for.</p>
<p>;wip: rest of newton's method, specifically defining the derivative values</p>
<p>The Hessian is the second order derivative, and gives uncertain points a higher weight ;wip: is this true? what is a hessian?</p>
<p>A kernel is <strong>universal</strong> if and only if for any compact set <span class="math inline">Z</span>, continuous function <span class="math inline">f: Z \to \mb{R}</span>, <span class="math inline">\epsilon &gt; 0</span>, there exists <span class="math inline">x_1, \ldots, x_n \in Z</span> and <span class="math inline">\alpha_1, \ldots, \alpha_n \in \mb{R}</span> such that <span class="math inline">\max_{x \in Z} \abs{f(x) - \sum \alpha_i \kappa(x, x_i)} \le \epsilon</span>. Essentially, this says that if you choose a universal kernel, we can approximate any continuous function, which means kernel methods plus linear classifiers/regressions can learn any continuous function. This also tells us that neural networks can learn any function if they have at least 2 layers, which has sparked a lot of interest in neural networks lately.</p>
<p>One example of a universal kernel is the Gaussian kernel <span class="math inline">\kappa(x, x&#39;) = \exp(-\magn{\vec x - \vec x&#39;}^2 / \sigma)</span>. Even though it performs poorly in practice, this is one reason we often use it, because it's theoretically guaranteed to work.</p>
<p>Recall the Gaussian distribution, with probability density function <span class="math inline">\frac{\dee}{\dee x} P(X \le x) = \frac{1}{\sqrt{2 \pi} \sigma} \exp(-\frac{(x - \mu)^2}{2 \sigma^2})</span>. Now, if <span class="math inline">x</span> was a vector instead, we get <span class="math inline">\frac{\dee}{\dee \vec x} P(\vec X \le \vec x) = \frac{1}{(2 \pi)^{d/2}} \abs{\Sigma}^{-1/2} \exp(-\frac 1 2 (\vec x - \vec \mu)^T \Sigma^{-1} (\vec x - \vec \mu))</span>, where <span class="math inline">\vec X \sim N_d(\vec \mu, \Sigma)</span>. ;wip: is actually a covariance matrix? the kernel is the covariance matrix of the Gaussian random variables?</p>
<p>Let <span class="math inline">A \in \,b{R}^{p \times d}</span>, then <span class="math inline">AX \sim N_d(A \vec \mu, A \Sigma A^T)</span>. Likewise, <span class="math inline">\begin{bmatrix} X_1 \\ X_2 \end{bmatrix} \sim N(\begin{bmatrix} \vec \mu_1 \\ \vec \mu_2 \end{bmatrix}, \begin{bmatrix} \Sigma_{1, 1} &amp; \Sigma_{1, 2} \\ \Sigma_{2, 1} &amp; \Sigma_{2, 2} \end{bmatrix})</span>. In other words, for any vector random variable <span class="math inline">\vec X</span>, we can partition it into two vectors <span class="math inline">\vec X_1, \vec X_2</span>, and the joint distibution between <span class="math inline">X_1</span> and <span class="math inline">X_2</span> can be broken up into the marginal distributions and the conditional distributions of <span class="math inline">X_1</span> and <span class="math inline">X_2</span>.</p>
<p>So <span class="math inline">X_1 \sim N(\vec \mu_1, \Sigma_{1, 1})</span>, and <span class="math inline">X_2 \mid X_1 \sim N(\vec \mu_2, \Sigma_{2, 1} \Sigma_{1, 1}^{-1} (\vec X_1 - \vec \mu_1), \Sigma_{2, 2}^{-1} - \Sigma{2, 1} \Sigma_{1, 1}^{-1} \Sigma_{1, 2})</span>. If we know <span class="math inline">X_1</span>, then we get a bit of information about <span class="math inline">X_2</span>, and that previous formula describes precisely what information.</p>
<p>A random variable is a function <span class="math inline">Z(\omega)</span>. If we roll a die, we see the result of <span class="math inline">Z(\omega)</span>, but not <span class="math inline">\omega</span> itself.</p>
<p>A <strong>Gaussian process</strong> is just a collection of Gaussian random variables <span class="math inline">G = \set{Z_t : t \in T}</span> such that any finite subset of <span class="math inline">G</span> is jointly Gaussian (for infinite subsets, there simply can't be a joint Gaussian distribution over infinite things). A Gaussian process is a function of two variables <span class="math inline">Z(t, \omega)</span>. For any finite <span class="math inline">N \subseteq T</span>, <span class="math inline">\set{Z_t = Z(t, \omega) \mid t \in N}</span> is a Guassian random vector, and for any fixed <span class="math inline">\omega</span>, <span class="math inline">Z(t)</span> is a path. Each Gaussian random variable can have its own <span class="math inline">\mu</span> and <span class="math inline">\sigma</span>. We can refer to each <span class="math inline">Z_t</span>'s parameters using the mean function <span class="math inline">\mu(t)</span> and sigma function <span class="math inline">\sigma(t)</span>, so <span class="math inline">Z_t \sim N(\mu(t), \sigma(t))</span>.</p>
<p>Usually, we prove that something is a Gaussian random process by proving that each <span class="math inline">Z_t</span> is Gaussian.</p>
<p>;wip: what do gaussian processes this have to do with kernels? seems like the kernel function is a gausian process</p>
<h1 id="section-12">24/10/17</h1>
<p>Assignment 3 due Tuesday, 1 week from now. Some useful sample code is available on the website.</p>
<p>Demonstration of Gaussian random processes. Sampling many <span class="math inline">t</span> values for a fixed <span class="math inline">\omega</span> value yields a really spiky curve for Laplace kernels (in fact, not differentiable anywhere), while a Guassian kernel gives a nice smooth curve. Demonstration of how changing Gaussian kernel parameters like <span class="math inline">\sigma</span> and <span class="math inline">\mu</span> affect the Gaussian random vectors drawn from the kernel.</p>
<p>If we sample <span class="math inline">Z(t, \omega)</span> for a fixed <span class="math inline">\omega</span> at many different <span class="math inline">t</span> values, we get a Gaussian random vector. If we do this for many different <span class="math inline">\omega</span> values and add them together, we get the mean of each Gaussian distribution.</p>
<p>The type of kernel we use depends on the application. For example, in financial models we might use a Laplace kernel to handle the spikiness, and in robotics we might use a gaussian kernel since it's smoother.</p>
<p>We initially introduced kernels as a feature transformation. We then looked at it as a bunch of random variables.</p>
<p>Imagine we have a problem where we want to learn a curve, and nature will pick a random curve from a fixed <span class="math inline">\omega</span> on <span class="math inline">Z(t, \omega)</span>. Our task would then be to figure out the parameters to the Gaussian process, such as <span class="math inline">\mu(t)</span>, so we can predict future samples. For example, we can think of stock prices as a sample of many <span class="math inline">t</span> values from <span class="math inline">Z(t, \omega)</span> where <span class="math inline">\omega</span> is fixed, and our task would be to predict future stock prices by learning the Gaussian process parameters.</p>
<p>How do we do this inference? Recall that <span class="math inline">\begin{bmatrix} X_1 \\ X_2 \end{bmatrix} \sim N(\begin{bmatrix} \vec \mu_1 \\ \vec \mu_2 \end{bmatrix}, \begin{bmatrix} \Sigma_{1, 1} &amp; \Sigma_{1, 2} \\ \Sigma_{2, 1} &amp; \Sigma_{2, 2} \end{bmatrix})</span>. Recall that the covariance function is a kernel.</p>
<p>Recall that linear regression is <span class="math inline">Y = \vec x \cdot \vec w + \vec \epsilon</span>, where <span class="math inline">\vec w</span> is deterministic and <span class="math inline">\epsilon</span> is Gaussian noise <span class="math inline">N(0, \sigma)</span>. Here, <span class="math inline">Y</span> is a Gaussian process where <span class="math inline">\vec x</span> is <span class="math inline">t</span> - <span class="math inline">Y \sim GP(\vec w \cdot \vec x, \kappa(\vec x, \vec x&#39;))</span>.</p>
<p>Let's verify that <span class="math inline">Y</span> is a Gaussian process. To do this, we will show that it gives a Gaussian random vector for any <span class="math inline">t</span>. Clearly, <span class="math inline">\begin{bmatrix} \vec w \\ \vec \epsilon \end{bmatrix}</span> is a Gaussian random vector (even though <span class="math inline">\vec w</span> is deterministic, it's a Gaussian random variable with 0 variance), so <span class="math inline">\begin{bmatrix} \vec x &amp; I \end{bmatrix} \begin{bmatrix} \vec w \\ \vec \epsilon \end{bmatrix}</span> is also a Gaussian random vector, since the matrix represents a linear transform, and any linear transform of a Gaussian random vector is also a Gaussian random vector.</p>
<p>Since <span class="math inline">Y(\vec x)</span> is always Gaussian random vector, any finite number of <span class="math inline">Y(\vec x)</span> values is also jointly Gaussian, and so must be a Gaussian process.</p>
<p>The generalized gaussian kernel <span class="math inline">\exp(-\magn{\vec x - \vec x&#39;}^t / \sigma)</span> where <span class="math inline">1 \le t \le 2</span>.</p>
<p>We find <span class="math inline">\vec w</span> using the method of maximum likelihood. Clearly, <span class="math inline">\max_{\vec w} P(\tup{\vec x_1, y_0} \land \ldots \land \tup{\vec x_1, y_0})</span>. As it turns out, if we write this out, we get <span class="math inline">\min_{\vec w} \magn{\vec x - \overline \vec x}^2</span>, which is the least squares linear regression. In other words, the maximum likelihood estimator for a Gaussian process is least-squares regression. ;wip: rest of formulas</p>
<p>Bayesian linear regression is a generalization of Gaussian linear regression, where <span class="math inline">Y = \vec x \cdot \vec W + \vec \epsilon</span> where <span class="math inline">\vec W \sim N(\vec \mu, \Sigma)</span> and <span class="math inline">\vec \epsilon \sim N(0, \sigma^2)</span>.</p>
<p>For this we want to use the maximum a posteriori method to find <span class="math inline">\vec W</span>. Essentially, we're maximizing <span class="math inline">\max_{\vec W} P(\vec w \mid \tup{\vec x_1, y_0} \land \ldots \land \tup{\vec x_1, y_0})</span>, the posterior probability, which by Bayes rule can be written as <span class="math inline">\frac{P(\tup{\vec x_1, y_0} \land \ldots \land \tup{\vec x_1, y_0} \mid \vec W) P(\vec W)}{P(\tup{\vec x_1, y_0} \land \ldots \land \tup{\vec x_1, y_0})}</span>. Note that <span class="math inline">P(\vec W)</span> is the prior - if we have <span class="math inline">\vec W \sim N(0, 1)</span> for our prior we get ridge regression, while if we have <span class="math inline">\vec W \sim \mathrm{Laplace}(0, 1)</span> we get Lasso regression.</p>
<p>;wip: everything from Abstract View slide</p>
<p>While we could think of a kernel as a dot product of a feature transformation, we can also think of a kernel as <span class="math inline">Z = \phi(\vec t) \cdot \vec W + \mu(t)</span> where <span class="math inline">\vec W \sim N(\vec 0, I)</span> and <span class="math inline">\mu(t) = E(\phi(t) \cdot \vec W + m(\vec t))</span>. Also, <span class="math inline">\kappa(s, t) = E(\phi(t))</span></p>
<p>In other words, a kernel is a linear regression problem in the feature space with a Gaussian distributed weight vector.</p>
<p>;wip: rest of class missed due to interview</p>
<h1 id="section-13">26/10/17</h1>
<p>Some notes about assignment 3: suppose we have a sequence of kernels <span class="math inline">k_1, k_2, \ldots</span>. What is the limit of those kernels <span class="math inline">\kappa(\vec s, \vec t) = \lim_{n \to \infty} k_n</span>?</p>
<p><span class="math inline">\kappa(\vec s, \vec t)</span> is a function of two vectors. Usually we compute this pointwise - we take a given <span class="math inline">\vec s, \vec t</span> and compute the limit on that.</p>
<p>Note that <span class="math inline">\kappa(\vec s, \vec t) = \kappa(\vec t, \vec s)</span> since all kernels are symmetric.</p>
<p>Recall that the density function for the multivariate gaussian distribution is <span class="math inline">P(\vec X = x) = \frac 1 {(2 \pi)^d} \abs{\Sigma}</span> ;wip: rest of slide</p>
<p>Consider a plot of people's heights in a typical population - we would have a bimodal distribution, one for men and one for women. To model this, we would need to use two Gaussian distributions.</p>
<p>To represent this, we can use a <strong>mixture of Gaussians</strong> here <span class="math inline">P(x \mid \theta) = \sum_{0 \le k \le K} P(z = k) P(x \mid z = k \land \theta)</span>. Here, <span class="math inline">z</span> is the person's sex, <span class="math inline">x</span> is the height, and <span class="math inline">\theta</span> are the remaining parameters. So <span class="math inline">P(z = k)</span> means the probability of a person having a particular sex, and once we know which class the person is in, we can say it's in that class's Gaussian distribution <span class="math inline">P(x \mid z = k \land \theta)</span> - each class can have its own Gaussian distribution.</p>
<p>Let <span class="math inline">\pi_k = P(z = k)</span>, so <span class="math inline">P(x \mid \theta) = \sum_{0 \le k \le K} \pi_k P(x \mid z = k \land \theta)</span>. Note that <span class="math inline">\pi_k \ge 0</span> and <span class="math inline">\sum \pi_k = 1</span> since they're just probabilities.</p>
<p>We can think of <span class="math inline">\pi_k</span> as the <strong>mixing distribution</strong>, and <span class="math inline">P(x \mid z = k \land \theta)</span> as the <strong>component distributions</strong> (a kernel for a particular component).</p>
<p>A <strong>Gaussian mixture model</strong> is <span class="math inline">P(x \mid \theta) = \sum_{1 \le k \le K} \pi_k N(x \mid \mu_k \land \Sigma_k)</span>. This lets us mix a bunch of kernels together when different classes require different kernels - each class <span class="math inline">k</span> has its own <span class="math inline">\mu_k</span> and <span class="math inline">\Sigma_k</span> to learn.</p>
<p>Gaussian mixture models with sufficiently many components (<span class="math inline">k</span> is large enough) can approximate any probability density function on <span class="math inline">\mb{R}^d</span>. This works because given any density function, we can simply keep adding Gaussian distributions with small <span class="math inline">\Sigma</span> values until we've created the shape of the density function, and this actually works for many different distributions, like student-T or chi-squared.</p>
<p>Note that for a Gaussian mixture model, we only have <span class="math inline">x</span>, not <span class="math inline">y</span>, so it's sort of unsupervised learning. There's a variation of this called <strong>mixture of experts</strong>, which is <span class="math inline">P(y \mid x \land \theta) = \sum_{1 \le k \le K} P(y \land z = k \mid x \land \theta) = \sum_{1 \le k \le K} P(z = k \mid x \land \theta) P(y \mid x \land z = k \land \theta)</span>. We write it in the last form because it allows us to separate the mixing and component distributions. Also, <span class="math inline">P(y \mid x \land z = k \land \theta)</span> can be <span class="math inline">N(y \mid \vec w_k \cdot \vec x, \sigma_k^2)</span>, which is just linear regression if we try to learn <span class="math inline">\vec w_k</span>.</p>
<p>Mixture models allow us to essentially use different kernels whenever they work better. The mixing distribution can be trained to learn when to use each component distribution, and the component distribution can just be trained for where it'll perform best. This is sort of analogous to a neural network, where some neurons can turn entire parts of the network on and off.</p>
<p>Interestingly, for Gaussian mixture models, the factorization is always unique - for any given <span class="math inline">P(x \mid \theta)</span>, there is only one <span class="math inline">k</span> such that <span class="math inline">P(x \mid \theta) = \sum_{1 \le k \le K} P((z = k \mid x) \land \theta) P(y \mid x \land z = k \land \theta)</span>.</p>
<p>Mixture models are heavily used in boosting, where we take a bunch of okay models and turn them into a single good model.</p>
<p>Crucially, we only know the datapoint <span class="math inline">x</span>, not the component <span class="math inline">z</span> (we call <span class="math inline">z</span> the <strong>latent variable</strong>). The problem setup is as follows: we're given independent and identically distributed <span class="math inline">X_1, X_2, \ldots \sim P(x \mid \theta)</span>, and we want to estimate <span class="math inline">\theta</span> or <span class="math inline">z</span>. Finding the maximum likelihood estimate is NP-hard, but we can use the variational form of maximum likelihood instead to make this feasible.</p>
<p>If <span class="math inline">L(\theta)</span> is the likelihood function, then <span class="math inline">\min L(\theta) = \sum -\ln P(x_i \mid \theta) = \sum -\ln \left(\sum P(x_i \land (z_i \mid \theta))\right)</span>. Computing this is NP-complete, but we can construct a sequence that converges to a good approximation, called the <strong>expectation maximization algorithm</strong> (also, it doesn't necessarily give a global optimum). As it turns out, <span class="math inline">\min L(\theta) = \min_{q_i(z_i), \theta} \sum_i (\sum_{j} q_i(z_j))</span> ;wip: rest of formula</p>
<p>If you fix <span class="math inline">q_i(z_i)</span>, we get <span class="math inline">\min_\theta -\sum_i \sum z_i q_i(z_i) \ln P()</span> ;wip; rest of slide</p>
<p>If we do this for a Gaussian mixture model, we get <span class="math inline">\pi_k = \frac 1 n \sum_i r_{i, k}</span> where <span class="math inline">r_{i, k} = P(z_i = k \mid (x_i \land \theta))</span> (the posterior likelihood, proportional to <span class="math inline">P(z_i = k) P(x_i \mid ;wip) \pi_k N(x_i \mid (\mu_k \land \Sigma_k))</span>).</p>
<p>One thing Gaussian mixture models are often used for is k-clustering, where we automatically find clusters and the number of clusters.</p>
<p>If we know the latent variable, we can also significantly simplify our mixture models. For example, we can approximate the L1 norm using a mixture of L2 norms. Since the L2 norms have a closed form solution, we can approximate the L1 norm using only a combination of closed form solutions to the L2 norms.</p>
<p>New notation: <span class="math inline">P(a \mid c_1, c_2, c_3) = P(a \mid (c_1 \land c_2 \land c_3))</span></p>
<h1 id="section-14">31/10/17</h1>
<p>Guest lecture by Aghastya from Focal Systems. This and the next four lectures are about deep learning.</p>
<h2 id="deep-learning">Deep Learning</h2>
<p>Loss/cost functions can be derived from the method of maximum likelihood - finding the input that maximizes the likelihood of getting the outcome that we did.</p>
<p>For supervised learning, we're trying to maximize <span class="math inline">P(y \mid x; \theta) = \prod P(y_i \mid x_i; \theta)</span>, but we're only allowed to change <span class="math inline">\theta</span>. This is equivalent to minimizing <span class="math inline">J(\theta) = -E_{x, y \sim \hat P_d} \ln P_m(y \mid x)</span>, where <span class="math inline">P_d</span> is the estimated distribution of the data, and <span class="math inline">P_m</span> is the distribution of the model.</p>
<p>Recall the regression problem: <span class="math inline">\hat y = W^T h + b</span> where <span class="math inline">h = f(x, \theta)</span>. Again, we're trying to maximize <span class="math inline">P(y \mid x)</span> by changing only <span class="math inline">\theta</span>.</p>
<p>;wip: deriving MSE from maximum likelihood</p>
<p>Once we have the MSE, we can minimize it using SGD.</p>
<p>The MSE has several interesting properties:</p>
<ul>
<li>The gradient of the MSE is linear with respect to <span class="math inline">x</span> since MSE is quadratic - gradients can explode</li>
<li>High gradient values can propagate throughout the neural network, while neural networks work best if the weights stay small</li>
<li>We often work around this by normalizing all feature vectors so their component is between 0 and 1 so the gradient stays under 1 - this helps the training converge in many cases.</li>
<li>MSE often has a lot of local minima, which means we will often get stuck in local minima</li>
<li>For the above reasons, we often use MSE for classification but not regression.</li>
</ul>
<p>For binary classification, we ;wip: read slide about binary classification, binary crossentropy is the loss function we usually use for binary classification, because P(y x) = y^n (1 - y)^{1 - n}</p>
<p>For multiclass classification, we ;wip: the next slide, softmax is the loss function we usually use for multiclass, derive that, also, the softmax uses exp to turn the log probabilities <span class="math inline">z_i, z_j</span> back into normal probabilities</p>
<p>;wip: softmax can't saturate, which is really good for SGD, prove that softmax will be approximately the same as max(exp(x_i)), but unlike that it is also smooth everywhere, which is useful for SGD - the softmax function encodes info about how to move in the gradient ;wip: z is the output from the hidden layer</p>
<p>;wip: for softmax, the prediction output is roughly <span class="math inline">z_i - \max_j z_j</span> - if the prediction is correct, that's <span class="math inline">z_i - z_i = 0</span>, if it's incorrect then <span class="math inline">z_i - z_j &gt; 0</span></p>
<p>In general, to get a cost function with this technique, we find the MLE, and try to minimize it as the loss function ;wip: do an example based on softmax</p>
<p>;wip: loss functions cheat sheet</p>
<h2 id="regularization">Regularization</h2>
<p>Intuitively, model bias is the size of the family of functions the model can represent - the smaller the family, the more biased the model is. For example, linear models have high bias, because they can only represent linear problems.</p>
<p>Intuitively, model variance is the number of model configurations that can solve the problem. For example, linear models have low variance because many of them just have a closed form.</p>
<p>Intuitively, model capacity is the number of functions the model can represent. Linear models have low capacity because they can only model linear functions, but deep neural nets have huge capacity - they can model almost any function (though there's high variance).</p>
<p>Error mainly comes from training error (how wrong we were on the training set), and generalization error (how much worse we did on the testing set than the training set - subtract training set error rate from testing set error rate).</p>
<p>A large model capacity reduces training error (reduces underfitting). A large variance increases generalization error (increases overfitting). Unfortunately, increasing capacity often also increases variance - there's generally a balance between variance.</p>
<p>The goal of regularization is to increase bias to decrease variance without removing too much capacity. For example, we can fit a set of <span class="math inline">n</span> points with a polynomial of degree <span class="math inline">n</span>, but it's probably not going to generalize to the testing set very well. Instead, we might ;wip</p>
<p>Regularization is <strong>any modification made to the learning algorithm intended to reduce generalization error, but not intended to reduce raining error</strong>. We usually use this on high capacity models where our training error is already small.</p>
<p>One common regularization technique is the L2 penalty, where we add <span class="math inline">\magn{\vec w}_2^2</span> to our minimizer objective function. Local constancy means that a small change in <span class="math inline">x</span> should result in a small change in <span class="math inline">y</span>. Many ML models assume local constancy, and the L2 penalty makes us prefer models that are more locally constant - if the weights are small, then the model output will change less with any change in <span class="math inline">x</span>.</p>
<p>Another common technique is the L1 penalty, where we add <span class="math inline">\magn{\vec w}_1</span>. Since the gradient of the L1 norm is just the sign of the input, this always tries to push weights toward 0. Therefore, the L1 penalty tries to make the model as sparse as possible.</p>
<p>While the L2 penalty tries to make all the weights be as evenly distributed as possible, the L1 penalty tries to force the weights to be 0 as often as possible.</p>
<p>Data augmentation is a very important technique for regularization. Essentially, we add mutated versions of the dataset to itself so the model has more training data to work with. For example, for ImageNet we might duplicate the dataset with rotated, flipped, and scaled versions of itself, so our model can then handle rotated/flipped/scaled versions of itself. Another example is a spelling corrector - we might intentionally introduce spelling errors into duplicated versions of the dataset, so our model can learn about the possible spelling mistakes.</p>
<p>;wip: slides after data augmentation</p>
<p>Multi-task training is also an important regularization technique - train the model to do many different tasks, so it has to be able to generalize to do well. This is rarely used, because it requires multiple labelled datasets, which is more expensive.</p>
<p>A very simple regularization technique is early stopping - checkpoint the model regularly, then after training is done choose the iteration count that minimizes cross-validation error. This is sort of like a hyperparameter search over time!</p>
<p>;wip: ensembles, will often give ~2% improvement on real-world competitions, really powerful if errors are independent</p>
<p>;wip: dropout, we conceptually train the neural network normally, then randomly delete some neurons with some probability (the dropout probability) to get a random subnetwork, then we take an ensemble of many different sampled subnetworks (we usually approx this by multplying all weights by dropout probability, this isn't formally proven to be a good approx but works well in practice) ;wip: essentially, dropout works by saying that no one neuron can be too important, since it might just get deleted in some networks - this encourages redundancy and therefore the ability to generalize ;wip: batch normalization and the rest of the slides ;wip: initialization is important, for example for reLU, we often initialize values randomly to between say 0.01 and 0.1, there's a lot of math to find proper initialization algorithms</p>
<h1 id="section-15">2/11/17</h1>
<p>Overview of assignment 4. Assignment 3 due tonight.</p>
<p>Another guest lecture by Aghastya.</p>
<p>Usually Minibatch SGD is just called SGD. Minibatch SGD is ;wip: what is it?</p>
<p>A Taylor series expansion at a given point can approximate any continuous function around that point. A neural network formalizes this idea.</p>
<p>Some issues that can happen with SGD are exploding gradients, local minima, and that it's really slow. In deep networks, we usually find that the space is so large and the dimension so high that we usually just find something really close to the global minima anyways.</p>
<p>;wip: momentum in SGD - not only do we store the current position, we also store the previous gradient. this means that if the gradient keeps being small in the direction we care about, they'll accumulate and we'll converge faster. we also be sure to use a terminal velocity dependent on <span class="math inline">g</span> and alpha (get this formula from slides, also alpha is the variables we're trying to optimize in that slide)</p>
<p>;wip: nesterov momentum, the slide has a mistake where the f call should be <span class="math inline">f(x; \theta + \alpha v)</span>, also v starts off at 0, and weights are randomly initialized (we can't initialize weights to 0 because that would make weights symmetric)</p>
<p>;wip: setting learning rate slide: with too high a learning rate, we tend to just converge to an easy solution and stay there, not getting to better solutions. a too low learning rate will work, but slower. a good learning rate pushes the error rate all the way toward 0. consider the analogy where we're trying to get to the bottom of a valley, but a too high learning rate makes us bounce back and forth between the sides of the valley, in this slide alpha is the learning rate</p>
<p>;wip: a commonly used learning decay function is step decay - run a large learning rate until we plateau, then decay (e.g., by 10x) the learning rate, then decay again when we plateau again, and so on - in fact this guarantees an optimal result. exponential decay is actually max(<em>{min}, </em>{start} exp(-kt)). exponential decay is strictly worse than step decay, because it decays learning rate before we absolutely have to (they both derive from the same idea though)</p>
<p>;wip: these are all rules of thumb, not proven rules</p>
<p>;wip: adagrad learning: this is mainly convex-only because the first gradient matters a lot - if we go the wrong direction, it's hard to recover. it suffers from premature decay because it's hard to get the learning rate back up if we get a single big gradient, RMSprop replaces the sum with an exponentially weighted moving average, which works really well in practice but doesn't have the theoretical basis adagrad learning does, just a lecture slide in hinton's ML course - it's pretty widely used.</p>
<p>;wip: adam momentum is sort of like adagrad with momentum, it's adding a per-parameter momentum term, we can usually leave parameters at their defaults, and is also almost an industry standard, adam doesn't really have many hyperparameters to tweak</p>
<p>;wip: how to choose slide, giving in-practice performance comparisons (not theoretical performance). anything before 2014 ish tends to use adagrad, everything after tends to use RMSprop and adam</p>
<p>Fully connected layer is a matrix multiplication. A convolutional layer is similar, and tries to preserve the spatial structure of the input - a pixel in one image is more likely to be important to nearby pixels than farther pixels.</p>
<p>For example, consider a 32x32x3 image - we could apply a 5x5x3 filter to each pixel, dot producting each possible 5x5 window on the ;wip: describe convolutional layer, also you can do this in one matrix mult, then reshape into the d+1 dimensional tensor</p>
<p>;wip: convolutional net</p>
<h1 id="section-16">7/11/17</h1>
<p>Assignment 4 should be a bit easier.</p>
<p>Guest lecture by Aghastya from focal systems.</p>
<p>;wip: N is the input side length, <span class="math inline">P</span> is number of values to pad on each side</p>
<p>;wip: the stride <span class="math inline">S</span> is how many rows/cols we skip for each pixel - a stride of 2 means we skip every other column and every other row when moving the filter, the stride should never be larger than the filter to avoid losing information</p>
<p>;wip: floor((N + 2P - F) / S) + 1 is the side length of the output image</p>
<p>;wip: define convolutional layer</p>
<p>If we have a 225 by 224 image with 3 channels, red/green/blue, we have a 224 by 224 by 3 matrix. When we apply a 5 by 5 filter, the depth of the filter must then be the same depth, so the filter is a 5 by 5 by 3 matrix. If we want our output image to have 64 channels, then we need 64 of these 5 by 5 by 3 filters.</p>
<p>So if <span class="math inline">F</span> is the filter side length, <span class="math inline">D_i</span> is the input image depth, and <span class="math inline">D_o</span> is the output image depth, then we have <span class="math inline">F^2 D_i D_2</span> parameters for all of the filters.</p>
<p>When not specified, we assume that <span class="math inline">D_i = D_o = 1</span> and <span class="math inline">S = 1</span> and we pad the edges with zeroes so the output image is exactly the same size as the input image.</p>
<p>Applying a filter means that we elementwise multiply each element of the filter with the corresponding element in the input image, then sum them all up - it's the dot product of the filter and the segment of the image if we flattened each one out into two big vectors. Each dot product is a channel in a pixel in the output image, known as a <strong>descriptor</strong> (because it describes the area covered by its corresponding filter).</p>
<p>The <strong>receptive field</strong> of one descriptor (channel of a pixel in the output image) is the region of the original image that the descriptor depends on, and is the 2D size of the filter - if we applied a 5 by 5 by 3 filter to an image, the resulting descriptor's receptive field would be 5 by 5. Note that a descriptor depends only on the receptive field - we can change anything in the input image outside of the receptive field, without affecting that channel of that pixel in the output image. The receptive field is usually the same for all of the descriptors in an output image.</p>
<p>As we nest convolutional layers, the receptive field grows. For example, if we apply a 5 by 5 filter to an image, and then a 3 by 3 filter to the resulting image, all with a stride of 1, each descriptor in the final image has a 7 by 7 receptive field from the first layer. Basically, the 5 by 5 filter has 1 pixel in the middle, and 2 pixels on each side, and the 3 by 3 filter has 1 pixel in the middle, and 1 pixel on each side. We then add the pixels on each side, so it's as if we had 1 pixel with 3 pixels on each side, which is a 7 by 7 square.</p>
<p>A stride of 2 doubles our perceptive field, a stride of 3 triples it, and so on. This allows us to have wide receptive fields using fewer filters, which are easier to change. It's commonly used to reduce the number of parameters.</p>
<p>Formally, the side length of the receptive field at layer <span class="math inline">i</span> is <span class="math inline">r_0 = 1, r_i = s_{i - 1} r_{i - 1} + F_i - s_{i - 1}</span>, where <span class="math inline">s_i</span> is the stride at layer <span class="math inline">i</span> and <span class="math inline">F_i</span> is the filter side length for layer <span class="math inline">i</span>.</p>
<p>Convolution networks need to be translation invariant in many real-world applications - we care more about whether a feature is present than exactly where it is in the original image. Naively, we can do this by making the image really low res, so a translation wouldn't make much of a difference in such a low-resolution image. However, this causes us to lose information. We can get around this by only preserving the most important information.</p>
<p><strong>Pooling</strong> is a technique where we downsample an image by taking a summary statistic for nearby descriptors. For example, max-pooling takes the maximum value within each <span class="math inline">k</span> by <span class="math inline">k</span> chunk of descriptors to get the value of that channel in the pooled output image. Likewise, mean-pooling and median-pooling takes the mean and median values. For example, if we have a 224 by 224 image and pool with a 2 by 2 max-filter and stride 2, we get a 112 by 112 image (that's max-pooling). <span class="math inline">k</span> is usually chosen by a human and the stride is almost always <span class="math inline">k</span> as well.</p>
<p>Pooling gives us slight translation invariance (proportional to <span class="math inline">k</span>), doesn't have any hyperparameters that need to me learned (because <span class="math inline">k</span> is chosen by a human), is easy to backpropagate (it's just the derivative of the max/mean), and significantly reduces the number of parameters. The idea with pooling is that we're making the image smaller while discarding less important features and keeping the important ones.</p>
<p>The purpose of the pooling layer is essentially to increase the receptive field of each descriptor without increasing the number of parameters too much.</p>
<p>In real-world architectures, we often alternate convolutional layers with pooling layers. Essentially, this is doing some transformations, summarizing the important features, doing more transformations, summarizing the important features, and so on until we're done. The outputs are then usually fed into a more typical neural network architecture, perhaps a couple of fully connected layers followed by a softmax. we often also use activation layers like ReLU after each conv layer to add nonlinearity.</p>
<p>;wip: AlexNet was first practical convolutional net, designed to be broken out over 2 GPUs, which makes the architecture pretty complicated, VGGNet has smaller filters and is a lot deeper, uses alternating conv, relu, pooling layers followed by a couple fully connected layers and then softmax</p>
<p>;wip: why not just keep adding more layers to make it deeper? we get <span class="math inline">y = f_1(... f_k(x) ...)</span>, and the gradient is the product of a bunch of chain rule results. note that the gradient starts to explode because we'll sometimes multiply a ton of small numbers or large numbers when those derivatives are large. we can clip huge gradients, but we can't clip small gradients because we don't know whether they should be positive/negative. to solve this, we can add an offset to each input to make the gradient closer to 1, like learning <span class="math inline">y = h(x) + x</span> instead of <span class="math inline">y = f(x)</span>. this is what resnet did ;wip: more info about this technique</p>
<p>;wip: fundamental idea of deep learning: we can represent any function by mixing simple nonlinearities</p>
<h1 id="section-17">9/11/17</h1>
<p>So far our neural network paradigm has been: input feeds into hidden transformations, which feeds into output. The output is then a representation of some aspect of the input. This is a <strong>feed-forward neural network</strong>.</p>
<p>However, real-world problems often can't be reduced to one fixed-size input. To solve this, we have <strong>recurrent neural networks</strong> (RNN), which simply feed some of the network outputs back into the inputs. We do this by iterating over the sequence - at each iteration, we take some of the outputs of the network from the previous iteration, combine it with the current iteration's input, and then feed it into the network. We can choose the initial value for &quot;some of the outputs of the network from the previous iteration&quot;, or choose to do iterations with fake data before or after the actual sequence - this is called the <strong>process sequence</strong>.</p>
<p>For example, for sentiment analysis, the input would be a sentence (maybe represented as a sequence of words), and the output would be how positive/negative that sentence is. At each word, the RNN would have both the current word as an input, and its previous outputs. The network would learn to create a good description of the input seen so far up to the current word, and to use that description along with the current word to output a new, better description.</p>
<p>Since we have a sequence of inputs and we're running the network on each input, an RNN takes in a sequence and outputs another sequence. RNNs can be many-to-many, many-to-one, one-to-many, and one-to-one (one-to-one RNNs are equivalent but not equal to feedforward networks, as there can still be multiple iterations), depending on the <strong>process sequence</strong>. This is a hyperparameter that is generally chosen by hand. For example, for sentiment analysis we would usually want to have a many-to-one process sequence.</p>
<p>RNNs are essentially <span class="math inline">y = f(x_t, f(x_{t - 1}, f(x_{t - 2}, \ldots f(x_0, v) \ldots)))</span>, where <span class="math inline">f</span> is the non-recurrent part of the network, <span class="math inline">x_i</span> is the sequence of inputs, and <span class="math inline">v</span> is the initial value of the previous iteration's output. For example, ;wip: the vanilla RNN slide formulas</p>
<p>New notation: <span class="math inline">s = \tanh</span>.</p>
<p>The <strong>vanilla RNN</strong>: given <span class="math inline">x \in \mb{R}^n, h \in \mb{R}^d, W \in \mb{R}^{d \times (m + d)}</span>, <span class="math inline">\vec h_t = \tanh\left(W \begin{bmatrix} \vec h_{t - 1} \\ \vec x_t \end{bmatrix}\right)</span>. <span class="math inline">h_0</span> and <span class="math inline">W</span> are learned through training, and <span class="math inline">h_0</span> is usually initialized to <span class="math inline">\vec 0</span>.</p>
<p>To train the vanilla RNN, at each iteration we expanding the entire recurrence so far into a closed form, then do standard backpropagation. For example, at iteration 3 we have <span class="math inline">\vec h_3 = \tanh\left(W \begin{bmatrix} \tanh\left(W \begin{bmatrix} \tanh\left(W \begin{bmatrix} \vec h_0 \\ \vec x_1 \end{bmatrix}\right) \\ \vec x_2 \end{bmatrix}\right) \\ \vec x_3 \end{bmatrix}\right)</span>.</p>
<p>Clearly, finding the gradient of this with respect to <span class="math inline">W</span> and <span class="math inline">h_0</span> would involve many factors of <span class="math inline">W</span> and a lot of repeated <span class="math inline">\tanh</span>, which results in rapidly exploding or vanishing gradients. This makes vanilla RNNs really hard to train. We usually limit the number of expansions so that the</p>
<p>A <strong>long-short-term-memory RNN</strong> (LSTM) is a variation in which we make the memory more explicit, to make the gradients more manageable. It's essentially a layer structure that has inputs for forgetting, saving, value to save, and whether to read its memory value (or output <span class="math inline">\vec 0</span>).</p>
<p>With LSTMs, we now have <span class="math inline">\vec c_0</span> and <span class="math inline">\vec h_0</span> rather than just <span class="math inline">\vec h_0</span>.</p>
<p>New notation: <span class="math inline">A \odot B</span> is an elementwise (Hadamar) multiplication.</p>
<p>Consider <span class="math inline">W \begin{bmatrix} \vec h_{t - 1} \\ \vec x_t \end{bmatrix}</span>. We can split this into four vectors (it doesn't matter too much how we split it, but usually into equally sized vectors), and apply certain activation functions to each one. The four vectors are <span class="math inline">\vec f</span> (forget vector), <span class="math inline">\vec i</span> (insert vector), <span class="math inline">\vec g</span> (value to insert), and <span class="math inline">\vec o</span> (output vector). We apply sigmoid to <span class="math inline">\vec i</span>, <span class="math inline">\vec i</span>, and <span class="math inline">\vec f</span> (so they're between 0 and 1), and <span class="math inline">\tanh</span> to <span class="math inline">\vec g</span> (so it's between -1 and 1).</p>
<p>We then have <span class="math inline">\vec c_t = \vec c_{t - 1} \odot \vec f + \vec g \odot \vec i</span>. Then, <span class="math inline">\vec h_t = \vec o \odot \tanh(\vec c_t)</span>. Note that <span class="math inline">c_t</span> depends on <span class="math inline">\vec h_{t - 1}</span> because the four vectors depend on <span class="math inline">\vec h_{t - 1}</span>.</p>
<p>This is interesting because the derivative of <span class="math inline">\vec c_t</span> with respect to <span class="math inline">\vec c_{t - 1}</span> is just <span class="math inline">\vec f</span>. This solves the vanishing/exploding gradient problem very neatly - the gradient with respect to <span class="math inline">c_0</span> is just a bunch of <span class="math inline">\vec f</span> values multiplied together, and the gradient with respect to <span class="math inline">W</span> is a lot simpler too because a lot of complexity came from <span class="math inline">\vec c_t</span>, which is a lot simpler now. We can't backpropagate arbitrarily, but we can backpropagate a lot farther than with vanilla RNNs without the gradients vanishing/exploding.</p>
<p>A bi-directional RNN can incorporate future and past information, instead of just past information like vanilla RNNs - we can do this when you have the future data already available, just not fed into the network yet. We train one RNN over the normal sequence, another RNN over the reversed sequence, and then a feedforward network takes the outputs of both and uses that to make a final output.</p>
<p>Siri used to use a bidirectional RNN, but since it requires all the future data to be available, it needed you to finish talking before it started processing. Instead, Siri changed to use an RNN with a 3 second delay, assuming that when talking, words stop affecting words more than 3 seconds in the past.</p>
<p>;wip: recursive networks from slides</p>
<p>;wip: mixture of Gaussians is good for low-dimensional data ;wip: dropout loss isn't used that much anymore in state of the art models</p>
<h1 id="section-18">14/11/17</h1>
<p>Assignment 4 overview. Some notes on assignment:</p>
<ul>
<li>The pseudocode has some numerical computation issues if implemented naively. Make sure to write it in a way that avoids this.</li>
<li>The MNIST dataset has all of the digits centered. That means the edges of every image are constant 0, which causes a division by 0 when trying to invert the determinant. To fix this, we can just add a very small number to the matrix diagonal (like <code>eps</code> in MATLAB). In general, whenever we divide by an arbitrary non-negative number, we'll want to add a small value to prevent zero division.</li>
<li>The covariance formula in the pseudocode is for a full covariance matrix. However, the actual matrix will end up being a diagonal matrix, so computing the full covariance matrix is really inefficient. There's a formula that is equivalent to the diagonal of the full matrix.</li>
<li><span class="math inline">\vec x^T A \vec x = \vec x \cdot A \vec x = \sum x_i (A_\vec x)_i = \sum_i x_i \sum_j A_{i, j} x_j = \sum_i \sum_j x_i x_j A_{i, j}</span>.</li>
<li><span class="math inline">r_{i, k} = \pi_k \abs{s_k}^{-1/2} \exp(-(x_i - \mu_k)^T s_k^{-1} (x_i - \mu_k))</span> has <span class="math inline">\abs{s_k}^{-1/2}</span>, which depends on the diagonals of <span class="math inline">s_k</span>. If the diagonals are tiny values, the determinant will be very tiny, and the inverse square will be huge. Avoid these numerical issues by taking the log of the factors and summing them instead of multuplying the original values, so <span class="math inline">\log r_{i, k} = \log \pi_k - \frac 1 2 \log s_k - (x_i - \mu_k)^T s_k^-1 (x_i - \mu_k)</span>.</li>
<li>There might also be numerical issues with <span class="math inline">r_{i, k} = \frac{r_{i, k}}{\sum_k r_{i, k}}</span>. We can use magnitude cancellation to improve this: in general, if we want to compute <span class="math inline">\sum e^{a_i}</span> where <span class="math inline">a_i</span> could potentially be pretty big, we can use the fact that <span class="math inline">\sum e^{a_i} = e^{\max_j a_j} \sum e^{a_i - \max_j a_j}</span>. In many ML problems, the <span class="math inline">e^{\max_j a_j}</span> will cancel out pretty well. In this case, since <span class="math inline">r_{i, k}</span> is in the numerator and denominator, we can factor out the exponential value directly with <span class="math inline">r_{i, k} = \frac{\exp(r_{i, k})}{\exp(\sum_k r_{i, k})} = \frac{1}{\exp((\sum_k r_{i, k}) - r_{i, k})}</span>.</li>
<li>For <span class="math inline">p \epsilon</span>, this is not a typo - the product of <span class="math inline">p</span> and <span class="math inline">\epsilon</span> is a Bernoulli distribution. It means that on average, we aren't going to change the value at all.</li>
<li>Computing vector expected values: <span class="math inline">E((\vec w \cdot (\vec x \odot \epsilon))^2)</span>. Make sure to compute this correctly, by expanding it out into a sum, like <span class="math inline">E((\sum_j w_j x_j \epsilon_j)^2) = E(\sum_j \sum_k w_j w_k x_j x_k \epsilon_j \epsilon_k)</span>.</li>
<li>To improve performance, you might apply PCA to the training set. The PCA can be applied on all of the classes at once, or per-class, depending on the dataset (usually we just try both and pick the better one), to reduce the feature dimensionality. When performing PCA, we usually apply the same transform we applied to the training set to the testing set, rather than performing PCA directly on the testing set.</li>
</ul>
<h2 id="generative-adversarial-networks">Generative Adversarial Networks</h2>
<p>GANs were invented a few years ago, and have been really useful for certain applications.</p>
<p>A <strong>generative model</strong> takes training data with a certain unknown distribution <span class="math inline">p_{data}</span> and generates new samples from the model distribution <span class="math inline">p_{model}</span> such that <span class="math inline">p_{model}</span> is really similar to <span class="math inline">p_{data}</span>. This is an example of unsupervised learning. For example, a generative model for images takes in a bunch of images, and tries to generate new images that are look similar - if we feed in pictures of cars, it might output pictures of other cars.</p>
<p>The naive approach to this is to set estimate the density function for <span class="math inline">p_{data}</span> and use that estimate as <span class="math inline">p_{model}</span>, then sample that to get the outputs - the Gaussian mixture model is essentially this approach, where the estimate is a mixture of Gaussians. Although this works, estimating the density function is actually a really hard problem, especially for high-dimensional data (which images almost always are). Additionally, sampling a high-dimensional distribution, in general, is very difficult as well (though easy for Gaussian and mixture of Gaussian distributions).</p>
<p>Instead of estimating the density functions and then sampling them, we could instead just directly generate the samples. For example, image super-resolution nets like Ledig et al 2017 takes a set of images, downsamples/pools them as the training set X, takes the original as the training set Y, and then trains a deep neural network to predict the original image given the downsampled/pooled image.</p>
<p>Essentially, it's sort of like an automated Turing test. We have two neural networks - a <strong>generator</strong>, trying to generate realistic images, and <strong>discriminator</strong>, trying to tell the difference between real and fake images. The generative network's objective is to fool the discriminator, and the classifier's objective is to not get fooled by the generator. Concretely, the generator network is fed by random noise, and the outputs are mixed with the training set images to get the inputs for the discriminator. The discriminator then tries to learn to correctly classify the images as either from the training set, or from the generator. We train these two networks in parallel, and get a GAN.</p>
<p>We can think of this as a minimax game <span class="math inline">\min_G \max_D E(\log D(x)) + E(\log(1 - D(G(z))))</span> where:</p>
<ul>
<li><span class="math inline">x \sim p_{data}</span> is the input distribution.</li>
<li><span class="math inline">z \sim p_z</span> is random noise.</li>
<li><span class="math inline">D(s)</span> is the predicted probability that some sample <span class="math inline">s</span> comes from <span class="math inline">p_{data}</span>.</li>
<li><span class="math inline">D</span> is the maximum probability of training data and the minimum probability of generated sample.</li>
<li><span class="math inline">\max_G</span> means maximize the probability that ;wip: slide about minimax GAN</li>
</ul>
<p>This works because <span class="math inline">E(\log(1 - D(G(z))))</span> is equivalent to <span class="math inline">E(\log(1 - D(m)))</span> where <span class="math inline">m \sim p_{model}</span>. Essentially, <span class="math inline">G(z)</span> is a function that transforms a uniform random sample into a <span class="math inline">p{model}</span> random sample, so if we feed it with random noise we get a random sample from <span class="math inline">p_{model}</span>.</p>
<p>;wip: Clearly, if we fix <span class="math inline">G</span>'s outputs, we can simplify the game to <span class="math inline">\min_G \max_D \int \left(\log D(x) P_{data}(x) \dee x + \int \log(1 - D(x)) P_{model}(x)\right) \dee x = \min_G \max_D \int \left(\log D(x) P_{data}(x) + \log(1 - D(x)) P_{model}(x)\right) \dee x</span>.</p>
<p>;wip: Clearly, <span class="math inline">\log D(x) P_{data}(x) + \log(1 - D(x)) P_{model}(x)</span> is equivalent to cross-entropy loss, so the discriminator is essentially trying to minimise its cross entropy loss.</p>
<p>So the optimal discriminator is one that lets <span class="math inline">\frac 1 {D(x)} P_{data}(x) + \frac{-1}{1 - D(x)} P_{model}(x) = 0</span>. Solving, we get <span class="math inline">D(x) = \frac{P_{data}(x)}{P_{data}(x) + P_{model}(x)}</span>. ;wip: KL divergence is non-negative, so 0 is the minimum value, and this occurs when p_data = p_model, so the discriminator's goal in the minimax game is to ???</p>
<h1 id="section-19">16/11/17</h1>
<p>More hints for A4:</p>
<ul>
<li>Don't confuse <span class="math inline">r_{i,.}</span> with <span class="math inline">r_{i, :}</span> or <span class="math inline">r_{., k}</span> with <span class="math inline">r_{:, k}</span></li>
<li>In the assignment hint for q1, the full formula for the diagonals of <span class="math inline">S_k</span> is given in the <span class="math inline">s_j</span> formula, but it's recommended that you try deriving it yourself.</li>
<li>For 1.2, we're supposed to compute a prior probability. Each class will end up having its own mixture of Gaussians model, and we then combine them to get the final prediction.</li>
<li>For q2.2, we're trying to integrate out <span class="math inline">Z_{t_{n + 2}}, \ldots, Z_{t_{n + p}}</span> out of the left side of the conditional distribution for <span class="math inline">Z_{t_{n + 2}}, \ldots, Z_{t_{n + p}} \mid Z_{t_1}, \ldots, Z_{t_n}</span>. We don't need to do anything to the right side of the conditional, since they're not a part of the distribution itself. There's a formula in the slides that does almost exactly this.</li>
</ul>
<p>The competition between the generator and the discriminator is a zero-sum game. We're tuning <span class="math inline">G</span> and <span class="math inline">D</span> such that they get better at competing with each other.</p>
<p>If we write the expectations in terms of integrals, it ends up becoming logistic regression! <span class="math inline">\min_G \max_D E_{x \sim p_{data}(\ln D(x))} + E_{x \sim p_{model}}(\ln(1 - D(x)))</span> becomes <span class="math inline">\min_G \max_D \int P_{data}(x) \ln D(x) + P_{model}(x) \ln(1 - D(x)) \dee x</span>. Since it's analogous to minimizing the cross-entropy loss <span class="math inline">y \ln p + (1 - y) \ln(1 - p)</span> where $P_{data}(x) = y, $, this becomes logistic regression.</p>
<p>For an optimal <span class="math inline">G</span>, <span class="math inline">D</span> is optimal when <span class="math inline">D(x) = \frac{P_{data}(x)}{P_{data}(x) + P_{model}(x)}</span> - if we have a perfect generator, the discriminator would never be able to tell the difference better than random guessing, so it might as well randomly guess. For an optimal <span class="math inline">D</span>, <span class="math inline">G</span> is optimal when <span class="math inline">p_{model} = p_{data}</span> - if we have a perfect discriminator, we have to generate indistinguishable images to fool it.</p>
<p>We can use the minimax theorem to swap the min/min (proof is out of the scope of this course). Suppose we do this, so we fix <span class="math inline">D</span> and then try to optimize <span class="math inline">G</span>. Since <span class="math inline">G</span> is trying to minimize the <span class="math inline">\ln(1 - D(x))</span>, it will focus on the ones where <span class="math inline">D(x)</span> gives the highest value, which results in the lowest values for <span class="math inline">\ln(1 - D(x))</span>. ;wip: what does collapsing on modes of <span class="math inline">D</span> mean? slide 16 IIRC</p>
<p>;wip: fixed G, what's optimal D? fixed D, what's optimal G?</p>
<p>As a result, GANs can't be trained by alternately training the generator and discriminator, because if we fix either <span class="math inline">G</span> or <span class="math inline">D</span>, training the other might not get a strictly better result. In general, we can't alternately train players for minimax games, because we easily get into oscillations.</p>
<p>The model space is the set of possible distributions <span class="math inline">p_{model}</span>. Usually, the model space isn't large enough to include the true <span class="math inline">p_{data}</span>, but it can get really close.</p>
<p>In practice, <span class="math inline">D</span> and <span class="math inline">G</span> are rarely optimal, and are usually deep convnets that are optimized with SGD. When computing <span class="math inline">E_{x \sim p_{data}(\ln D(x))}</span> or <span class="math inline">E_{x \sim p_{model}}(\ln(1 - D(x)))</span>, we approximate the expected value by taking the average of those formulas for each value in the training set.</p>
<p>;wip: pseudocode for GAN training algorithm</p>
<p>Note that we don't fix <span class="math inline">G</span> or <span class="math inline">D</span> and then train the other in this algorithm (as we discussed above, this wouldn't work very well). Instead, for each iteration we perform <span class="math inline">k</span> SGD steps for <span class="math inline">D</span> and one SGD step for <span class="math inline">G</span> - we're repeatedly switching between training <span class="math inline">G</span> and <span class="math inline">D</span> to ensure they improve in a compatible way.</p>
<p>For small values for <span class="math inline">D(x)</span>, the gradient of <span class="math inline">\ln(1 - D(G(z)))</span> tends to be really small. To make the gradient larger in these cases, one trick is to replace <span class="math inline">\ln(1 - D(G(z)))</span> with <span class="math inline">-\ln(D(G(z)))</span>, which has a larger gradient near these value. When we do this we essentially have <span class="math inline">\min_G \max_D E_{x \sim p_{data}(\ln D(x))} + E_{x \sim p_{model}}(-\ln D(x))</span>. There's no strong mathematical basis for this, but it's used pretty often in industry. However, if we look at what happens when we fix <span class="math inline">G</span> or <span class="math inline">D</span> and optimize the other, we can show that <span class="math inline">G</span> and <span class="math inline">D</span> are optimal with the same conditions.</p>
<p>After training the GAN, the usual output is to use the generator network to generate new images. Sometimes we also use the discriminator network to tell the difference between real and fake images. For example, we can feed the generator noise to generate one image, and then perturb the noise slightly and run it through the generator again, we get a slightly different image.</p>
<p>The Wasserstein GAN is a brand new objective function for GANs that is much better behaved. The GAN's current objective function is equivalent to <span class="math inline">\min_G \operatorname{JS}(p_{data} \| p_{model})</span>, which can be discontinuous, but a GAN using the Wasserstein loss function like <span class="math inline">\min_G(W(p_{data}, p_{model}))</span> will be a lot better behaved. Also, we can approximate it using kernel methods, which makes practical implementation very interesting. The Wasserstein function is out of the scope of this course, as it relies heavily on measure theory.</p>
<h2 id="decision-trees">Decision trees</h2>
<p>A tree is an acyclic connected undirected graph. A decision tree is a tree where each interior node contains a variable, and each edge is a possible value of that variable (or range of values). Each leaf node contains the output value.</p>
<p>We make decisions using decision trees by starting at the root node, and following all of the edges that match until there are no decisions left and we've reached a set of leaf nodes. We can represent any boolean function as a decision tree, though the resulting tree might end up being infeasibly large.</p>
<p>For classification purposes, leaf nodes contain the output class, and we can do something like take the majority vote of all the leaf nodes we reach. For regression purposes, leaf nodes contain the predicted value, and we can do something like take the average of all the leaf nodes we reach.</p>
<p>Decision trees are often also represented as binary trees, where each node contains a boolean test (like <span class="math inline">x_5 &lt; 0.45</span>), and we take the left node if the test fails and the right node if the test passes.</p>
<p>To train decision trees, we need to choose an variable for each node, choose which edges of a node to grow out, decide when to stop growing, and decide what to put in the leaf nodes. Choosing variables to test and edges to grow is already NP-hard, but we'll use a greedy approach by always choosing the lowest-cost variable and lowest-cost value ranges.</p>
<p>;wip: formula for decision tree training, <span class="math inline">t</span> is a real number so there are infinite values, but we can restrict it to values in the training set, because that is enough to arbitrarily partition the training set points.</p>
<p>For deciding when to stop training a decision tree, we usually stop once the tree reaches a certain depth, after a certain amount of time spent, if the loss function stops shrinking, or if we manage to correctly classify all of the training set points. Stopping at a certain depth is most common, and the depth limit is sort of a regularization technique.</p>
<p>;wip: loss function for regression using decision trees - the variance of the <span class="math inline">y</span> values like <span class="math inline">\sum_i (y_i - \overline y)^2</span> where <span class="math inline">\overline y</span> is the average of all the leaf node values we've reached ;wip: it's NP complete to train decision trees optimally because it reduces to the sum problem</p>
<h1 id="section-20">21/11/17</h1>
<p>For classifiers we generally use mean-squared-error for the loss function. For decision trees we often use the classification cost <span class="math inline">\hat p_c = \frac 1 {\abs{D}} \sum_{i \in D} 1_{y_i = c}</span> and the prediction <span class="math inline">\hat y = \argmax_c \hat p_c</span> to construct loss functions, where <span class="math inline">D</span> is the training set. For example, the misclassification error is <span class="math inline">\ell(D) = 1 - \hat p_c</span> ;wip: entropy and etc.</p>
<p>An ideal decision node in a decision tree should lead to all of one class passing the test, and all of the other failing the test. In other words, we want to try choose decisions that partition the X values such that each partition has Y values as homogenous as possible.</p>
<p>In practice, we usually train a decision tree via pruning. We first grow a very deep, very accurate tree on the training set, which would definitely be very overfit. We then go through the tree bottom-up, and along the way, simplify nodes that increase our estimated generalization error the most (e.g., replacing the node with a node that always predicts 1, or always predicts 0, etc.).</p>
<p>Decision trees are nice because they are relatively fast to train and are often even interpretable by humans.</p>
<h2 id="baggingboosting">Bagging/Boosting</h2>
<p>When solving a machine learning problem, we generally want to try a lot of options (e.g., SVM, deep neural nets, decision trees) and pick the best one. However, what if we could combine a bunch of algorithms together? If we had the resources to run all of those, this could potentially give better results than any single algorithm could give.</p>
<p>Consider the jellybean guessing game, where the person who has the best guess as to how many jellybeans are in a jar wins the game. As it turns out, we can consistently get closer results by averaging many guesses. This is the power of aggregation, and these types of models are known as <strong>ensemble models</strong>.</p>
<p>Supose we have many binary classifiers <span class="math inline">h_1, \ldots, h_t</span> on separate datasets <span class="math inline">D_1, \ldots, D_t</span>, where each classifier has accuracy better than 50%. Since the datasets are independent, the classifiers must be independent as well, so the random variables representing a given classifier making a mistake must be independent as well.</p>
<p>If we then predict the output as being the majority label among all <span class="math inline">t</span> classifiers (assuming <span class="math inline">t</span> is odd), we'll make a correct prediction whenever the majority of <span class="math inline">h_1, \ldots, h_t</span> are correct. Since they're independent, we have many independent Bernoulli distributions, the sum of which is a Binomial distribution. So the probability we get it wrong is <span class="math inline">P(\text{majority is right}) \le \sum_{k = \ceil{\frac t 2}}^t {t \choose k} p^k(1 - p)^{t - k}</span>, where <span class="math inline">p</span> is the lowest-accuracy classifier.</p>
<p>This is hard to analyze, so we approximate this using a Gaussian distribution <span class="math inline">N(tp, tp(1 - p))</span>. So as <span class="math inline">t</span> goes to infinity, the probability of an error approaches 0. ;wip: better proof</p>
<p>In other words, as we use more and more weak classifiers, taking the majority vote from all those classifiers will eventually give us an arbitrarily small error, assuming the classifiers are independent. However, in practice we can't make this assumption, since we would need independent training sets for each classifier, which would often require an infeasible amount of training data.</p>
<p>To help with this, we do <strong>bagging</strong> (Bootstrap Aggregating) - for each classifier we randomly sample a bunch of training set entries with replacement (which gives a training set with possibly duplicated or missing entries), train the classifiers are those, then aggregate the classifier predictions by taking the mode (for classification) or the mean/median (for regression). In contrast, dividing the training set up for each classifier is like sampling without replacement. Sampling with replacement can't give us fully independent training sets, but in practice, using bagging over straightforward splitting of the dataset almost always yields an improvement.</p>
<p>Bagging can also be used with regression, where we replace the classifiers with regressors, and then take the mean of the regressor outputs rather than the majority vote of the classifiers. Averaging those <span class="math inline">t</span> independent outputs reduces variance by a factor of <span class="math inline">t</span>.</p>
<p>Another practical improvement to regression with bagging is to add a small noise to every Y value in the training set, which is different for each regressor. The equivalent of this for classification is to randomly flip some of the training set Y values (say, 5% to 10%). This works because it improves independence of the training sets, and works as a form of regularization. We can think of this technique as analogous to dropout in neural networks.</p>
<p>Bagging is basically averaging over classifiers or regressors - it smooths over instabilities/high-variance-regions in the component classifiers, so it's beneficial if performance of certain classifiers changes a lot with small perturbations. That means bagging works well for algorithms like decision trees (which can be relatively unstable), but not kNN (which is very stable already).</p>
<p><strong>Hoefting's inequality</strong> says that given independent and identical Bernoulli random variables <span class="math inline">X_1, \ldots, X_n \in \set{0, 1}</span> where <span class="math inline">P(X_i = 1) = p</span>, then <span class="math inline">P(\sum X_i \le (p - \epsilon)n) \le \exp(-2 \epsilon^2 n)</span> for any <span class="math inline">\epsilon &gt; 0</span>. We can think of the classifier outputs as independent and identically distributed Bernoulli random variables, where <span class="math inline">X_i = 1</span> means classifier <span class="math inline">i</span> made a prediction error and <span class="math inline">X_i = 0</span> means it was correct. Therefore, the expected error rate of the ensemble model is under <span class="math inline">100k</span>% with <span class="math inline">c</span>% confidence, where <span class="math inline">c = 1 - \exp(-2 \epsilon^2 n)</span> and <span class="math inline">c = p - \epsilon</span>.</p>
<p>This gives us an upper bound on the number of classifiers we need to get a certain accuracy (with a certain confidence). Namely, we choose the number of classifiers <span class="math inline">n</span> by solving for <span class="math inline">n</span> in <span class="math inline">c = 1 - \exp(-2 \epsilon^2 n)</span> and <span class="math inline">c = p - \epsilon</span>, to get <span class="math inline">n = -frac{\ln(1 - c)}{2 (p - c)^2}</span>.</p>
<p>The <strong>random forest</strong> algorithm combines some of the techniques we've seen so far. We use bagging-style sampling without replacement to get many &quot;independent&quot; training sets, then train a bunch of decision trees on those, taking the majority vote (classification) or average (regression) of all of their predictions.</p>
<p>Given many classifiers <span class="math inline">h_1, \ldots, h_t</span>, each one barely better than randomly guessing, is it always possible to construct a classifier with really good performance? Schapire showed that this is possible for an arbitrary level of accuracy using a specific algorithm in 1990 (specifically, by proving that majority voting on 3 classifiers is strictly better than the individual classifiers, and we can repeat this to get arbitrarily good accuracy), and this was improved by Freund in 1995 (a more practical algorithm that weights and re-weights training set points according to how often classifiers get them right).</p>
<h1 id="section-21">23/11/17</h1>
<p>In-class demo of hedge algorithm on a real-world example.</p>
<p>Freund and Schapire further improved this in 1997 with their combined algorithm, known as the Hedge Algorithm:</p>
<p>This algorithm is actually powerful enough to work well in practice for stock markets, sports betting, and many more difficult machine learning problems - it's competitive with our current state of the art algorithms.</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="co"># hyperparameters include: number of classifiers, learning rate, number of iterations or number of passes through training set</span>
<span class="kw">def</span> hedge(classifiers, training_set, learning_rate <span class="op">=</span> <span class="fl">0.5</span>):
  <span class="co"># the amount by which we penalize losses</span>
  beta <span class="op">=</span> np.full(<span class="bu">len</span>(classifiers), learning_rate)

  <span class="co"># the fraction of resources we&#39;ll spend on each classifier (usually starts off initialized to random values)</span>
  weights <span class="op">=</span> np.random.uniform(<span class="fl">0.1</span>, <span class="fl">1.0</span>, num_classifiers)

  <span class="co"># we don&#39;t necessarily have to go through the training set in order, and we might make multiple passes too</span>
  <span class="cf">for</span> x, y <span class="kw">in</span> training_set:
    <span class="co"># normalize so that all elements sum up to 1</span>
    <span class="co"># we also usually add a small epsilon value to the denominator to prevent underflow</span>
    weights <span class="op">/=</span> weights.<span class="bu">sum</span>() <span class="op">+</span> <span class="fl">1e-12</span>

    <span class="co"># receive performance data for each classifier for this training set element</span>
    <span class="co"># all loss function values must be between 0 and 1 inclusive, but besides that they can be arbitrary functions</span>
    loss <span class="op">=</span> weights <span class="op">@</span> [classifier.loss(y, classifier.predict(x)) <span class="cf">for</span> classifier <span class="kw">in</span> classifiers]

    <span class="co"># reweight classifiers based on how they performed this time</span>
    weights <span class="op">*=</span> beta <span class="op">**</span> loss
  
  <span class="co"># the resulting weights can then be used to combine the classifiers into the final model</span></code></pre></div>
<p>The hedge algorithm can provably always do at least as well as the best individual classifier, because eventually it will allocate all the weight to the best one. Even if we have an opponent that knows our weight vector and controls our loss vector, trying to maximize our losses, the individual classifiers would all end up doing at least as bad. The loss vectors in most algorithm are assumed to be independent and identically distributed, but that isn't required for hedge, and the loss vectors can even be adversarial.</p>
<p>In fact, if <span class="math inline">N</span> is the number of classifiers, <span class="math inline">\vec p^t</span> is the weight vector at time <span class="math inline">t</span>, and <span class="math inline">\vec l^t</span> is the loss vector at time <span class="math inline">t</span>, then <span class="math inline">\sum_t \vec p^t \cdot \vec l^t \le \frac{\ln N - \ln ;wip}{1 - \beta}</span>. If we choose <span class="math inline">\beta = ;wip</span>, then <span class="math inline">;wip: the second version of the hedge algorithm performance bound formula</span>.</p>
<p>Basically, if we were betting on horses, we can use the hedge algorithm to get arbitrarily close to doing just as well as someone who can see the future and choose a single horse to always bet on, by not seeing the future and being able to change our bet. If we were seeing a particular horse perform poorly, we might stop betting as much on it in subsequent rounds.</p>
<p>The algorithm was inspired by the weighting-by-majority-vote algorithm, which was invented by the person who invented Winnow for perceptrons - that's why there are some similarities between how they work.</p>
<p>Adaptive boost (AdaBoost) is the hedge algorithm converted into a boosting meta-algorithm - given some training data and some machine learning algorithms, it trains those algorithms and produces a better-performing classifier (in contrast, the hedge algorithm just takes already-trained classifiers). We treat each training set entry as a horse, and get a certain amount on each horse. The resulting loss values are used to reweight how much we bet on each horse. ;wip: intuitive explanation</p>
<p>;wip: pseudocode for adaboost and making final predictions <span class="math inline">h_f(x)</span> from the output ;wip: it's called adaptive boost because we change <span class="math inline">\beta</span> over time ;wip: the closer the prediction is to the true value, the larger the exponent - we lower the weight of training set points that are performing well, so the ones we're performing poorly on are more represented in the training set, which forces the classifiers to focus more on those bad-performing examples when they're being trained ;wip: the overall performance needs to be strictly more than 0.5, because when we set <span class="math inline">\beta = \epsilon / (1 - \epsilon)</span>, we need <span class="math inline">0 &lt; \beta &lt; 1</span> ;wip; the larger is, the larger is, so the more its weight is preserved ;wip: when we have a multiplicative algorithm like this one (weights always multiplied by a value, like winnow), the weight will always be nonzero if they start off nonzero, and will stay 0 if they start off 0. therefore we must ensure that we never initialize any weight vector elements to 0 - always positive numbers ;wip: afterwards, we often throw away any classifiers that have a weight below the certain threshold, because they have very little effect on the end result ;wip: if we use h_t with p_{t + 1}, we will get <span class="math inline">\epsilon = 0.5</span>, because the way we chose <span class="math inline">\beta</span> and the exponent of beta such that the classifier sucks again. this means the classifier is no better than chance, so the hypothesis needs to improve to do better than chance - it prevents us from just returning the same hypothesis over and over again, which wouldn't improve anything</p>
<p>;wip: the training error is guaranteed to decay as per <span class="math inline">P(h_f(x_i) \ne y_i) \le 2^T \prod_{t = 1}^T \sqrt{\epsilon_t (1 - \epsilon_t)}</span>. the training error goes down exponentially fast, since <span class="math inline">\epsilon_t \le \frac 1 2 - \gamma \le \exp(-2T \gamma^2)</span></p>
<p>;wip: we can think of AdaBoost as doing gradient descent to minimize the exponential loss <span class="math inline">\sum_i \exp(-y_i h(x_i))</span> where <span class="math inline">h</span> is in the conic hull of all the <span class="math inline">h_t</span>'s ;wip: since the training error drops so fast (exponentially), is overfitting a concern? no, because we usually use super simple classifiers like decision stumps, which are not individually prone to overfitting at all. ;wip: another explanation compares bagging and boosting: as the number of classifiers increases, the training error drops to 0 almost immediately, while test error is continues going down even after the error is 0. the adaboost authors explained it as follows: the adaboost algorithm isn't just trying to minimize training error, but also maximizing the margin <span class="math inline">y h(x)</span> (this is 1 if they agree, and -1 if they don't agree). so the training error drops to 0, but it continues trying to maximize the margin ;wip: grove and shuurmans showed that latter explanation isn't so good in 1998: they just ran it for more iterations, and eventually the test error started increasing again! in other words, run adaboost long enough and it will start overfitting. also, they invented LP-boosting, which explicitly tries to maximize the margin (using linear programming), and LP-boosting turned out to perform very poorly. this implies that maximizing the margin isn't a very good goal, because explicitly maximizing it didn't decrease test error ;wip: therefore, why adaboost works so well isn't that well understood even today, despite other explanations like soft-margins and so on</p>
<p>;wip: adaboost is a straightforward way to boost performance and doesn't have many requirements for acceptable classifiers, but the results are hard to interpret and is hard to parallelize</p>
<p>;wip: for bagging, we already have a bunch of classifiers and try to weight their outputs. for boosting, we train a weak classifier on a training set, modify the training set, and then repeatedly retrain until we get a bunch of trained weak classifiers that work really well. basically we train a weak classifier on the training set, train a weak classifier on the ones it did poorly on, train a weak classifier on the ones those two didn't do well on, and so on ;wip: bagging is easily parallelized, because the classifiers don't depend on each other, while boosting can't really be parallelized because each training set depends on the results of the previous classifiers</p>
<p>Adaboost was used in Viola-Jones face detection, where there were lots of simple pattern detectors adaboosted into a strong face signal. Viola-Jones used a cascading architecture - the first classifier is really fast and uses few features, the second is slower with more features, and so on (the presented architecture used 38 layers). This allows us to move this window across the image really fast and accurately, crucial for realtime face recognition, because at each level the classifier can say &quot;that's definitely not a face&quot;.</p>
<p>We have to use an asymmetric loss function here because the number of detection window that has a face is actually really small, or else it would just always predict no-face for a really high accuracy. Viola-Jones used one where agreement give 0 loss, missing a true face gives cost <span class="math inline">\sqrt{k}</span> and falsely finding a face gives cost <span class="math inline">\frac 1 {\sqrt{k}}</span>, where <span class="math inline">k</span> is a hyperparameter that is a large positive number.</p>
<h1 id="section-22">28/11/17</h1>
<p>Overview of the exam: open book, all notes allowed, minimal linear algebra, format will be similar to assignments but proportionately easier. One question about designing an algorithm, a few true/false, short answer, derivations, proofs, calculations. 6 questions total. Overview of assignment 5, including some highlighted piazza answers.</p>
<p>Course review:</p>
<ul>
<li>Perceptron:</li>
<li>The perceptron classifies by learning a weight vector - when it classifies a feature wrong, it just adds the feature to the weight (possibly scaled by a learning rate).</li>
<li>The perceptron convergence theorem says that the perceptron will always converge on linearly separable data, and faster the larger the margin is. However, it just gives us an arbitrary separating hyperplane, not necessarily the best one.</li>
<li>The perceptron bounding theorem says that if the data isn't linearly separable, the perceptron will eventually cycle.</li>
<li>We usually stop perceptron when the error stops decreasing or the validation set reaches a certain accuracy.</li>
<li>To make it multi-class, we can use one vs. all (each perceptron demarcates class <span class="math inline">i</span> vs. every other class, highest activation wins), and one vs. one (each perceptron demarcates class <span class="math inline">i</span> vs. class <span class="math inline">j</span>, plurality vote).</li>
<li>The winnow algorithm trains perceptrons by multiplying by input features rather than adding. Winnow is a lot better at eliminating irrelevant features quickly, because its convergence time grows with respect to the number of dimensions rather than the number of training set points. Winnow also requires that the weight vector always be non-negative, which we can do by appending the negative version of each feature vector to itself.</li>
<li>Linear regression:</li>
<li>We can solve for the least squares equation <span class="math inline">X^T X W = X^T Y</span> by taking its derivative and solving for when the derivative is 0.</li>
<li>Huber loss is really nice because it has a good derivative near the origin but stays linear when far away from the origin.</li>
<li>Training techniques: Ridge regression (L2 regularization, encourages minimal squared weights overall which makes the weights all consistently small), Lasso regression (L1 regularization, encourages individual values to go to 0 which makes the weights more sparse).</li>
<li>Splitting the test/train/validation sets, using cross-validation (especially when we don't have validation sets). Why we should never test on the test set.</li>
<li>Multilayer perceptrons:</li>
<li>Useful when the data isn't linearly separable. Classic example: XOR dataset, with points <span class="math inline">\tup{0, 0} \to 0, \tup{0, 1} \to 1, \tup{1, 0} \to 1, \tup{1, 1} \to 0</span>. Prove that it's not linearly separable.</li>
<li>Multiple perceptrons stacked together, but instead of <span class="math inline">\sign</span> we can use a bunch of other nice activation functions like ReLU, which have nicer gradients for gradient descent purposes.</li>
<li>To perform gradient descent, we define a loss function, then take the gradient of the loss function with respect to each weight.</li>
<li>Training takes place in two phases: a forward phase where the inputs are run through the network, and a backward pass where we compute derivative values, then update weights according to the gradients.</li>
<li>Chain rule: if <span class="math inline">f(x) = g(h(x))</span>, then <span class="math inline">f&#39;(x) = g&#39;(h(x)) * h&#39;(x)</span>. The forward pass computes <span class="math inline">h(x)</span>, and the backwards pass computes the rest.</li>
<li>A two-layer perceptron can approximate an arbitrary continuous function with arbitrary precision, if we have an activation function that is locally bounded, has a negligible closure of discontinuous points, and isn't a polynomial (almost all real-world activation functions). Note that kernel methods with linear classifiers can also approximate arbitrary continuous functions.</li>
<li>Logistic regression:</li>
<li>Treat each label <span class="math inline">y_i</span> as Bernoulli random variables, with a hidden parameter <span class="math inline">p_i = P(y_i = 1)</span>. We then assume that each <span class="math inline">y_i</span> is indpeendent. Then, the probability of seeing the training set <span class="math inline">X</span> given <span class="math inline">w</span> is <span class="math inline">\prod P(\vec x_i; \vec w)^{y_i} ;wip</span></li>
<li>We define <span class="math inline">P(\vec x_i; \vec w) = \log \frac{P(\vec x_i; \vec w)}{1 - P(\vec x_i; \vec w)}</span>. This is equivalent to <span class="math inline">P(\vec x_i; \vec w) = \frac{1}{1 + \exp(-\vec w \cdot \vec x_i)}</span>.</li>
<li>We are then optimizing the following: <span class="math inline">\sum \ln(1 + \exp(;wip))</span>.</li>
<li>Cross-entropy loss:</li>
<li><span class="math inline">\ell(\Theta) = -\sum_{i = 1}^n \sum_{c = 1}^k \vec y_{i, c} \ln \vec \hat y_{i, c}</span> where <span class="math inline">\vec y</span> and <span class="math inline">\vec \hat y</span> are one-hot vectors.</li>
<li>Suppose we have a bunch of values <span class="math inline">q_i</span>, where <span class="math inline">q_i \ge 0</span> and <span class="math inline">\sum_i q_i = 1</span> (we call this a <strong>distribution</strong>). The entropy of <span class="math inline">q</span> is <span class="math inline">H(q) = \sum q_i \ln q_i</span>. The regression problem wants us to minimize the square loss <span class="math inline">(y - \hat y)^2</span>, which is minimized at <span class="math inline">\hat y = y</span>.</li>
<li>Suppose we have <span class="math inline">y_i</span> as labels in one-hot encoding. Clearly, each <span class="math inline">y_i</span> is a distribution because each element of <span class="math inline">y_i</span> is non-negative and they sum up to 1.</li>
<li>The KL divergence <span class="math inline">KL(\vec y_i \| \vec p_i) = \sum_c y_{i, c} \ln \frac{y_{i, c}}{p_{i, c}} = \sum_c y_{i, c} \ln y_{i, c} - \sum_c y_{i, c} \ln p_{i, c}</span>.</li>
<li>Since the first term above is a constant, minimizing the KL divergence is equivalent to minimizing <span class="math inline">-\sum_c y_{i, c} \ln p_{i, c}</span>.</li>
<li>Therefore, minimizing the cross entropy is just the same thing as minimizing the KL divergence between <span class="math inline">\vec y_i</span> and <span class="math inline">\vec p_i</span>.</li>
<li>Logistic regression does the same thing as SVM (determining a linear decision boundary), but minimizes logistic loss rather than hinge loss. SVM usually works a bit better though, since hinge loss is less sensitive to outliers and can classify with full confidence if a point is outside the margin area.</li>
</ul>
<h1 id="section-23">30/11/17</h1>
<p>Exam is in MC 2034 for section 001 undergrads, duration 150 minutes. Format will be similar to the sample midterm, but harder.</p>
<p>Continued course review:</p>
<ul>
<li>The Bayes error is the absolute best classifier. We can derive it as follows:</li>
<li>The error of a classifier is is <span class="math inline">P(f(X) \ne Y)</span>. The expected error is then <span class="math inline">E(1_{f(X) \ne Y})</span>. This is a common pattern, replacing a probability with the expectation of an indicator variable.</li>
<li>Assuming a binary classifier, <span class="math inline">E(1_{f(X) \ne Y}) = E(1_{f(X) = 1, Y = -1} + 1_{f(X) = -1, Y = 1})</span>.</li>
<li>Since <span class="math inline">f(X)</span> and <span class="math inline">y</span> are independent, <span class="math inline">E(1_{f(X) = 1, Y = -1} + 1_{f(X) = -1, Y = 1}) = E(1_{f(X) = 1} 1_{Y = -1} + 1_{f(X) = -1} 1_{Y = 1})</span>.</li>
<li>Condition on <span class="math inline">x</span> to turn <span class="math inline">X</span> into a constant <span class="math inline">x</span>: <span class="math inline">E(1_{f(X) = 1} 1_{Y = -1} + 1_{f(X) = -1} 1_{Y = 1}) = E(E(1_{f(x) = 1} 1_{Y = -1} + 1_{f(x) = -1} 1_{Y = 1} \mid x))</span>.</li>
<li>That turns <span class="math inline">f(X)</span> into <span class="math inline">f(x)</span>, a constant, which lets us move its indicators out of the expectation: <span class="math inline">E(E(1_{f(x) = 1} 1_{Y = -1} + 1_{f(x) = -1} 1_{Y = 1} \mid x)) = E(1_{f(x) = 1} E(1_{Y = -1} \mid x) + 1_{f(x) = -1} E(1_{Y = 1} \mid x))</span>.</li>
<li>Since exactly one of <span class="math inline">1_{f(x) = 1}</span> and <span class="math inline">1_{f(x) = -1}</span> are 1 and the other is 0. ;wip: which one is more than 50%, choose that one</li>
<li>k-nearest-neighbors:</li>
<li><span class="math inline">k</span> is chosen to balance overfitting vs. underfitting - larger <span class="math inline">k</span> tends to underfit, smaller tends to overfit.</li>
<li>1-NN has worst-case error of about twice the Bayes error.</li>
<li>SVM: like perceptron, we're finding separating hyperplane, but we're also trying to maximize the margin of the hyperplane: <span class="math inline">\min_{\vec w, b} \frac 1 2 \magn{\vec w}^2</span> subject to <span class="math inline">y_i(\vec w \cdot \vec x_i + b) \ge 1</span> for all <span class="math inline">i</span>.</li>
<li>Note that the <span class="math inline">1</span> can be any positive number, and the exponent on <span class="math inline">\magn{w}</span> can also be any positive number. The above is the most common formulation, however.</li>
<li>Some of the closest points to the separating hyperplane are support vectors (formally, support vectors are those where <span class="math inline">\alpha \ne 0</span> in the Lagrangian dual). Essentially the support vectors are part of each convex hull that are necessary in order for the minimum-difference line segment between the two convex hulls to be what it is.</li>
<li>For hard-margin SVM, the solution for <span class="math inline">\vec w, b</span> always exists and is always unique.</li>
<li>To get kernel SVM, we take the Lagrangian dual of the primal SVM problem, which contains a <span class="math inline">\vec x \cdot \vec x</span>. We can then replace <span class="math inline">\vec x \cdot \vec x</span> with <span class="math inline">\kappa(\vec x, \vec x)</span> to get kernelized SVM from the dual problem.</li>
<li>The Lagrangian dual of SVM can be thought of as minimizing the distance between the convex hull of the two classes (the separating hyperplane then passes through the midpoint of that line segment with the line segment's direction as the normal). The primal SVM problem can be thought of as maximizing the margin of a hyperplane.</li>
<li>To get soft-margin SVM, we pay a cost <span class="math inline">C</span> for misclassifying points rather than having a constraint saying we can't misclassify any of them. We can compute the Lagrangian dual for this too, and the only difference from hard-margin is that we have <span class="math inline">\max_{C \ge \alpha \ge 0}</span> rather than <span class="math inline">\max_{\alpha \ge 0}</span>.</li>
<li>Soft-margin SVM usually uses hinge loss, but we also often use epsilon-insensitive loss (hinge loss with a bit of play <span class="math inline">\epsilon</span>), L2 loss, L1 loss, and Huber loss.</li>
<li>Lagrangian duals:</li>
<li>We usually solve min/max problems such as <span class="math inline">\min_x f(x)</span> by taking the derivative <span class="math inline">f&#39;(x)</span> and setting it to 0, then solving for <span class="math inline">x</span>. However, if there are constraints, we can no longer do this.</li>
<li>Lagrangian duals solve this. We move the constraints into the objective function by introducing a maximizer, use the minimax theorem to swap the min and max, then taking derivative and solving for the min to leave only the <span class="math inline">\max</span>.</li>
<li>The Lagrangian dual is often useful because it switches the problem from working on each row of the training set to each dimension. When we have high-dimensional problems, we often use the primal to improve performance, while we might use the dual when we have a lot of training set points. Additionally, the dual might lend itself better to kernelizing, like for SVM.</li>
<li>Kernels: kernelized machine learning algorithms give us a way to efficiently transform each input <span class="math inline">\vec x</span> using some function <span class="math inline">\phi(\vec x)</span>, even if computing <span class="math inline">\phi(\vec x)</span> can't be computed efficiently, as long as we can efficiently compute <span class="math inline">\kappa(\vec x_1, \vec x_2) = \phi(\vec x_1) \cdot \phi(\vec x_2)</span>.</li>
<li>To kernelize a machine learning algorithm, we write it in a way such that it contains <span class="math inline">\vec x_1 \cdot \vec x_2</span>, and then replace that with <span class="math inline">\kappa(\vec x_1, \vec x_2)</span>.</li>
<li>To prove that a function <span class="math inline">\kappa(\vec x_1, \vec x_2)</span> is actually a kernel, we prove that its corresponding matrix is positive semidefinite, or construct <span class="math inline">\phi(\vec x)</span> such that <span class="math inline">\kappa(\vec x_1, \vec x_2) = \phi(\vec x_1) \cdot \phi(\vec x_2)</span>.</li>
<li>This is especially useful for linear classifiers, because a linear classifier plus a non-linear transform is equivalent to a non-linear classifier.</li>
<li>Gaussian process: a function <span class="math inline">Z(t, \omega)</span> such that <span class="math inline">Z(t, \omega)</span> at any real fixed <span class="math inline">t</span> is a Gaussian distribution.</li>
<li>If we take <span class="math inline">Z(t, \omega)</span> at a bunch of values of <span class="math inline">t</span>, <span class="math inline">\begin{bmatrix} Z(t_1, \omega) \\ \vdots \\ Z(t_k, \omega) \end{bmatrix}</span>, we get a multivariate Gaussian distribution.</li>
<li>;wip: what does it mean to marginalize a gaussian? if we integrate <span class="math inline">\vec X_2</span> out of <span class="math inline">\begin{bmatrix} X_1 \\ X_2 \end{bmatrix}</span> we just get <span class="math inline">X_1</span></li>
<li>There's a 1:1 correspondence between kernels and Gaussian processes - they're actually roughly interchangeable. A Gaussian process is specified by a mean function <span class="math inline">m(t)</span> and a covariance function <span class="math inline">\kappa</span>. Note that the covariance function is a kernel - every Gaussian process is defined by a mean function and kernel.</li>
<li>Mixture of Gaussians model: <span class="math inline">p(x \mid \theta) = \sum_k \pi_k G_k</span> where <span class="math inline">G_k \sim N(\mu_k, S_k)</span>.</li>
<li>We can train this using expectation-maximization, since it's too hard to directly minimize the log likelihood function, because it contains an absolute value.</li>
<li>Useful trick: <span class="math inline">\min_x 2 \abs{x}</span> is roughly equivalent to <span class="math inline">\min_{x, y &gt; 0} \frac{x^2}{y} + y</span> (introduce a new variable <span class="math inline">y</span>), because the minimum usually occurs when <span class="math inline">\frac{x^2}{y} = y</span>, so <span class="math inline">y^2 = x^2</span> and <span class="math inline">y = \abs{x}</span>.</li>
<li>Expectation maximization: If we fix <span class="math inline">x</span>, we can minimize <span class="math inline">y</span>. If we fix <span class="math inline">y</span>, we can minimize <span class="math inline">x</span>. Repeat until we get a pretty minimal value.</li>
<li>;wip: EM algorithm slide, where <span class="math inline">y</span> is a distribution <span class="math inline">q_i</span> and then we alternatingly estimate and minimize each one.</li>
<li>CNNs:</li>
<li>A 3 by 3 filter is actually 3 by 3 by (number of channels in the lower layer). A convolutional layer can contain many filters.</li>
<li>Pooling layers operate on individual channels - they don't change the number of channels, only the image dimensions.</li>
</ul>
<p>ML interview questions: What is logistic regression doing (minimizing logistic loss)? Can you prove perceptron error bound (nope)?</p>
<p>;wip: logistic loss is log_2, not ln ;wip: blog post explaining lagrangian duals simply and tricks like the one we use to derive soft-margin SVM</p>
<div class="status-banner" style="display: none; position: fixed; bottom: 0; left: 0; right: 0; text-align: center;">
    <div style="display: inline-block; padding: 0.8em 2em 0.5em 2em; background: black; color: white; font-size: 2em;">
        Rendering <svg xmlns="http://www.w3.org/2000/svg" height="1.4em" viewbox="0 0 1200 500" style="vertical-align: text-bottom"><title>LaTeX logo</title><g transform="matrix(45 0 0 45 40 40)" fill="white"><path d="M5.5 4.4C5.5 4.4 5.2 4.4 5.2 4.4 5.1 5.4 5 6.7 3.2 6.7 3.2 6.7 2.4 6.7 2.4 6.7 1.9 6.7 1.9 6.6 1.9 6.3 1.9 6.3 1.9 1 1.9 1 1.9 0.6 1.9 0.5 2.9 0.5 2.9 0.5 3.2 0.5 3.2 0.5 3.2 0.5 3.2 0.2 3.2 0.2 2.8 0.2 1.9 0.2 1.5 0.2 1.1 0.2 0.3 0.2 0 0.2 0 0.2 0 0.5 0 0.5 0 0.5 0.2 0.5 0.2 0.5 1 0.5 1 0.6 1 0.9 1 0.9 1 6.2 1 6.2 1 6.6 1 6.7 0.2 6.7 0.2 6.7 0 6.7 0 6.7 0 6.7 0 7 0 7 0 7 5.2 7 5.2 7 5.2 7 5.5 4.4 5.5 4.4z"/><path d="M5.3 0.2C5.3 0 5.2 0 5.1 0 5 0 4.9 0 4.9 0.2 4.9 0.2 3.3 4.2 3.3 4.2 3.2 4.4 3.1 4.7 2.5 4.7 2.5 4.7 2.5 5 2.5 5 2.5 5 4 5 4 5 4 5 4 4.7 4 4.7 3.7 4.7 3.5 4.6 3.5 4.4 3.5 4.3 3.5 4.3 3.6 4.2 3.6 4.2 3.9 3.4 3.9 3.4 3.9 3.4 5.9 3.4 5.9 3.4 5.9 3.4 6.3 4.4 6.3 4.4 6.3 4.4 6.3 4.5 6.3 4.5 6.3 4.7 5.9 4.7 5.8 4.7 5.8 4.7 5.8 5 5.8 5 5.8 5 7.7 5 7.7 5 7.7 5 7.7 4.7 7.7 4.7 7.7 4.7 7.6 4.7 7.6 4.7 7.1 4.7 7.1 4.7 7 4.5 7 4.5 5.3 0.2 5.3 0.2zM4.9 0.9C4.9 0.9 5.8 3.1 5.8 3.1 5.8 3.1 4 3.1 4 3.1 4 3.1 4.9 0.9 4.9 0.9z"/><path d="M13.3 0.2C13.3 0.2 7.2 0.2 7.2 0.2 7.2 0.2 7 2.5 7 2.5 7 2.5 7.3 2.5 7.3 2.5 7.4 0.9 7.6 0.5 9.1 0.5 9.3 0.5 9.5 0.5 9.6 0.6 9.8 0.6 9.8 0.7 9.8 0.9 9.8 0.9 9.8 6.2 9.8 6.2 9.8 6.5 9.8 6.7 8.8 6.7 8.8 6.7 8.4 6.7 8.4 6.7 8.4 6.7 8.4 7 8.4 7 8.8 6.9 9.8 6.9 10.3 6.9 10.7 6.9 11.7 6.9 12.2 7 12.2 7 12.2 6.7 12.2 6.7 12.2 6.7 11.8 6.7 11.8 6.7 10.7 6.7 10.7 6.5 10.7 6.2 10.7 6.2 10.7 0.9 10.7 0.9 10.7 0.7 10.7 0.6 10.9 0.6 11 0.5 11.3 0.5 11.5 0.5 13 0.5 13.1 0.9 13.2 2.5 13.2 2.5 13.5 2.5 13.5 2.5 13.5 2.5 13.3 0.2 13.3 0.2z"/><path d="M18.7 6.7C18.7 6.7 18.4 6.7 18.4 6.7 18.2 8.2 17.9 8.9 16.2 8.9 16.2 8.9 14.9 8.9 14.9 8.9 14.4 8.9 14.4 8.8 14.4 8.5 14.4 8.5 14.4 5.9 14.4 5.9 14.4 5.9 15.3 5.9 15.3 5.9 16.3 5.9 16.4 6.2 16.4 7 16.4 7 16.6 7 16.6 7 16.6 7 16.6 4.4 16.6 4.4 16.6 4.4 16.4 4.4 16.4 4.4 16.4 5.2 16.3 5.5 15.3 5.5 15.3 5.5 14.4 5.5 14.4 5.5 14.4 5.5 14.4 3.2 14.4 3.2 14.4 2.8 14.4 2.8 14.9 2.8 14.9 2.8 16.2 2.8 16.2 2.8 17.7 2.8 18 3.3 18.1 4.7 18.1 4.7 18.4 4.7 18.4 4.7 18.4 4.7 18.1 2.5 18.1 2.5 18.1 2.5 12.5 2.5 12.5 2.5 12.5 2.5 12.5 2.8 12.5 2.8 12.5 2.8 12.7 2.8 12.7 2.8 13.5 2.8 13.5 2.9 13.5 3.2 13.5 3.2 13.5 8.4 13.5 8.4 13.5 8.8 13.5 8.9 12.7 8.9 12.7 8.9 12.5 8.9 12.5 8.9 12.5 8.9 12.5 9.2 12.5 9.2 12.5 9.2 18.2 9.2 18.2 9.2 18.2 9.2 18.7 6.7 18.7 6.7z"/><path d="M21.7 3.1C21.7 3.1 23 1.1 23 1.1 23.3 0.8 23.6 0.5 24.5 0.5 24.5 0.5 24.5 0.2 24.5 0.2 24.5 0.2 22.1 0.2 22.1 0.2 22.1 0.2 22.1 0.5 22.1 0.5 22.5 0.5 22.7 0.7 22.7 0.9 22.7 1 22.7 1.1 22.6 1.2 22.6 1.2 21.5 2.8 21.5 2.8 21.5 2.8 20.2 0.9 20.2 0.9 20.2 0.9 20.1 0.8 20.1 0.8 20.1 0.7 20.4 0.5 20.8 0.5 20.8 0.5 20.8 0.2 20.8 0.2 20.4 0.2 19.7 0.2 19.3 0.2 19 0.2 18.4 0.2 18 0.2 18 0.2 18 0.5 18 0.5 18 0.5 18.2 0.5 18.2 0.5 18.8 0.5 19 0.5 19.2 0.8 19.2 0.8 21 3.6 21 3.6 21 3.6 19.4 6 19.4 6 19.2 6.2 18.9 6.7 17.9 6.7 17.9 6.7 17.9 7 17.9 7 17.9 7 20.3 7 20.3 7 20.3 7 20.3 6.7 20.3 6.7 19.8 6.7 19.7 6.4 19.7 6.2 19.7 6.1 19.7 6.1 19.8 6 19.8 6 21.2 3.9 21.2 3.9 21.2 3.9 22.8 6.3 22.8 6.3 22.8 6.3 22.8 6.3 22.8 6.4 22.8 6.5 22.6 6.7 22.2 6.7 22.2 6.7 22.2 7 22.2 7 22.5 6.9 23.2 6.9 23.6 6.9 24 6.9 24.5 7 24.9 7 24.9 7 24.9 6.7 24.9 6.7 24.9 6.7 24.7 6.7 24.7 6.7 24.2 6.7 24 6.6 23.8 6.3 23.8 6.3 21.7 3.1 21.7 3.1z"/></g></svg> math...
    </div>
</div>
<div class="license">
  <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-nc-sa/4.0/80x15.png" /></a> This work by <a xmlns:cc="http://creativecommons.org/ns#" href="https://uberi.github.io/" property="cc:attributionName" rel="cc:attributionURL">Anthony Zhang</a> is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>.
  Copyright 2013-2017 Anthony Zhang.
</div>
</body>
</html>
