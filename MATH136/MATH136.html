<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <title>MATH136 | Anthony Zhang</title>
  <link rel="stylesheet" href="../css/base.css" type="text/css">
  <link rel="stylesheet" href="../css/note.css" type="text/css">
  <link rel="stylesheet" href="../highlight/styles/default.css">
  <link rel="stylesheet" href="../highlight/styles/paraiso.light.css">
  <script src="../highlight/highlight.pack.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>
  <script src="../katex/katex.min.js"></script>
  <link rel="stylesheet" href="../katex/katex.min.css" />
  <script type="text/javascript">
  window.onload = function() {
    document.getElementsByClassName("status-banner")[0].style.display = "block";
    setTimeout(function() {
      renderMathElements(document.getElementsByClassName("math"));
      document.getElementsByClassName("status-banner")[0].style.display = "none";
    }, 1);
  }

  function renderMathElements(mathElements) {
    var mathOptions = {
      macros: {
        "\\set": "\\left\\{ #1 \\right\\}",
        "\\tup": "\\left\\langle #1 \\right\\rangle",
        "\\abs": "\\left\\lvert #1 \\right\\rvert",
        "\\floor": "\\left\\lfloor #1 \\right\\rfloor",
        "\\ceil": "\\left\\lceil#1 \\right\\rceil",
        "\\mb": "\\mathbb{#1}",
        "\\rem": "\\operatorname{rem}",
        "\\ord": "\\operatorname{ord}",
        "\\sign": "\\operatorname{sign}",
        "\\imag": "\\bm{i}",
        "\\dee": "\\mathop{}\\!\\mathrm{d}",
        "\\lH": "\\overset{\\text{l'H}}{=}",
        "\\evalat": "\\left.\\left(#1\\right)\\right|",
        "\\sech": "\\operatorname{sech}",
        "\\spn": "\\operatorname{Span}",
        "\\proj": "\\operatorname{proj}",
        "\\prp": "\\operatorname{perp}",
        "\\refl": "\\operatorname{refl}",
        "\\magn": "\\left\\lVert #1 \\right\\rVert",
        "\\rank": "\\operatorname{rank}",
        "\\sys": "\\left[ #1 \\mid #2\\space \\right]",
        "\\range": "\\operatorname{Range}",
        "\\adj": "\\operatorname{adj}",
        "\\cof": "\\operatorname{cof}",
        "\\coord": "{\\left\\lbrack #1 \\right\\rbrack}_{#2}",
        "\\diag": "\\operatorname{diag}",
        "\\formlp": "\\operatorname{Form}(\\mathcal{L}^P)",

        // not yet available in KaTeX
        "\\operatorname": "\\mathop{\\text{#1}}\\nolimits",
        "\\not": "\\cancel", //wip: looks off
        "\\bm": "\\mathbf", //wip: not italic
      },
      throwOnError: false,
    };
    for (var i=0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      try {
        katex.render(texText.data, mathElements[i], mathOptions);
      } catch (e) {
        console.error(e);
        console.log(mathElements[i]);
      }
    }
  }
  </script>
</head>
<body>
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-68271407-1', 'auto');
    ga('send', 'pageview');

  </script>
  <h1>Lecture Notes by <a href="/">Anthony Zhang</a>.</h1>
  <ul class="site_links">
    <li><a href="/blog/" class="page">blog</a></li>
    <span class="divider"></span>
    <li><a href="http://uberi.github.io/University-Notes" class="page">notes</a></li>
    <span class="divider"></span>
    <li><a href="/resume.pdf" class="page">résumé</a></li>
    <span class="divider"></span>
    <li><a href="https://github.com/Uberi" class="contact">github</a></li>
    <span class="divider"></span>
    <li><a href="https://www.linkedin.com/in/uberi/" class="contact">linkedin</a></li>
    <span class="divider"></span>
    <li><a href="mailto:me@anthonyz.ca" class="contact">email</a></li>
    <span class="divider"></span>
    <li><a href="https://www.facebook.com/anthony.zhang.user" class="contact">facebook</a></li>
    <span class="divider"></span>
    <li><a href="https://twitter.com/anthony926535" class="contact">twitter</a></li>
    <span class="divider"></span>
    <li><a href="https://keybase.io/uberi" class="info">public key</a></li>
  </ul>
<div class="status-banner" style="display: none"><div>Rendering <svg xmlns="http://www.w3.org/2000/svg" height="1.4em" viewbox="0 0 1200 500" style="vertical-align: text-bottom"><title>LaTeX logo</title><g transform="matrix(45 0 0 45 40 40)" fill="white"><path d="M5.5 4.4C5.5 4.4 5.2 4.4 5.2 4.4 5.1 5.4 5 6.7 3.2 6.7 3.2 6.7 2.4 6.7 2.4 6.7 1.9 6.7 1.9 6.6 1.9 6.3 1.9 6.3 1.9 1 1.9 1 1.9 0.6 1.9 0.5 2.9 0.5 2.9 0.5 3.2 0.5 3.2 0.5 3.2 0.5 3.2 0.2 3.2 0.2 2.8 0.2 1.9 0.2 1.5 0.2 1.1 0.2 0.3 0.2 0 0.2 0 0.2 0 0.5 0 0.5 0 0.5 0.2 0.5 0.2 0.5 1 0.5 1 0.6 1 0.9 1 0.9 1 6.2 1 6.2 1 6.6 1 6.7 0.2 6.7 0.2 6.7 0 6.7 0 6.7 0 6.7 0 7 0 7 0 7 5.2 7 5.2 7 5.2 7 5.5 4.4 5.5 4.4z"/><path d="M5.3 0.2C5.3 0 5.2 0 5.1 0 5 0 4.9 0 4.9 0.2 4.9 0.2 3.3 4.2 3.3 4.2 3.2 4.4 3.1 4.7 2.5 4.7 2.5 4.7 2.5 5 2.5 5 2.5 5 4 5 4 5 4 5 4 4.7 4 4.7 3.7 4.7 3.5 4.6 3.5 4.4 3.5 4.3 3.5 4.3 3.6 4.2 3.6 4.2 3.9 3.4 3.9 3.4 3.9 3.4 5.9 3.4 5.9 3.4 5.9 3.4 6.3 4.4 6.3 4.4 6.3 4.4 6.3 4.5 6.3 4.5 6.3 4.7 5.9 4.7 5.8 4.7 5.8 4.7 5.8 5 5.8 5 5.8 5 7.7 5 7.7 5 7.7 5 7.7 4.7 7.7 4.7 7.7 4.7 7.6 4.7 7.6 4.7 7.1 4.7 7.1 4.7 7 4.5 7 4.5 5.3 0.2 5.3 0.2zM4.9 0.9C4.9 0.9 5.8 3.1 5.8 3.1 5.8 3.1 4 3.1 4 3.1 4 3.1 4.9 0.9 4.9 0.9z"/><path d="M13.3 0.2C13.3 0.2 7.2 0.2 7.2 0.2 7.2 0.2 7 2.5 7 2.5 7 2.5 7.3 2.5 7.3 2.5 7.4 0.9 7.6 0.5 9.1 0.5 9.3 0.5 9.5 0.5 9.6 0.6 9.8 0.6 9.8 0.7 9.8 0.9 9.8 0.9 9.8 6.2 9.8 6.2 9.8 6.5 9.8 6.7 8.8 6.7 8.8 6.7 8.4 6.7 8.4 6.7 8.4 6.7 8.4 7 8.4 7 8.8 6.9 9.8 6.9 10.3 6.9 10.7 6.9 11.7 6.9 12.2 7 12.2 7 12.2 6.7 12.2 6.7 12.2 6.7 11.8 6.7 11.8 6.7 10.7 6.7 10.7 6.5 10.7 6.2 10.7 6.2 10.7 0.9 10.7 0.9 10.7 0.7 10.7 0.6 10.9 0.6 11 0.5 11.3 0.5 11.5 0.5 13 0.5 13.1 0.9 13.2 2.5 13.2 2.5 13.5 2.5 13.5 2.5 13.5 2.5 13.3 0.2 13.3 0.2z"/><path d="M18.7 6.7C18.7 6.7 18.4 6.7 18.4 6.7 18.2 8.2 17.9 8.9 16.2 8.9 16.2 8.9 14.9 8.9 14.9 8.9 14.4 8.9 14.4 8.8 14.4 8.5 14.4 8.5 14.4 5.9 14.4 5.9 14.4 5.9 15.3 5.9 15.3 5.9 16.3 5.9 16.4 6.2 16.4 7 16.4 7 16.6 7 16.6 7 16.6 7 16.6 4.4 16.6 4.4 16.6 4.4 16.4 4.4 16.4 4.4 16.4 5.2 16.3 5.5 15.3 5.5 15.3 5.5 14.4 5.5 14.4 5.5 14.4 5.5 14.4 3.2 14.4 3.2 14.4 2.8 14.4 2.8 14.9 2.8 14.9 2.8 16.2 2.8 16.2 2.8 17.7 2.8 18 3.3 18.1 4.7 18.1 4.7 18.4 4.7 18.4 4.7 18.4 4.7 18.1 2.5 18.1 2.5 18.1 2.5 12.5 2.5 12.5 2.5 12.5 2.5 12.5 2.8 12.5 2.8 12.5 2.8 12.7 2.8 12.7 2.8 13.5 2.8 13.5 2.9 13.5 3.2 13.5 3.2 13.5 8.4 13.5 8.4 13.5 8.8 13.5 8.9 12.7 8.9 12.7 8.9 12.5 8.9 12.5 8.9 12.5 8.9 12.5 9.2 12.5 9.2 12.5 9.2 18.2 9.2 18.2 9.2 18.2 9.2 18.7 6.7 18.7 6.7z"/><path d="M21.7 3.1C21.7 3.1 23 1.1 23 1.1 23.3 0.8 23.6 0.5 24.5 0.5 24.5 0.5 24.5 0.2 24.5 0.2 24.5 0.2 22.1 0.2 22.1 0.2 22.1 0.2 22.1 0.5 22.1 0.5 22.5 0.5 22.7 0.7 22.7 0.9 22.7 1 22.7 1.1 22.6 1.2 22.6 1.2 21.5 2.8 21.5 2.8 21.5 2.8 20.2 0.9 20.2 0.9 20.2 0.9 20.1 0.8 20.1 0.8 20.1 0.7 20.4 0.5 20.8 0.5 20.8 0.5 20.8 0.2 20.8 0.2 20.4 0.2 19.7 0.2 19.3 0.2 19 0.2 18.4 0.2 18 0.2 18 0.2 18 0.5 18 0.5 18 0.5 18.2 0.5 18.2 0.5 18.8 0.5 19 0.5 19.2 0.8 19.2 0.8 21 3.6 21 3.6 21 3.6 19.4 6 19.4 6 19.2 6.2 18.9 6.7 17.9 6.7 17.9 6.7 17.9 7 17.9 7 17.9 7 20.3 7 20.3 7 20.3 7 20.3 6.7 20.3 6.7 19.8 6.7 19.7 6.4 19.7 6.2 19.7 6.1 19.7 6.1 19.8 6 19.8 6 21.2 3.9 21.2 3.9 21.2 3.9 22.8 6.3 22.8 6.3 22.8 6.3 22.8 6.3 22.8 6.4 22.8 6.5 22.6 6.7 22.2 6.7 22.2 6.7 22.2 7 22.2 7 22.5 6.9 23.2 6.9 23.6 6.9 24 6.9 24.5 7 24.9 7 24.9 7 24.9 6.7 24.9 6.7 24.9 6.7 24.7 6.7 24.7 6.7 24.2 6.7 24 6.6 23.8 6.3 23.8 6.3 21.7 3.1 21.7 3.1z"/></g></svg> math...</div></div>
<h1 id="math136">MATH136</h1>
<p>Linear Algebra</p>
<pre><code>Instructor: Dan Wolczuk
Section 081 (online)
Email: dstwolcz@uwaterloo.ca
Office Hours: MC 4013 on Mondays, Wednesdays, Fridays 1:00pm-2:00pm, Thursdays 1:30pm-4:00pm</code></pre>
<h1 id="section">13/1/14</h1>
<h2 id="notation">Notation</h2>
<p>We represent vectors as column vectors (matrices of size <span class="math inline">n \times 1</span>) to distinguish them from points: <span class="math inline">\vec{v} = \begin{bmatrix} v_1 \\ \vdots \\ v_n \end{bmatrix}, v_i \in \mathbb{R}, 1 \le i \le n</span>.</p>
<p>However, sometimes we will also write them in <span class="math inline">n</span>-tuple form (for example, <span class="math inline">(3, 7, 2)</span>)</p>
<p><span class="math inline">\mb{R}^n, n \in \mathbb{Z}, n &gt; 0</span> is the set of all elements of the form <span class="math inline">(x_1, \ldots, x_n), x_i \in \mathbb{R}, 1 \le i \le n</span>.</p>
<p>We usually think of these elements as points, but now we will think of them as vectors, abstract objects that we can perform operations on, like adding and subtracting.</p>
<p>This is called an <span class="math inline">n</span>-dimensional Euclidean space, and is the set of all vectors that are of length (dimension) <span class="math inline">n</span>. For example, <span class="math inline">\mb{R}^3</span> (arr-three) is the set of all 3D vectors, and <span class="math inline">\vec{0} = \begin{bmatrix} 0 \\ 0 \\ 0 \end{bmatrix}</span>.</p>
<p>So <span class="math inline">\mb{R}^3 = \set{\begin{bmatrix} x_1 \\ x_2 \\ x_3 \end{bmatrix} \middle| x_1, x_2, x_3 \in \mathbb{R}}</span>.</p>
<p>Likewise, <span class="math inline">\mb{R}^n = \set{\begin{bmatrix} x_1 \\ \vdots \\ x_n \end{bmatrix} \middle| x_1, \ldots, x_n \in \mathbb{R}}</span>.</p>
<p>It might also occasionally be useful to think of these vectors as points, where the vectors are offsets from the origin. For example, a function <span class="math inline">f\left(\begin{bmatrix} x_1 \\ \vdots \\ x_n \end{bmatrix}\right) = f(x_1, \ldots, x_n)</span>.</p>
<h2 id="operations">Operations</h2>
<p>Let <span class="math inline">\vec{x} = \begin{bmatrix} x_1 \\ \vdots \\ x_n \end{bmatrix}, \vec{y} = \begin{bmatrix} y_1 \\ \vdots \\ y_n \end{bmatrix}, c \in \mb{R}</span>.</p>
<h3 id="equality">Equality</h3>
<p><span class="math inline">x = y</span> if and only if <span class="math inline">\forall 1 \le i \le n, x_i = y_i</span>.</p>
<p>In other words, two vectors are equal if and only if all their components are equal. Note that this is defined only for vectors with the same size (number of components).</p>
<h3 id="additionsubtraction">Addition/Subtraction</h3>
<p><span class="math display">\displaystyle x + y = \begin{bmatrix} x_1 \\ \vdots \\ x_n \end{bmatrix} + \begin{bmatrix} y_1 \\ \vdots \\ y_n \end{bmatrix} = \begin{bmatrix} x_1 + y_1 \\ \vdots \\ x_n + y_n \end{bmatrix}</span></p>
<p>Also, <span class="math inline">\vec{x} - \vec{y} = \vec{x} + (-\vec{y})</span>. Therefore, <span class="math inline">x - y = \begin{bmatrix} x_1 \\ \vdots \\ x_n \end{bmatrix} - \begin{bmatrix} y_1 \\ \vdots \\ y_n \end{bmatrix} = \begin{bmatrix} x_1 - y_1 \\ \vdots \\ x_n - y_n \end{bmatrix}</span>.</p>
<h3 id="scalar-multiplication">Scalar Multiplication</h3>
<p><span class="math display">\displaystyle cx = c\begin{bmatrix} x_1 \\ \vdots \\ x_n \end{bmatrix} = \begin{bmatrix} cx_1 \\ \vdots \\ cx_n \end{bmatrix}</span></p>
<p>If <span class="math inline">c = -1</span>, <span class="math inline">c\vec{x} = -x</span>.</p>
<h2 id="linear-combination">Linear Combination</h2>
<p>A <strong>linear combination</strong> of vectors <span class="math inline">\vec{v}_1, \ldots, \vec{v}_n</span> is the value <span class="math inline">L = c_1 v_1 + \ldots + c_n v_n, c_i \in \mathbb{R}, 1 \le i \le n</span>.</p>
<p>It is a linear expression with each term containing a coefficient and one vector value - a sum of scalar multiples.</p>
<h3 id="theorem-1.1.1">Theorem 1.1.1</h3>
<p>If <span class="math inline">\vec{x}, \vec{y}, \vec{w} \in \mathbb{R}^n, c, d \in \mathbb{R}</span>, then:</p>
<ul>
<li>Closure under addition: <span class="math inline">\vec{x} + \vec{y} \in \mathbb{R}^n</span>.</li>
<li>Associativity: <span class="math inline">(\vec{x} + \vec{y}) + \vec{w} = \vec{x} + (\vec{y} + \vec{w})</span></li>
<li>Commutativity: <span class="math inline">\vec{x} + \vec{y} = \vec{y} + \vec{x}</span></li>
<li>Additive identity: <span class="math inline">\exists \vec{0} \in \mathbb{R}, \forall \vec{x} \in \mb{R}^n, \vec{x} + \vec{0} = \vec{x}</span> (<span class="math inline">\vec{0}</span> is called the zero vector).</li>
<li>Additive inverse: <span class="math inline">\forall \vec{x} \in \mathbb{R}^n, \exists -\vec{x} \in \mathbb{R}^n, \vec{x} + (-\vec{x}) = \vec{0}</span>.</li>
<li>Closure under scalar multiplication: <span class="math inline">c\vec{x} \in \mathbb{R}^n</span>.</li>
<li><span class="math inline">c(d\vec{x}) = (cd)\vec{x}</span>.</li>
<li>Scalar distributivity: <span class="math inline">(c + d)\vec{x} = c\vec{x} + d\vec{x}</span>.</li>
<li>Vector distributivity: <span class="math inline">c(\vec{x} + \vec{y}) = c\vec{x} + c\vec{y}</span>.</li>
<li>Multiplicative identity: <span class="math inline">1\vec{x} = \vec{x}</span>.</li>
</ul>
<p>Closure under addition means that when we add two elements in the set, the resulting sum is also in the set.</p>
<p>Closure under scalar multiplication means that when we multiply an element in the set by a scalar, the resulting product is also in the set.</p>
<p>Because these Euclidean spaces are closed under addition and scalar multiplication, they are closed under linear combinations - all linear combinations of vectors in <span class="math inline">\mb{R}^n</span> are also in <span class="math inline">\mb{R}^n</span>.</p>
<h2 id="span">Span</h2>
<p>The <strong>span</strong> of a set of vectors <span class="math inline">\mathcal{B} = \set{\vec{v}_1, \ldots, \vec{v}_k}</span> is the set of all linear combinations of the vectors in the set.</p>
<p>The span of an empty set is the set containing only the zero vector - <span class="math inline">\spn \emptyset = \set{\vec{0}}</span>.</p>
<p>In other words, <span class="math inline">\spn \mathcal{B} = \spn \set{\vec{v}_1, \ldots, \vec{v}_k} = \set{c_1 v_1 + \ldots + c_n v_n \middle| c_1, \ldots, c_n \in \mathbb{R}}</span>.</p>
<p>A span is always a subset of the space it is in. In other words, <span class="math inline">\spn \mathcal{B} \subseteq \mb{R}^n</span>.</p>
<p>A span is just a set. Therefore, we can say that a set <span class="math inline">\mathcal{B}</span> <strong>spans</strong> another set <span class="math inline">\mathcal{C}</span> if and only if <span class="math inline">\mathcal{C}</span> is exactly the set of all linear combinations of the vectors in <span class="math inline">\mathcal{B}</span>.</p>
<p>The <strong>vector equation</strong> of a set of vectors is the generalized equation for the linear combinations of each element in the set. For example, <span class="math inline">\vec{x} = c_1 v_1 + \ldots + c_n v_n</span> is the vector equation for <span class="math inline">\mathcal{B}</span>.</p>
<h3 id="theorem-1.1.2">Theorem 1.1.2</h3>
<p>If <span class="math inline">\vec{v}_{k + 1}</span> can be written as a linear combination of <span class="math inline">\vec{v}_1, \ldots, \vec{v}_k</span>, then <span class="math inline">\spn \set{\vec{v}_1, \ldots, \vec{v}_k, \vec{v}_{k + 1}} = \spn \set{\vec{v}_1, \ldots, \vec{v}_k}</span>.</p>
<p>In other words, if a vector in a set of vectors can already be represented by a linear combination of other vectors in the set, it doesn't affect the span at all.</p>
<p>Proof:</p>
<blockquote>
<p>Assume <span class="math inline">\vec{v}_{k + 1}</span> can be written as a linear combination of <span class="math inline">\vec{v}_1, \ldots, \vec{v}_k</span>.<br />
So <span class="math inline">\exists c_1, \ldots, c_k \in \mathbb{R}, \vec{v}_{k + 1} = c_1 \vec{v}_1 + \ldots + c_k \vec{v}_k</span>.<br />
We want to show that <span class="math inline">\spn \set{\vec{v}_1, \ldots, \vec{v}_k, \vec{v}_{k + 1}} \subseteq \spn \set{\vec{v}_1, \ldots, \vec{v}_k}</span>.<br />
Let <span class="math inline">\vec{x}</span> be an arbitrary element of <span class="math inline">\spn \set{\vec{v}_1, \ldots, \vec{v}_k, \vec{v}_{k + 1}}</span>.<br />
So <span class="math inline">\exists d_1, \ldots, d_{k + 1} \in \mathbb{R}, \vec{x} = d_1 \vec{v}_1 + \ldots + d_k \vec{v}_k + d_{k + 1} \vec{v}_{k + 1}</span>.<br />
So <span class="math inline">\exists d_1, \ldots, d_{k + 1} \in \mathbb{R}, \vec{x} = d_1 \vec{v}_1 + \ldots + d_k \vec{v}_k + d_{k + 1} (c_1 \vec{v}_1 + \ldots + c_k \vec{v}_k) = d_1 \vec{v}_1 + \ldots + d_k \vec{v}_k + d_{k + 1} c_1 \vec{v}_1 + \ldots + d_{k + 1} c_k \vec{v}_k = d_1 \vec{v}_1 + \ldots + d_k \vec{v}_k + d_{k + 1} c_1 \vec{v}_1 + \ldots + d_{k + 1} c_k \vec{v}_k = (d_1 + d_{k + 1} c_1) \vec{v}_1 + \ldots + (d_k + d_{k + 1} c_k) \vec{v}_k</span>.<br />
Clearly, <span class="math inline">x \in \spn \set{\vec{v}_1, \ldots, \vec{v}_k}</span>.<br />
Clearly, <span class="math inline">\spn \set{\vec{v}_1, \ldots, \vec{v}_k} \subseteq \spn \set{\vec{v}_1, \ldots, \vec{v}_k, \vec{v}_{k + 1}}</span>.<br />
Therefore, <span class="math inline">\spn \set{\vec{v}_1, \ldots, \vec{v}_k, \vec{v}_{k + 1}} = \spn \set{\vec{v}_1, \ldots, \vec{v}_k}</span>.</p>
</blockquote>
<p>Consider <span class="math inline">\spn \set{\begin{bmatrix} 1 \\ 0 \\ 1 \end{bmatrix}, \begin{bmatrix} 2 \\ 0 \\ 2 \end{bmatrix}}</span>. Clearly, <span class="math inline">\begin{bmatrix} 2 \\ 0 \\ 2 \end{bmatrix} = 2\begin{bmatrix} 1 \\ 0 \\ 1 \end{bmatrix}</span>.</p>
<p>Clearly, <span class="math inline">\begin{bmatrix} 2 \\ 0 \\ 2 \end{bmatrix} = 2\begin{bmatrix} 1 \\ 0 \\ 1 \end{bmatrix}</span>, so <span class="math inline">\spn \set{\begin{bmatrix} 1 \\ 0 \\ 1 \end{bmatrix}, \begin{bmatrix} 2 \\ 0 \\ 2 \end{bmatrix}} = \spn \set{\begin{bmatrix} 1 \\ 0 \\ 1 \end{bmatrix}}</span> represents a line in 3D.</p>
<p>The span of the zero vector is itself. The span of a linearly independent vector is a line. The span of two linearly independent vectors is a plane. The span of 3 linearly independent vectors is a 3D space. The span of 4 linearly independent vectors is a 4D space, and so on.</p>
<h3 id="linear-independence">Linear Independence</h3>
<p>When we say <span class="math inline">\vec{v}_{k + 1}</span> can be written as a linear combination of <span class="math inline">\vec{v}_1, \ldots, \vec{v}_k</span>, we can also write <span class="math inline">\vec{v}_{k + 1} \in \spn \set{\vec{v}_1, \ldots, \vec{v}_k}</span>.</p>
<p>Note that <span class="math inline">\vec{0}</span> can always be written as a linear combination of any vectors in the same space. The zero vector exists in all dimensions.</p>
<p>It is important to determine which vectors can be written as linear combinations of others.</p>
<p>A set of vectors is <strong>linearly independent</strong> if and only if the vectors in the set cannot be written as linear combinations of each other. Otherwise, the set is <strong>linearly dependent</strong>.</p>
<p>In other words, <span class="math inline">\exists c_1, \ldots, c_n \in \mathbb{R}, c_1 \cdot \ldots \cdot c_n \ne 0 \implies c_1 \vec{v}_1 + \ldots + c_{k - 1} \vec{v}_{k - 1} + c_{k + 1} \vec{v}_{k + 1} + \ldots + c\_n vec{v}_n = c_k \vec{v}_k</span>. Rearranging, we get <span class="math inline">\exists c_1, \ldots, c_n \in \mathbb{R}, c_1 \cdot \ldots \cdot c_n \ne 0 \implies c_1 \vec{v}_1 + \ldots + c_{k - 1} \vec{v}_{k - 1} - c_k \vec{v}_k + c_{k + 1} \vec{v}_{k + 1} + \ldots + c\_n vec{v}_n = \vec{0}</span>.</p>
<h3 id="theorem-1.1.3">Theorem 1.1.3</h3>
<p>Therefore, a set of vectors <span class="math inline">\set{\vec{v}_1, \ldots, \vec{v}_n}</span> is linearly dependent if and only if <span class="math inline">\exists c_1, \ldots, c_n \in \mathbb{R}, c_1 \cdot \ldots \cdot c_n \ne 0 \implies c_1 \vec{v}_1 + \ldots + c_n vec{v}_n = \vec{0}</span>, and otherwise linearly independent.</p>
<p>The solution where <span class="math inline">c_i = 0, 1 \le i \le n</span> is called the trivial solution, because all the coefficients are 0. The trivial solution is always a solution to the above equation.</p>
<p>In other words, the set is linearly independent if and only if the zero vector cannot be written as a linear combination of the vectors in the set, except when all the coefficients are 0.</p>
<p>To prove that a set is linearly independent, we need to prove that the only solution to the above equation is the trivial solution.</p>
<p>To prove that a set is linearly independent, we need to prove that there is a solution to the above equation where at least one of the coefficients is non-aero.</p>
<p>When we find a solution to the equation, we can use this to find the vectors that are in the span of all the others - the vectors that can be written as a linear comnbination of the others. In the solution, <strong>all vectors with non-zero coefficients can be written as linear combinations of all the other vectors</strong>. If we rearrange the solved equation, we can find this linear combination.</p>
<h3 id="theorem-1.1.4">Theorem 1.1.4</h3>
<p>Note that any set of vectors that contains <span class="math inline">\vec{0}</span> is linearly dependent.</p>
<p>Proof:</p>
<blockquote>
<p>Let <span class="math inline">\mathcal{B} = \set{\vec{v}_1, \ldots, \vec{v}_n}</span> and <span class="math inline">\vec{v}_i = \vec{0}, 1 \le i \le n</span>.<br />
<span class="math inline">\mathcal{B}</span> is linearly dependent if and only if <span class="math inline">\exists c_1, \ldots, c_n \in \mathbb{R}, c_1 \vec{v}_1 + \ldots + c_n \vec{v}_n = \vec{0} \wedge \exists 1 \le i \le n, c_i \ne 0</span>.<br />
Construct <span class="math inline">c_1, \ldots, c_{i - 1}, c_{i + 1}, \ldots, c_n = 0, c_i = 1</span>.<br />
Then <span class="math inline">c_i \ne 0</span> and <span class="math inline">c_1 \vec{v}_1 + \ldots + c_n \vec{v}_n = \vec{0}</span>.<br />
Therefore, <span class="math inline">\mathcal{B}</span> is linearly dependent.</p>
</blockquote>
<p>The vector equation of a linearly independent set is known as a simplified vector equation. A vector equation can be converted into a simplified one by removing terms where the vector can be written as a linear combination of the others.</p>
<p>Geometrically, two vectors span a plane if they are not parallel, and three vectors span a 3D space if they do not all lie on the same plane.</p>
<h2 id="bases">Bases</h2>
<p>A linearly independent set of vectors is always simpler than a linearly dependent one. A linearly dependent set of vectors can always be converted into a linearly independent one with the same span. Therefore, the simplest set of vectors that spans a given set is always linearly independent.</p>
<p>Given <span class="math inline">\mathcal{B} = \set{\vec{v}_1, \ldots, \vec{v}_n}</span>, <span class="math inline">\mathcal{B}</span> is a <strong>basis</strong> for <span class="math inline">\spn \mathcal{B}</span> if <span class="math inline">\mathcal{B}</span> is linearly independent.</p>
<p>In other words, a basis for a set is a linearly independent set of vectors such that its span is exactly the set.</p>
<p>The basis for <span class="math inline">\set{\vec{0}}</span> is the empty set (<span class="math inline">\emptyset</span>).</p>
<p>The <strong>standard basis</strong> is a basis that is easy to write for any number of dimensions. The standard basis is of the form <span class="math inline">\set{\vec{e}_1, \ldots, \vec{e}_n}</span>, where <span class="math inline">\vec{e}_i</span> has all components set to 0, except for the <span class="math inline">i</span>-th component, for <span class="math inline">1 \le i \le n</span>.</p>
<p>For example, the standard basis of <span class="math inline">\mb{R}^4</span> is <span class="math inline">\set{\begin{bmatrix} 1 \\ 0 \\ 0 \\ 0 \end{bmatrix}, \begin{bmatrix} 0 \\ 1 \\ 0 \\ 0 \end{bmatrix}, \begin{bmatrix} 0 \\ 0 \\ 1 \\ 0 \end{bmatrix}, \begin{bmatrix} 0 \\ 0 \\ 0 \\ 1 \end{bmatrix}}</span>.</p>
<p>In order to prove that a set is the basis of another, we need to prove that it is linearly independent, and that it spans the set (by proving that an arbitrary vector in the second set can be written as a linear combination of the elements of the first).</p>
<p>Prove that <span class="math inline">\mathcal{B} = \set{\begin{bmatrix} 1 \\ 3 \end{bmatrix}, \begin{bmatrix} -1 \\ -1 \end{bmatrix}}</span> is a basis for <span class="math inline">\mb{R}^2</span>:</p>
<blockquote>
<p>Let <span class="math inline">\begin{bmatrix} x_1 \\ x_2 \end{bmatrix} \in \mathbb{R}^2</span>.<br />
We will find the coefficients we want by solving <span class="math inline">\begin{bmatrix} x_1 \\ x_2 \end{bmatrix} = c_1 \begin{bmatrix} 1 \\ 3 \end{bmatrix} + c_2 \begin{bmatrix} -1 \\ -1 \end{bmatrix}</span> for <span class="math inline">c_1, c_2</span>.<br />
Construct <span class="math inline">c_1 = \frac{x_2 - x_1}{2}, c_2 = \frac{x_2 - 3x_1}{2}</span>.<br />
Clearly, <span class="math inline">\begin{bmatrix} x_1 \\ x_2 \end{bmatrix} = \frac{x_2 - x_1}{2} \begin{bmatrix} 1 \\ 3 \end{bmatrix} + \frac{x_2 - 3x_1}{2} \begin{bmatrix} -1 \\ -1 \end{bmatrix}</span>.<br />
So <span class="math inline">\exists c_1, c_2 \in \mathbb{R}, \begin{bmatrix} x_1 \\ x_2 \end{bmatrix} = c_1 \begin{bmatrix} 1 \\ 3 \end{bmatrix} + c_2 \begin{bmatrix} -1 \\ -1 \end{bmatrix}</span>.<br />
So <span class="math inline">\mathcal{B}</span> spans <span class="math inline">\mb{R}^2</span>.<br />
Note that there is only the trivial solution when <span class="math inline">\begin{bmatrix} x_1 \\ x_2 \end{bmatrix} = \begin{bmatrix} 0 \\ 0 \end{bmatrix}</span>, so <span class="math inline">\mathcal{B}</span> is also linearly independent.<br />
Therefore, <span class="math inline">\mathcal{B}</span> is a basis for <span class="math inline">\mb{R}^2</span>.</p>
</blockquote>
<p>A basis for <span class="math inline">\mb{R}^n</span> has exactly <span class="math inline">n</span> elements.</p>
<h2 id="shapes">Shapes</h2>
<p>Points are conceptually applicable in any dimension above 0.</p>
<p>Lines are conceptually applicable in any dimension above 1.</p>
<p>Planes are conceptually applicable in any dimension above 2.</p>
<p>A line in <span class="math inline">\mb{R}^n</span> takes the form of <span class="math inline">\vec{x} = c_1 \vec{v}_1 + \vec{b}, c_1 \in \mathbb{R}</span> given <span class="math inline">\vec{v}_1, \vec{b} \in \mathbb{R}^n</span>. This is a line in <span class="math inline">\mb{R}^n</span> that passes through <span class="math inline">\vec{b}</span>.</p>
<p>A plane in <span class="math inline">\mb{R}^n</span> takes the form of <span class="math inline">\vec{x} = c_1 \vec{v}_1 + c_2 \vec{v}_2 + \vec{b}, c_1, c_2 \in \mathbb{R}</span> given <span class="math inline">\vec{v}_1, \vec{v}_2, \vec{b} \in \mathbb{R}^n</span> and <span class="math inline">\set{\vec{v}_1, \vec{v}_2}</span> being a linearly independent set. This is a plane in <span class="math inline">\mb{R}^n</span> that passes through <span class="math inline">\vec{b}</span>.</p>
<p>A <span class="math inline">k</span>-plane in <span class="math inline">\mb{R}^n</span> takes the form of <span class="math inline">\vec{x} = c_1 \vec{v}_1 + \ldots + c_k \vec{v}_k + \vec{b}, c_1, c_2 \in \mathbb{R}</span> given <span class="math inline">\vec{v}_1, \ldots, \vec{v}_k, \vec{b} \in \mathbb{R}^n</span> and <span class="math inline">\set{\vec{v}_1, \ldots, \vec{v}_k}</span> being a linearly independent set. This is a <span class="math inline">k</span>-plane in <span class="math inline">\mb{R}^n</span> that passes through <span class="math inline">\vec{b}</span>. A <span class="math inline">k</span>-plane is a <span class="math inline">k</span>-dimensional plane.</p>
<p>A hyperplane is a <span class="math inline">k</span>-plane, where <span class="math inline">k = n - 1</span>. It is an <span class="math inline">n - 1</span>-dimensional plane.</p>
<p>For example, <span class="math inline">\spn \set{\begin{bmatrix} 1 \\ 0 \\ 0 \\ 1 \end{bmatrix}, \begin{bmatrix} 0 \\ 1 \\ 0 \\ -2 \end{bmatrix}, \begin{bmatrix} 0 \\ 1 \\ 1 \\ -1 \end{bmatrix}}</span> defines a hyperplane in <span class="math inline">\mb{R}^n</span>, since the set is linearly independent.</p>
<h1 id="section-1">20/1/14</h1>
<h2 id="subspaces">Subspaces</h2>
<p>A <strong>subspace</strong> of <span class="math inline">\mb{R}^n</span> is a non-empty subset <span class="math inline">\mb{S}</span> of <span class="math inline">\mb{R}^n</span> such that it satisfies all ten properties defined in Theorem 1.1.1.</p>
<p>All subspaces are <strong>spaces</strong>, which are subsets of a vector space. The simplest subspace is <span class="math inline">\set{\vec{0}}</span>.</p>
<p>But since properties 2-5 and 7-10 follow from properties 1 and 6, all we need to do to prove all ten properties is to prove properties 1 and 6 hold.</p>
<h3 id="theorem-1.2.1-subspace-test">Theorem 1.2.1 (Subspace Test)</h3>
<p>Given a set <span class="math inline">\mb{S}</span>, <span class="math inline">\mb{S}</span> is a subspace of <span class="math inline">\mb{R}^n</span> if and only if:</p>
<ul>
<li><span class="math inline">\mb{S}</span> is a subset of <span class="math inline">\mb{R}^n</span>.</li>
<li><span class="math inline">\mb{S}</span> is non-empty.</li>
<li><span class="math inline">\forall \vec{x}, \vec{y} \in \mb{S}, \vec{x} + \vec{y} \in \mb{S}</span>.</li>
<li><span class="math inline">\forall \vec{x} \in \mb{S}, c \in \mb{R}, c\vec{x} \in \mb{S}</span></li>
</ul>
<p>To prove a set is a subspace, we need to prove all four properties.</p>
<p>Clearly, if conditions 2 and 4 hold, <span class="math inline">\vec{0} \in \mb{S}</span>. So if <span class="math inline">\vec{0} \notin \mb{S}</span>, then one or both of the conditions is not met and the set is not a subspace.</p>
<p>We can use this to check if a set is a subspace by seeing if <span class="math inline">\vec{0}</span> is in it.</p>
<p>Prove <span class="math inline">\mb{S} = \set{ \begin{bmatrix} x \\ y \\ z \end{bmatrix} \middle| x + y = 0, x - z = 0}</span> is a subspace of <span class="math inline">\mb{R}^3</span> and write a basis for the subspace:</p>
<blockquote>
<p>Clearly, <span class="math inline">\mb{S}</span> is a subset of <span class="math inline">\mb{R}^3</span>.<br />
Clearly, <span class="math inline">\vec{0} \in \mb{S}</span>, so the set is non-empty.<br />
Let <span class="math inline">\vec{a}, \vec{b} \in \mb{S}</span>.<br />
So <span class="math inline">a_1 + a_2 = a_1 - a_3 = 0</span> and <span class="math inline">b_1 + b_2 = b_1 - b_3 = 0</span>.<br />
Clearly, <span class="math inline">(a_1 + b_1) + (a_2 + b_2) = (a_1 + b_1) - (a_3 + b_3) = 0</span>.<br />
So <span class="math inline">\vec{a} + \vec{b} \in \mb{S}</span> and <span class="math inline">\mb{S}</span> is closed under addition.<br />
Clearly, <span class="math inline">c(a_1 + a_2) = c(a_1 - a_3) = c0</span>.<br />
So <span class="math inline">c\vec{a} \in \mb{S}</span> and <span class="math inline">\mb{S}</span> is closed under scalar multiplication.<br />
So <span class="math inline">\mb{S}</span> is a subspace of <span class="math inline">\mb{R}^3</span>.<br />
Since <span class="math inline">a_1 + a_2 = a_1 - a_3 = 0</span>, <span class="math inline">a_2 = -a_1</span> and <span class="math inline">a_3 = a_1</span>.<br />
So the general vector is <span class="math inline">\vec{v}(a_1) = \begin{bmatrix} a_1 \\ -a_1 \\ a_1 \end{bmatrix} = a_1 \begin{bmatrix} 1 \\ -1 \\ 1 \end{bmatrix}</span>.<br />
So a basis for <span class="math inline">\mb{S}</span> is <span class="math inline">\set{\begin{bmatrix} 1 \\ -1 \\ 1 \end{bmatrix}}</span>.</p>
</blockquote>
<p>We find a basis of a subspace by finding the general form of a vector in the subspace, using this to find a spanning set for <span class="math inline">S</span>, and then reducing it into a linearly independent set.</p>
<h3 id="theorem-1.2.2">Theorem 1.2.2</h3>
<p>If <span class="math inline">\vec{v}_1, \ldots, \vec{v}_k \in \mb{R}^n</span>, then <span class="math inline">\mb{S} = \spn \set{\vec{v}_1, \ldots, \vec{v}_k}</span> is a subspace of <span class="math inline">\mb{R}^n</span>.</p>
<p>Proof:</p>
<blockquote>
<p>Clearly, the set is a subset of <span class="math inline">\mb{R}^n</span>.<br />
Clearly, <span class="math inline">\vec{0} \in \mb{S}</span>, since <span class="math inline">\vec{0} = 0\vec{v}_1 + \ldots + 0\vec{v}_k</span>.<br />
Let <span class="math inline">\vec{a}, \vec{b} \in \mb{S}, w \in \mb{R}</span>.<br />
Then <span class="math inline">\exists c_1, \ldots, c_k \in \mb{R}, \vec{a} = c_1 \vec{v}_1 + \ldots + c_k \vec{v}_k</span> and <span class="math inline">\exists d_1, \ldots, d_k \in \mb{R}, \vec{b} = d_1 \vec{v}_1 + \ldots + d_k \vec{v}_k</span>.<br />
So <span class="math inline">\vec{a} + \vec{b} = (c_1 + d_1) \vec{v}_1 + \ldots + (c_k + d_k) \vec{v}_k</span>, and <span class="math inline">\vec{a} + \vec{b} \in \mb{S}</span>.<br />
So <span class="math inline">w\vec{a} = wc_1 \vec{v}_1 + \ldots + wc_k \vec{v}_k</span> and <span class="math inline">w\vec{a} \in \mb{S}</span>.<br />
So <span class="math inline">\mb{S}</span> is a subspace of <span class="math inline">\mb{R}^n</span>.</p>
</blockquote>
<h2 id="dot-product">Dot Product</h2>
<p>The <strong>dot product</strong> of two vectors <span class="math inline">\vec{a}</span> and <span class="math inline">\vec{b}</span> is defined as <span class="math inline">\vec{a} \cdot \vec{b} = \begin{bmatrix} a_1 b_1 \\ \vdots \\ a_k b_k \end{bmatrix}</span>.</p>
<p>The dot product also has the geometric interpretation <span class="math inline">\vec{a} \cdot \vec{b} = \abs{a} \abs{b} \cos \theta</span>, where <span class="math inline">\theta</span> is the angle between the two vectors.</p>
<p>The dot product is also known as the <strong>scalar product</strong> or <strong>standard inner product</strong> of <span class="math inline">\mb{R}^n</span>.</p>
<h3 id="theorem-1.3.2">Theorem 1.3.2</h3>
<p>If <span class="math inline">\vec{x}, \vec{y}, \vec{z} \in \mb{R}^n, s, t, \in \mb{R}</span>, then:</p>
<ul>
<li><span class="math inline">\vec{x} \cdot \vec{x} \ge 0</span>, and <span class="math inline">\vec{x} \cdot \vec{x} = 0 \iff \vec{x} = \vec{0}</span>.</li>
<li><span class="math inline">\vec{x} \cdot \vec{y} = \vec{y} \cdot \vec{x}</span>.</li>
<li><span class="math inline">\vec{x} \cdot (s\vec{y} + t\vec{z}) = s (\vec{x} \cdot \vec{y}) + t (\vec{x} \cdot \vec{z})</span>.</li>
</ul>
<p>The <strong>length</strong> or <strong>norm</strong> (Euclidean norm) of a vector <span class="math inline">\vec{v} = \begin{bmatrix} v_1 \\ \vdots \\ v_k \end{bmatrix}</span> is <span class="math inline">\magn{\vec{v}} = \sqrt{\sum_{i = 1}^k v_i^2}</span>. Note the similarity to scalars, where <span class="math inline">\abs{x} = \sqrt{x^2}</span>.</p>
<p>A vector of length 1 is a unit vector. A unit vector is therefore one such that <span class="math inline">\sum_{i = 1}^k v_i^2 = 1</span>.</p>
<p>Also, <span class="math inline">\vec{x} \cdot \vec{x} = \magn{x}^2</span>, which should be obvious since <span class="math inline">\theta = 0</span> and <span class="math inline">\cos \theta = 1</span>, so <span class="math inline">\vec{x} \cdot \vec{x} = \magn{x} \magn{x} = \magn{x}^2</span>.</p>
<h3 id="theorem-1.3.3">Theorem 1.3.3</h3>
<p>If <span class="math inline">\vec{x}, \vec{y} \in \mb{R}^n, c \in \vec{R}</span>, then:</p>
<ul>
<li><span class="math inline">\magn{\vec{x}} \ge 0</span>, and <span class="math inline">\magn{\vec{x}} = 0 \iff \vec{x} = \vec{0}</span></li>
<li><span class="math inline">\magn{cx} = \abs{c}\magn{\vec{x}}</span></li>
<li><span class="math inline">\abs{\vec{x} \cdot \vec{y}} \le \magn{\vec{x}}\magn{\vec{y}}</span> (Cauchy-Schwarz-Buniakowski Inequality)</li>
<li><span class="math inline">\magn{\vec{x} + \vec{y}} \le \magn{\vec{x}} + \magn{\vec{y}}</span> (Triangle Inequality)</li>
</ul>
<p>Proof of third consequence:</p>
<blockquote>
<p>Clearly, if <span class="math inline">\vec{x} = \vec{0}</span> then the inequality holds. Assume <span class="math inline">\vec{x} \ne \vec{0}</span>.<br />
Clearly, <span class="math inline">\forall t \in \mb{R}, (t\vec{x} + \vec{y}) \cdot (t\vec{x} + \vec{y}) \ge 0</span> (from properties of dot product), so <span class="math inline">t^2(\vec{x} \cdot \vec{x}) + 2t(\vec{x} \cdot \vec{y}) + \vec{y} \cdot \vec{y} \ge 0</span>. Clearly, this is a quadratic polynomial where <span class="math inline">t</span> is the variable. The polynomial is greater or equal to 0 if and only if it has at most 1 root.<br />
So the discriminant in the quadratic formula, <span class="math inline">b^2 - 4ac</span>, must be less or equal to 0.<br />
So <span class="math inline">4(\vec{x} \cdot \vec{y})^2 - 4(\vec{x} \cdot \vec{x})(\vec{y} \cdot \vec{y}) \le 0</span>.<br />
So <span class="math inline">(\vec{x} \cdot \vec{y})^2 \le (\vec{x} \cdot \vec{x})(\vec{y} \cdot \vec{y})</span> and <span class="math inline">(\vec{x} \cdot \vec{y})^2 \le \magn{x}^2\magn{y}^2</span>.<br />
So <span class="math inline">\vec{x} \cdot \vec{y} \le \magn{x}\magn{y}</span>.</p>
</blockquote>
<h3 id="angle">Angle</h3>
<p>The <strong>angle</strong> between two vectors <span class="math inline">\vec{x}</span> and <span class="math inline">\vec{y}</span> is defined as the angle <span class="math inline">\theta</span> such that <span class="math inline">\vec{x} \cdot \vec{y} = \magn{x} \magn{y} \cos \theta</span>.</p>
<p>In other words, <span class="math inline">\theta = \arccos\left(\frac{\vec{x} \cdot \vec{y}}{\magn{x} \magn{y}}\right)</span>.</p>
<p>Two vectors are <strong>orthogonal</strong> if the angle between them is 90 degrees - if their dot product is 0.</p>
<p>One of the reasons the standard bases are easy to work with is specifically because they are orthogonal to each other and because they are unit vectors.</p>
<h3 id="planeshyperplanes">Planes/Hyperplanes</h3>
<p>We can use the dot product to derive a <strong>scalar equation</strong> form for a plane, that makes use of the fact that the normal of a plane is just a vector.</p>
<p>Let <span class="math inline">A(a_1, a_2, a_3)</span> be a fixed point on the plane. Let <span class="math inline">X(x_1, x_2, x_3)</span> be an arbitrary point on the plane.</p>
<p>Then <span class="math inline">\vec{X} - \vec{A}</span> is a vector that lies on the plane.</p>
<p>In other words, if <span class="math inline">\vec{x} = s\vec{u} + t\vec{v}, s, t \in \mb{R}</span>, then <span class="math inline">\vec{x} - \vec{b}</span> is clearly a vector that lies on the plane.</p>
<p>Let <span class="math inline">\vec{n}</span> be a vector normal to the plane. Clearly, <span class="math inline">\vec{n} \cdot (\vec{X} - \vec{A}) = 0</span>.</p>
<p>So <span class="math inline">n_1 (x_1 - a_1) + n_2 (x_2 - a_2) + n_3 (x_3 - a_3) = 0 = n_1 x_1 + n_2 x_2 + n_3 x_3 - n_1 a_1 - n_2 a_2 - n_3 a_3</span>.</p>
<p>So <span class="math inline">n_1 x_1 + n_2 x_2 + n_3 x_3 = n_1 a_1 + n_2 a_2 + n_3 a_3</span>. Since <span class="math inline">\vec{A}</span> is fixed, <span class="math inline">n_1 a_1 + n_2 a_2 + n_3 a_3</span> is constant and the equation is a function of <span class="math inline">x_1, x_2, x_3</span>.</p>
<p>Since <span class="math inline">\vec{x}</span> is arbitrary, this holds for every point on the plane, and so is an equation of the plane.</p>
<p>Using the above, we can find the scalar equation of any plane given the normal and a fixed point on the plane.</p>
<p>Additionally, we can easily extend this to hyperplanes. The scalar equation of a hyperplane given the normal <span class="math inline">\vec{n}</span> and fixed point <span class="math inline">\vec{a}</span> pn the plane is <span class="math inline">\vec{n} \cdot \vec{x} = \vec{n} \cdot \vec{a}</span>.</p>
<h3 id="cross-product">Cross Product</h3>
<p>However, we will typically be given the vector equation of the plane, like <span class="math inline">\vec{x} = c_1 \vec{u} + c_2 \vec{v} + \vec{w}</span>.</p>
<p>We can calculate the normal of the plane by finding two linearly independent vectors that lie on the plane, and finding a vector that is orthogonal to both of them.</p>
<p>Clearly, <span class="math inline">\vec{u}</span> and <span class="math inline">\vec{v}</span> lie on the plane, being the solutions when <span class="math inline">c_2 = 0</span> or <span class="math inline">c_1 = 0</span>, respectively.</p>
<p>Then for the normal <span class="math inline">\vec{n}</span> we know <span class="math inline">\vec{n} \cdot \vec{u} = \vec{n} \cdot \vec{v} = 0</span>. So <span class="math inline">n_1 u_1 + n_2 u_2 + n_3 u_3 = n_1 v_1 + n_2 v_2 + n_3 v_3 = 0</span>.</p>
<p>Solving two equations for three unknowns, we find that one possible solution is <span class="math inline">\vec{n} = \begin{bmatrix} u_2 v_3 - u_3 v_2 \\ u_3 v_1 - u_1 v_3 \\ u_1 v_2 - u_2 v_1 \end{bmatrix}</span>.</p>
<p>This problem is so common that we gave its solution a name, the <strong>cross product</strong>. The cross product of two vectors <span class="math inline">\vec{u}</span> and <span class="math inline">\vec{v}</span> is <span class="math inline">\vec{u} \times \vec{v} = \begin{bmatrix} u_2 v_3 - u_3 v_2 \\ u_3 v_1 - u_1 v_3 \\ u_1 v_2 - u_2 v_1 \end{bmatrix}</span>, and is always a vector that is orthogonal to both vectors.</p>
<p>Note that this operation is only defined in <span class="math inline">\mb{R}^3</span> and <span class="math inline">\mb{R}^7</span>.</p>
<h2 id="projections">Projections</h2>
<p>Given the vectors <span class="math inline">\vec{u}, \vec{v}, \vec{w} \in \mb{R}^n, \vec{v} \ne \vec{0}, \vec{v} \cdot \vec{w} = 0</span>, we want to write <span class="math inline">\vec{u}</span> as the sum of a scalar multiple of <span class="math inline">\vec{v}</span> and <span class="math inline">\vec{w}</span>, as the vector <span class="math inline">\vec{u} = c\vec{v} + \vec{w}, c \in \mb{R}</span>.</p>
<p>We first need to find out how much of <span class="math inline">\vec{u}</span> is in the direction of <span class="math inline">\vec{v}</span> - find <span class="math inline">c</span>. Clearly, <span class="math inline">\vec{u} \cdot \vec{v} = (c\vec{v} + \vec{w}) \cdot \vec{v} = c\magn{v}^2 + \vec{w} \cdot \vec{v} = c\magn{v}^2</span>.</p>
<p>So <span class="math inline">c = \frac{\vec{u} \cdot \vec{v}}{\magn{v}^2}</span>.</p>
<p>The <strong>projection</strong> of <span class="math inline">\vec{u}</span> onto <span class="math inline">\vec{v}</span> is defined as <span class="math inline">\proj_{\vec{v}} \vec{u} = c\vec{v} = \frac{\vec{u} \cdot \vec{v}}{\magn{v}^2}\vec{v}</span>, and is the vector along the same direction as <span class="math inline">\vec{v}</span> such that it has the same extent along <span class="math inline">\vec{v}</span> as <span class="math inline">\vec{u}</span>.</p>
<p>The <strong>perpendicular</strong> of <span class="math inline">\vec{u}</span> onto <span class="math inline">\vec{v}</span> is the vector that when added to the projection, results in <span class="math inline">\vec{u}</span>. Therefore, the perpendicular is <span class="math inline">\prp_{\vec{v}} \vec{u} = \vec{u} - \proj_{\vec{v}} \vec{u}</span>.</p>
<h3 id="planes">Planes</h3>
<p>How do we project a vector onto a plane? We can notice that the projection of a vector onto a plane is the perpendicular of the vector projected onto the normal of the plane.</p>
<p>Therefore, the projection of a vector <span class="math inline">\vec{v}</span> onto a plane with normal <span class="math inline">\vec{n}</span> is <span class="math inline">\prp_{\vec{n}} \vec{v}</span>.</p>
<h2 id="systems-of-linear-equations">Systems of Linear Equations</h2>
<p>A <strong>linear equation</strong> is an equation of the form <span class="math inline">a_1 x_1 + \ldots + a_n x_n = b, a_k, b \in \mb{C}, 1 \le k \le n, n \in \mb{N}</span>. This also includes equations that can be rewritten into this form.</p>
<p>A set of linear equations with the same variables <span class="math inline">x_1, \ldots, x_n</span> (including those where there are zero coefficients) is called a <strong>system of linear equations</strong>.</p>
<p>A system of linear equations can be written as <span class="math inline">\begin{cases} a_{1, 1} x_1 + \ldots + a_{1, n} x_n = b_1 \\ \vdots a_{m, 1} x_1 + \ldots + a_{m, n} x_n = b_m \\ \end{cases}</span>. Here, <span class="math inline">a_{i, j}</span> represents the <span class="math inline">j</span>th coefficient in the <span class="math inline">i</span>th equation.</p>
<p>A <strong>solution</strong> to a system of linear equation is a vector <span class="math inline">\vec{s} = \begin{bmatrix} s_1 \\ \vdots \\ s_n \end{bmatrix}, \vec{s} \in \mb{R}^n</span> such that if <span class="math inline">(\forall 1 \le i \le n, x_i = s_i)</span>, all the equations in the system are satisfied.</p>
<p>A system of linear equations is <strong>consistent</strong> if it has at least one solution, and <strong>inconsistent</strong> otherwise.</p>
<p>Two systems of linear equations are <strong>equivalent</strong> if and only if they both have the same solution set.</p>
<p>Interpreted geometrically, a system of <span class="math inline">m</span> linear equations with <span class="math inline">n</span> variables is a set of <span class="math inline">m</span> hyperplanes in <span class="math inline">\mb{R}^n</span>. The solution to this system is represented by a vector that lies on all these hyperplanes.</p>
<h3 id="theorem-2.1.1">Theorem 2.1.1</h3>
<p>If a set of linear equations is consistent with more than 1 solution, then it has infinite solutions.</p>
<p>In other words, if <span class="math inline">\begin{cases} a_{1, 1} x_1 + \ldots + a_{1, n} x_n = b_1 \\ \vdots \\ a_{m, 1} x_1 + \ldots + a_{m, n} x_n = b_m \\ \end{cases}</span> has solutions <span class="math inline">\vec{s}</span> and <span class="math inline">\vec{t}</span> such that <span class="math inline">\vec{s} \ne \vec{t}</span>, then <span class="math inline">\vec{s} + c(\vec{s} - \vec{t})</span> is a solution for all <span class="math inline">c \in \mb{R}</span>.</p>
<p>Proof:</p>
<blockquote>
<p>Assume <span class="math inline">\begin{cases} a_{1, 1} x_1 + \ldots + a_{1, n} x_n = b_1 \\ \vdots a_{m, 1} x_1 + \ldots + a_{m, n} x_n = b_m \\ \end{cases}</span> has solutions <span class="math inline">\vec{s}</span> and <span class="math inline">\vec{t}</span>.<br />
Clearly, for all <span class="math inline">1 \le i \le n</span>, <span class="math inline">a_{i, 1} (s_1 + c(s_1 - t_1)) + \ldots + a_{i, n} (s_n + c(s_n - t_n)) = (a_{i, 1} s_1 + \ldots + a_{i, n} s_n) + c(a_{i, 1} s_1 + \ldots + a_{i, n} s_n) - c(a_{i, 1} t_1 + \ldots + a_{i, n} t_n) = b_i + cb_i - cb_i = b_i</span>.<br />
So each equation is satisfied, and the system is consistent.</p>
</blockquote>
<p>The set of all solutions to a system of linear equations is known as a <strong>solution set</strong>.</p>
<h3 id="solving">Solving</h3>
<p>When we use the substitution and elimination techniques for solving systems of linear equations, we are forming new systems of lienar equations that have the same solution set as the original. We aim to obtain one that is easier to find the solution set for, and therefore solve the original system.</p>
<p>Note that when we solve a linear system, we don't really need the <span class="math inline">x</span> variables. Instead, we could write the system <span class="math inline">\begin{cases} a_{1, 1} x_1 + \ldots + a_{1, n} x_n = b_1 \\ \vdots a_{m, 1} x_1 + \ldots + a_{m, n} x_n = b_m \\ \end{cases}</span> more concisely as <span class="math inline">\left[\begin{array}{ccc|c} a_{1, 1} &amp; \ldots &amp; a_{1, n} &amp; b_1 \\ \vdots &amp; \vdots &amp; \vdots &amp; \vdots \\ a_{m, 1} &amp; \ldots &amp; a_{m, n} &amp; b_1 \end{array}\right]</span>.</p>
<p>This is called the <strong>augmented matrix</strong> of the system of linear equations.</p>
<p>The <strong>coefficient matrix</strong> of the system of linear equations is the same thing, but without the last column containing the constant value. In this case, it would be <span class="math inline">\left[\begin{array}{ccc} a_{1, 1} &amp; \ldots &amp; a_{1, n} \\ \vdots &amp; \vdots &amp; \vdots \\ a_{m, 1} &amp; \ldots &amp; a_{m, n} \end{array}\right]</span>.</p>
<p>We can combine a coefficient matrix <span class="math inline">A</span> and a constant vector <span class="math inline">\vec{b}</span> into an augmented matrix by using the <span class="math inline">\left[A \middle| \vec{b}\right]</span>. This simply adds <span class="math inline">\vec{b}</span> as a column to the end of <span class="math inline">A</span>.</p>
<p>Solve <span class="math inline">\begin{cases} 2x_1 + 3x_2 &amp;= 11 \\ 3x_1 + 6x_2 &amp;= 7 \\ \end{cases}</span>:</p>
<blockquote>
<p>We multiply the first equation by <span class="math inline">-3</span> to obtain <span class="math inline">-6x_1 - 9x_2 = -33</span>.<br />
We multiply the second equation by <span class="math inline">2</span> to obtain <span class="math inline">6x_1 + 12x_2 = 14</span>.<br />
Now we add the first equation to the second equation to obtain <span class="math inline">0x_1 + 3x_2 = -19</span>.<br />
Now we multiply the second equation by <span class="math inline">\frac{1}{3}</span> to obtain <span class="math inline">0x_1 + x_2 = -\frac{19}{3}</span>.<br />
Now we add the second equation, multiplied by <span class="math inline">9</span>, to the first equation to obtain <span class="math inline">-6x_1 + 0x_2 = -90</span>.<br />
Now we multiply the first equation by <span class="math inline">-6</span> to obtain <span class="math inline">x_1 + 0x_2 = 15</span>.<br />
So <span class="math inline">x_1 = 15</span> and <span class="math inline">x_2 = -\frac{19}{3}</span>.</p>
</blockquote>
<p>Now solve using operations on the matrix form of the equations, <span class="math inline">\left[\begin{array}{cc|c} 2 &amp; 3 &amp; 11 \\ 3 &amp; 6 &amp; 7 \end{array}\right]</span>:</p>
<blockquote>
<p>We multiply the first row by <span class="math inline">-3</span> to obtain <span class="math inline">\left[\begin{array}{cc|c} -6 &amp; -9 &amp; -33 \\ 3 &amp; 6 &amp; 7 \end{array}\right]</span>.<br />
We multiply the second row by <span class="math inline">2</span> to obtain <span class="math inline">\left[\begin{array}{cc|c} -6 &amp; -9 &amp; -33 \\ 6 &amp; 12 &amp; 14 \end{array}\right]</span>.<br />
We add the first to the seocnd to obtain <span class="math inline">\left[\begin{array}{cc|c} -6 &amp; -9 &amp; -33 \\ 0 &amp; 3 &amp; -19 \end{array}\right]</span>.<br />
We multiply the second equation by <span class="math inline">\frac{1}{3}</span> to obtain <span class="math inline">\left[\begin{array}{cc|c} -6 &amp; -9 &amp; -33 \\ 0 &amp; 1 &amp; -\frac{19}{3} \end{array}\right]</span>.<br />
Now we add the second equation multiplied by 9 to the first equation to obtain <span class="math inline">\left[\begin{array}{cc|c} -6 &amp; 0 &amp; -90 \\ 0 &amp; 1 &amp; -\frac{19}{3} \end{array}\right]</span>.<br />
Now we multiply the first equation by <span class="math inline">-\frac{1}{6}</span> to obtain <span class="math inline">\left[\begin{array}{cc|c} 1 &amp; 0 &amp; 15 \\ 0 &amp; 1 &amp; -\frac{19}{3} \end{array}\right]</span>.<br />
So <span class="math inline">x_1 = 15</span> and <span class="math inline">x_2 = -\frac{19}{3}</span>.</p>
</blockquote>
<p>Note that at every step, we had a system of linear equations that had the same solution set as the original system. Eventually, we reduced the matrix down into a form that we could easily read the values off of.</p>
<p>Also note that there were only really two different operations that we used to solve the system. To make it easier for computers to work with, we define an additional swapping operation:</p>
<ul>
<li>Multiplying a row by a non-zero scalar (<span class="math inline">cR_i</span> multiplies the <span class="math inline">i</span>th row by <span class="math inline">c \in \mb{R}, c \ne 0</span>).</li>
<li>Adding a multiple of one row to another (<span class="math inline">R_i + cR_j</span> adds the <span class="math inline">j</span>th row multiplied by <span class="math inline">c \in \mb{R}, c \ne 0, i \ne j</span> to the <span class="math inline">i</span>th row).</li>
<li>Swapping two rows (<span class="math inline">R_i \leftrightarrow R_j, i \ne j</span>).</li>
</ul>
<p>These operations we call the <strong>elementary row operations</strong> (EROs). Note that they are also fully reversible - all EROs have an inverse that undoes the effect of the operation. This is trivial to prove.</p>
<p>Two matrices are <strong>row equivalent</strong> if one can be transformed into another by application of EROs. Since EROs are reversible, if a matrix <span class="math inline">A</span> is row equivalent to <span class="math inline">B</span>, then <span class="math inline">B</span> is also transformable into <span class="math inline">A</span> via the inverse of those EROs, and so <span class="math inline">B</span> is row equivalent to <span class="math inline">A</span>. In other words, row equivalence is commutative.</p>
<p>A matrix of size <span class="math inline">m \times n</span> (&quot;<span class="math inline">m</span> by <span class="math inline">n</span>&quot;) has <span class="math inline">m</span> rows and <span class="math inline">n</span> columns. With <span class="math inline">x_{i, j}</span>, rows are indexed by <span class="math inline">i</span>, and columns by <span class="math inline">j</span>. This is called row-major ordering.</p>
<h3 id="row-reduced-echelon-form">Row Reduced Echelon Form</h3>
<p>A matrix is in <strong>reduced row echelon form/row canonical form</strong> (RREF) if and only if:</p>
<ol type="1">
<li>All rows with only zeroes are at the bottom. For example, <span class="math inline">\begin{bmatrix} 1 &amp; 2 \\ 0 &amp; 0 \end{bmatrix}</span>, but not <span class="math inline">\begin{bmatrix} 0 &amp; 0 \\ 1 &amp; 2 \end{bmatrix}</span>.</li>
<li>The first non-zero entry in each row is 1 (this is called the <strong>leading one</strong>). For example, <span class="math inline">\begin{bmatrix} 1 &amp; 0 &amp; 1 \\ 0 &amp; 1 &amp; 3 \end{bmatrix}</span>, but not <span class="math inline">\begin{bmatrix} 4 &amp; 0 &amp; 1 \\ 0 &amp; 1 &amp; 3 \end{bmatrix}</span>.</li>
<li>The leading one in each row, if it exists, appears to the right of the leading one of the row above it. For example, <span class="math inline">\begin{bmatrix} 1 &amp; 0 \\ 0 &amp; 1 \end{bmatrix}</span>, but not <span class="math inline">\begin{bmatrix} 1 &amp; 0 \\ 1 &amp; 0 \end{bmatrix}</span>.</li>
<li>The leading one in each row must be the only non-zero entry in its column. For example, <span class="math inline">\begin{bmatrix} 1 &amp; 0 &amp; 5 \\ 0 &amp; 1 &amp; 2 \end{bmatrix}</span>, but not <span class="math inline">\begin{bmatrix} 1 &amp; 3 &amp; 5 \\ 0 &amp; 1 &amp; 2 \end{bmatrix}</span>.</li>
</ol>
<p>All matrices have one and only one representation in RREF.</p>
<h3 id="theorem-2.2.2">Theorem 2.2.2</h3>
<p>The RREF of a matrix is unique. In other words, each matrix has at most one possible matrix that it is both row equivalent to, and is in RREF.</p>
<p>Proof:</p>
<blockquote>
<p>;wip: something about proving this in chapter 4</p>
</blockquote>
<p>Row reduce <span class="math inline">\left[\begin{array}{ccc|c} 1 &amp; 1 &amp; 0 &amp; -7 \\ 2 &amp; 4 &amp; 1 &amp; -16 \\ 1 &amp; 2 &amp; 1 &amp; 9 \end{array}\right]</span>:</p>
<blockquote>
<p><span class="math inline">\left[\begin{array}{ccc|c} 1 &amp; 1 &amp; 0 &amp; -7 \\ 2 &amp; 4 &amp; 1 &amp; -16 \\ 1 &amp; 2 &amp; 1 &amp; 9 \end{array}\right]</span> is row equivalent to <span class="math inline">\left[\begin{array}{ccc|c} 1 &amp; 1 &amp; 0 &amp; -7 \\ 0 &amp; 0 &amp; -1 &amp; -34 \\ 1 &amp; 2 &amp; 1 &amp; 9 \end{array}\right]</span> via <span class="math inline">R_2 + (-2)R_3</span>.<br />
This is row equivalent to <span class="math inline">\left[\begin{array}{ccc|c} 1 &amp; 1 &amp; 0 &amp; -7 \\ 1 &amp; 2 &amp; 1 &amp; 9 \\ 0 &amp; 0 &amp; -1 &amp; -34 \end{array}\right]</span> via <span class="math inline">R_2 \leftrightarrow R_3</span>.<br />
This is row equivalent to <span class="math inline">\left[\begin{array}{ccc|c} 1 &amp; 1 &amp; 0 &amp; -7 \\ 1 &amp; 2 &amp; 1 &amp; 9 \\ 0 &amp; 0 &amp; 1 &amp; 34 \end{array}\right]</span> via <span class="math inline">(-1)R_3</span>.<br />
This is row equivalent to <span class="math inline">\left[\begin{array}{ccc|c} 1 &amp; 1 &amp; 0 &amp; -7 \\ 0 &amp; 1 &amp; 1 &amp; 16 \\ 0 &amp; 0 &amp; 1 &amp; 34 \end{array}\right]</span> via <span class="math inline">R_2 - R_1</span>.<br />
This is row equivalent to <span class="math inline">\left[\begin{array}{ccc|c} 1 &amp; 1 &amp; 0 &amp; -7 \\ 0 &amp; 1 &amp; 0 &amp; -18 \\ 0 &amp; 0 &amp; 1 &amp; 34 \end{array}\right]</span> via <span class="math inline">R_2 - R_3</span>.<br />
This is row equivalent to <span class="math inline">\left[\begin{array}{ccc|c} 1 &amp; 0 &amp; 0 &amp; 11 \\ 0 &amp; 1 &amp; 0 &amp; -18 \\ 0 &amp; 0 &amp; 1 &amp; 34 \end{array}\right]</span> via <span class="math inline">R_2 - R_3</span>.<br />
This is in RREF.</p>
</blockquote>
<p>Note the general technique used in reducing the matrix. First, we want to obtain a triangle of 0 elements with leading ones diagonally rightwards, by going from top to bottom. Then, we make sure each leading one is the only non-zero element in its column by subtracting from rows below from bottom to top.</p>
<p>This technique is basically modifying the matrix until all the properties except for property 4. Then property 4 can be solved for relatively easily.</p>
<p>It is the basic technique behind <strong>Guass-Jordan elimination</strong>.</p>
<h3 id="solution-set">Solution Set</h3>
<p>A matrix is inconsistent if and only if its RREF contains a row of the form <span class="math inline">\left[\begin{array}{ccc|c} 0 &amp; \ldots &amp; 0 &amp; 1 \end{array}\right]</span> - no possible values can satisfy this equation (<span class="math inline">0 = 1</span>), so there are no solutions.</p>
<p>Recall that if consistent, a system of linear equations has either one or infinitely many solutions.</p>
<p>Solve <span class="math inline">\begin{cases} x_1 + x_2 + x_3 &amp;= 4 \\ x_2 + x_3 &amp;= 3 \end{cases}</span>:</p>
<blockquote>
<p>The augmented matrix is <span class="math inline">\left[\begin{array}{ccc|c} 1 &amp; 1 &amp; 1 &amp; 4 \\ 0 &amp; 1 &amp; 1 &amp; 3 \end{array}\right]</span>.<br />
The matrix in RREF is <span class="math inline">\left[\begin{array}{ccc|c} 1 &amp; 0 &amp; 0 &amp; 1 \\ 0 &amp; 1 &amp; 1 &amp; 3 \end{array}\right]</span>.<br />
Note that this corresponds to <span class="math inline">x_1 = 1</span> and <span class="math inline">x_2 + x_3 = 3</span>, and that the second equation has infinitely many solutions.<br />
We can find the general solution by writing everything in terms of the fewest variables possible.<br />
Clearly, <span class="math inline">x_3 = 3 - x_2</span>, so the solution is <span class="math inline">\vec{x} = \begin{bmatrix} 1 \\ x_2 \\ x_3 \end{bmatrix} = \begin{bmatrix} 1 \\ x_2 \\ 3 - x_2 \end{bmatrix} = \begin{bmatrix} 1 \\ 0 \\ 3 \end{bmatrix} + x_2\begin{bmatrix} 0 \\ 1 \\ -1 \end{bmatrix}</span>.<br />
We can denote this as <span class="math inline">\vec{x} = \begin{bmatrix} 1 \\ 0 \\ 3 \end{bmatrix} + s\begin{bmatrix} 0 \\ 1 \\ -1 \end{bmatrix}, s \in \mb{R}</span>, by assigning <span class="math inline">x_2</span> to the parameter <span class="math inline">s</span>.<br />
This represents a line at the intersection of two planes in <span class="math inline">\mb{R}^3</span>.</p>
</blockquote>
<p>The solution set is, geometrically, either a point, a line, a plane, a hyperplane, or empty (no solutions). If there is only one solution, the solution set is a point.</p>
<p>If a <strong>column</strong> in a coefficient matrix has <strong>no leading ones</strong>, the corresponding variable is called a <strong>free variable</strong>. A free variable is a variable that can be any value. Free variables exist if and only if there are an infinite number of solutions.</p>
<h3 id="solving-linear-systems">Solving Linear Systems</h3>
<p>We can take this technique and generalize it a bit to solve any linear system, or show it to not have any solutions:</p>
<ol type="1">
<li>Write the augmented matrix for the system.</li>
<li>Row reduce the matrix.</li>
<li>Write the linear system for the new, reduced matrix.</li>
<li>Check for equations of the form <span class="math inline">0 = b, b \ne 0</span>, and stop and flag as inconsistent if found.</li>
<li>Write all variables in terms of the free variables.</li>
<li>Assign a parameter to each free variable.<br />
</li>
<li>Write the components that are not already fully solved in terms of the free variables.</li>
<li>Write the solution as a linear combination of vectors using vector operations.</li>
</ol>
<p>The standard technique for row reducing matrices is Guass-Jordan elimination. However, this is not always the most efficient way to do it, and there are a lot of tricks that can be used to speed it up in some special cases.</p>
<p>;wip: rewrite my linear solver lib to multiply only, and divide only at the end, this is precision-preserving through bignum and gmp</p>
<p>However, we don't always need to solve a system. Sometimes, we just need to figure out if there are zero, one, or infinite solutions. We will now look at some tools that can be used to analyze this.</p>
<p>The <strong>rank</strong> of a matrix is the number of leading ones in the RREF of the matrix. This is equivalent to the number of variables minus the number of free variables.</p>
<p>The rank of a matrix <span class="math inline">A</span> is denoted <span class="math inline">\rank A</span>. It is always true that <span class="math inline">n \ge \rank A</span> and <span class="math inline">m \ge \rank A</span>.</p>
<h3 id="theorem-2.2.3">Theorem 2.2.3</h3>
<p>Given an <span class="math inline">m \times n</span> coefficient matrix <span class="math inline">A</span> for a linear system:</p>
<ul>
<li>If <span class="math inline">\rank A &lt; \rank \sys{A}{\vec{b}}</span>, then the system is inconsistent, where <span class="math inline">\vec{b}</span> is the right hand side vector for the linear system. In other words, if the rank of the coefficent matrix is less than that of the augmented matrix, then the matrix is inconsistent.</li>
<li>If <span class="math inline">\sys{A}{\vec{b}}</span> is consistent, then there are <span class="math inline">n - \rank A</span> free variables. If there are 0 free variables, then the system has only one solution.</li>
<li><span class="math inline">\rank A = m</span> if and only if the system <span class="math inline">\sys{A}{\vec{b}}</span> is consistent for every <span class="math inline">\vec{b} \in \mb{R}^m</span>. In other words, there are no free variables if and only if the the system is consistent for every possible vector of right hand side values.</li>
</ul>
<p>Proof of first:</p>
<p>(point 1)</p>
<blockquote>
<p>Assume <span class="math inline">\rank A &lt; \rank \sys{A}{\vec{b}}</span>.<br />
Then there must be a row in the RREF of the augmented matrix with all zeros except the last element, which is 1.<br />
This corresponds to <span class="math inline">0 = 1</span>, which makes the system inconsistent.</p>
</blockquote>
<p>Proof:</p>
<p>(point 2)</p>
<blockquote>
<p>Assume <span class="math inline">\sys{A}{\vec{b}}</span> is consistent.<br />
The number of free variables is the number of variables minus the number of those that are not free.<br />
Those that are not free have leading ones, so <span class="math inline">\rank A</span> is the number of non-free variables.<br />
So the number of free variables is <span class="math inline">n - \rank A</span>.</p>
</blockquote>
<p>Proof:</p>
<p>(point 3)</p>
<blockquote>
<p>First, we will prove that if the system <span class="math inline">\sys{A}{\vec{b}}</span> is not consistent for every <span class="math inline">\vec{b} \in \mb{R}^m</span>, then <span class="math inline">\rank A \ne m</span>.<br />
Suppose <span class="math inline">\sys{A}{\vec{b}}</span> is inconsistent for some <span class="math inline">\vec{b} \in \mb{R}^m</span>.<br />
Then there must be a row in the RREF of the augmented matrix with all zeros except the last element, which is 1.<br />
This row does not contain a leading one, so <span class="math inline">\rank A &lt; m</span>.<br />
Now we will prove that if <span class="math inline">\rank A \ne m</span>, then the system <span class="math inline">\sys{A}{\vec{b}}</span> is not consistent for every <span class="math inline">\vec{b} \in \mb{R}^m</span>.<br />
Suppose <span class="math inline">\rank A \ne m</span>. Then <span class="math inline">\rank A &lt; m</span>. Then there is a row of all zeroes in the RREF of A.<br />
Then there must be a row in the RREF of the augmented matrix with all zeros except the last element, which is 1.<br />
Then the system is inconsistent.</p>
</blockquote>
<h3 id="homogenous-systems">Homogenous Systems</h3>
<p>A system is <strong>homogenous</strong> if and only if the right-hand side contains only zeros. In other words, it has the form <span class="math inline">\sys{A}{\vec{0}}</span>.</p>
<p>A homogenous matrix is one where the last column is all 0 - a matrix corresponding to a homogenous system.</p>
<p>Note that since the right side is always 0, the RREF of a homogenous matrix also has the right side always 0.</p>
<p>Also note that it is impossible to get a row where every element is 0 except the right hand side. Therefore, <strong>homogenous systems are always consistent</strong>.</p>
<p>For example, <span class="math inline">\vec{0}</span> is always a solution to a homogenous system. This is called the <strong>trivial solution</strong>.</p>
<p>When we row reduce a homogenous matrix, we can save space by just row reducing the coefficient matrix instead. The last column will all be 0 anyways.</p>
<h3 id="theorem-2.2.4">Theorem 2.2.4</h3>
<p>Given an <span class="math inline">m \times n</span> matrix <span class="math inline">A</span>, <span class="math inline">\rank A \le \min{m, n}</span>.</p>
<h3 id="theorem-2.2.5">Theorem 2.2.5</h3>
<p>The solution set of a homogenous system with <span class="math inline">n</span> variables is a subspace of <span class="math inline">\mb{R}^n</span>.</p>
<p>Note that the solution set of a homogenous system written as a vector equation always has <span class="math inline">\vec{0}</span> as the constant term. That means that all solutions are linear combinations of a few vectors.</p>
<p>As a result, we can always write the solution set of a homogenous system as the span of a set of vectors.</p>
<p>Because all spans are subspaces (by Theorem 1.2.2), the solution set is a subspace.</p>
<p>Because the solution set is a subspace, we often call it the <strong>solution space</strong> of a aystem.</p>
<h1 id="section-2">30/1/14</h1>
<h2 id="matrices">Matrices</h2>
<p>Matrices are abstract objects like vectors. We usually denote matrix variables with capital letters.</p>
<p>An <span class="math inline">m \times n</span> matrix <span class="math inline">A</span> is a rectangular array with <span class="math inline">m</span> rows and <span class="math inline">n</span> columns. We represent the <span class="math inline">i</span>th row and <span class="math inline">j</span>th column as <span class="math inline">a_{i, j}</span>, or <span class="math inline">(A)_{i, j}</span>.</p>
<p>In other words, <span class="math inline">A = \begin{bmatrix} a_{1, 1} &amp; \ldots &amp; a_{1, n} \\ \vdots &amp; \vdots &amp; \vdots \\ a_{m, 1} &amp; \ldots &amp; a_{m, n} \end{bmatrix}</span>.</p>
<p>All <span class="math inline">m \times n</span> matrices belong to the set <span class="math inline">M_{m \times n}</span>, similar to how all <span class="math inline">n</span>-dimensional vectors belong to the set <span class="math inline">\mb{R}^n</span>.</p>
<h3 id="operations-1">Operations</h3>
<p>Let <span class="math inline">A, B, C</span> be matrices.</p>
<p>Equality: <span class="math inline">A = B \iff \forall 1 \le i \le m, \forall 1 \le j \le n, (A)_{i, j} = (B)_{i, j}</span>.</p>
<p>In other words, two matrices are equal if and only if they are both the same size and contain the same entries.</p>
<p>Addition/subtraction: <span class="math inline">A \pm B = C \iff \forall 1 \le i \le m, \forall 1 \le j \le n, (C)_{i, j} = (A)_{i, j} \pm (B)_{i, j}</span>.</p>
<p>In other words, when we add matrices (only defined for matrices of the same size), we simply add their corresponding entries.</p>
<p>Scalar multiplication: <span class="math inline">t \in \mb{R}; tA = C \iff \forall 1 \le i \le m, \forall 1 \le j \le n, (C)_{i, j} = t(A)_{i, j}</span>.</p>
<p>In other words, when we multiply a matrix by a scalar or a scalar by a matrix, we simply multiply each entry in the matrix by the scalar.</p>
<p>With these operations, we can take linear combinations of matrices.</p>
<h3 id="theorem-3.1.1">Theorem 3.1.1</h3>
<p>For all <span class="math inline">A, B, C \in M_{m \times n}, s, t, \in \mb{R}</span>:</p>
<ul>
<li>Closure under addition: <span class="math inline">A + B \in M_{m \times n}</span>.</li>
<li>Associativity: <span class="math inline">(A + B) + C = A + (B + C)</span></li>
<li>Commutativity: <span class="math inline">A + B = B + A</span></li>
<li>Additive identity: <span class="math inline">\exists O \in M_{m \times n}, \forall A \in M_{m, n}, A + O = A</span> (<span class="math inline">O</span> is called the zero matrix).</li>
<li>Additive inverse: <span class="math inline">\forall A \in M_{m, n}, \exists -A \in M_{m, n}, A + (-A) = O</span>.</li>
<li>Closure under scalar multiplication: <span class="math inline">cA \in M_{m, n}</span>.</li>
<li><span class="math inline">c(dA) = (cd)A</span></li>
<li>Scalar distributivity: <span class="math inline">(c + d)A = cA + dA</span>.</li>
<li>Vector distributivity: <span class="math inline">c(A + B) = cA + cB</span>.</li>
<li>Multiplicative identity: <span class="math inline">1A = A</span>.</li>
</ul>
<h3 id="transpose">Transpose</h3>
<p>Vectors and matrices are closely related. We have been writing vectors as columns for a while now. In fact, we can use vectors to represent the columns of a matrix.</p>
<p>For example, if <span class="math inline">\vec{u} = \begin{bmatrix} 1 \\ 2 \\ 3 \end{bmatrix}, \vec{v} = \begin{bmatrix} 4 \\ 5 \\ 6 \end{bmatrix}</span>, then <span class="math inline">\begin{bmatrix} \vec{u} &amp; \vec{v} \end{bmatrix} = \begin{bmatrix} 1 &amp; 4 \\ 2 &amp; 5 \\ 3 &amp; 6 \end{bmatrix}</span>.</p>
<p>The <strong>transpose</strong> of a matrix <span class="math inline">A \in M_{m, n}</span> is <span class="math inline">A^T \in M_{n, m}</span> (&quot;A transposed&quot;), a matrix such that <span class="math inline">(A)_{i, j} = (A^T)_{j, i}</span>.</p>
<p>Note that the dimensions were swapped around. Basically, the rows became the columns, and the columns became the rows - we reflected the entries around a diagonal line starting from the top left.</p>
<ul>
<li><span class="math inline">A = \begin{bmatrix} 1 \\ 2 \\ 3 \end{bmatrix}; A^T = \begin{bmatrix} 1 &amp; 2 &amp; 3 \end{bmatrix}</span>.</li>
<li><span class="math inline">B = \begin{bmatrix} 1 &amp; 2 \\ 3 &amp; 4 \\ 5 &amp; 6 \end{bmatrix}; B^T = \begin{bmatrix} 1 &amp; 3 &amp; 5 \\ 2 &amp; 4 &amp; 6 \end{bmatrix}</span>.</li>
<li><span class="math inline">C = \begin{bmatrix} 1 &amp; 0 \\ 0 &amp; 3 \end{bmatrix}; C^T = \begin{bmatrix} 1 &amp; 0 \\ 0 &amp; 3 \end{bmatrix}</span>.</li>
</ul>
<p>Some matrices satisfy the property <span class="math inline">A = A^T</span>. These matrices are called <strong>symmetric</strong>, because they are symmetric about the diagonal line. Note that only square matrices can be symmetric.</p>
<p>Transposes are useful for representing the rows of a matrix with vectors, rather than columns.</p>
<p>For example, if <span class="math inline">\vec{u} = \begin{bmatrix} 1 \\ 2 \\ 3 \end{bmatrix}, \vec{v} = \begin{bmatrix} 4 \\ 5 \\ 6 \end{bmatrix}</span>, then <span class="math inline">\begin{bmatrix} \vec{u}^T \\ \vec{v}^T \end{bmatrix} = \begin{bmatrix} 1 &amp; 2 &amp; 3 \\ 4 &amp; 5 &amp; 6 \end{bmatrix}</span>.</p>
<h3 id="theorem-3.1.2">Theorem 3.1.2</h3>
<p>For all <span class="math inline">A, B \in M_{m, n}, c \in \mb{R}</span>:</p>
<ul>
<li><span class="math inline">(A^T)^T = A</span></li>
<li><span class="math inline">(A + B)^T = A^T + B^T</span></li>
<li><span class="math inline">(cA)^T = c(A^T)</span></li>
</ul>
<p>Also, <span class="math inline">(AB)^T = B^T A^T</span>. Note that the order reversed.</p>
<p>Proof:</p>
<p>(proof of first)</p>
<blockquote>
<p>Clearly, <span class="math inline">\forall 1 \le i \le m, \forall 1 \le j \le n, ((A^T)^T)_{i, j} = (A^T)_{j, i} = (A)_{i, j}</span>.<br />
So <span class="math inline">(A^T)^T = A</span>.</p>
</blockquote>
<p>(proof of second)</p>
<blockquote>
<p>Clearly, <span class="math inline">\forall 1 \le i \le m, \forall 1 \le j \le n, ((A + B)^T)_{i, j} = (A + B)_{j, i} = (A)_{j, i} + (B)_{j, i} = (A^T)_{i, j} + (B^T)_{i, j}</span>.<br />
So <span class="math inline">(A + B)^T = A^T + B^T</span>.</p>
</blockquote>
<p>(proof of third)</p>
<blockquote>
<p>Clearly, <span class="math inline">\forall 1 \le i \le m, \forall 1 \le j \le n, ((cA)^T)_{i, j} = (cA)_{j, i} = c((A)_{j, i}) = c((A^T)_{i, j})</span>.<br />
So <span class="math inline">(cA)^T = c(A^T)</span>.</p>
</blockquote>
<h1 id="section-3">1/2/14</h1>
<h2 id="matrix-multiplication">Matrix Multiplication</h2>
<h3 id="matrix-vector-multiplication">Matrix-Vector Multiplication</h3>
<p>Consider the linear system <span class="math inline">\sys{A}{\vec{B}}</span> with the solution <span class="math inline">\vec{x}</span>.</p>
<p>When we multiply a matrix by a vector, we want to have it so that <span class="math inline">A\vec{x} = \vec{b}</span>. This is because it would be very useful to help solve systems of linear equations.</p>
<p>Matrix-vector multiplication can be defined in terms of matrices with represented with row vectors or column vectors.</p>
<p>We will first define it in terms of row vectors.</p>
<p>Let <span class="math inline">A = \begin{bmatrix} \vec{a}_1^T \\ \vdots \\ \vec{a}_m^T \end{bmatrix}</span> and <span class="math inline">A \in M_{m, n}</span>.</p>
<p>Note that <span class="math inline">\vec{a}_i^T</span> represents the coefficients of the <span class="math inline">i</span>th equation of the system. So if and only if <span class="math inline">\vec{x}</span> is a solution, <span class="math inline">\vec{x} \cdot \vec{a}_i^T = b_i</span>.</p>
<p>Now we can define matrix multiplication: <span class="math inline">A\vec{x} = \begin{bmatrix} \vec{a}_1^T \cdot \vec{x} \\ \vdots \\ \vec{a}_m^T \cdot \vec{x} \end{bmatrix} = \vec{b}</span>.</p>
<p>Basically, we take the dot product of each row and assemble it together into a vector. The product of a matrix and a vector is therefore a vector.</p>
<p>Note that in order for matrix-vector multiplication to be defined, the number of columns in <span class="math inline">A</span> must equal the number of rows/components in <span class="math inline">\vec{x}</span> in order for the dot product to work. An <span class="math inline">m \times n</span> matrix can only be multiplied by vectors in <span class="math inline">\mb{R}^n</span>.</p>
<p>Now we can also define it in terms of column vectors.</p>
<p>Note that the system <span class="math inline">\begin{cases} a_{1, 1} x_1 + \ldots + a_{1, n} x_n = b_1 \\ \vdots a_{m, 1} x_1 + \ldots + a_{m, n} x_n = b_m \\ \end{cases}</span> has the coefficient matrix <span class="math inline">A = \begin{bmatrix} \begin{bmatrix} a_{1, 1} \\ \vdots \\ a_{m, 1} \end{bmatrix} \cdots \begin{bmatrix} a_{1, n} \\ \vdots \\ a_{m, n} \end{bmatrix} \end{bmatrix} = \begin{bmatrix} \vec{c}_1 \ldots \vec{c}_n \end{bmatrix}</span>, where <span class="math inline">\vec{c}_i</span> represents the <span class="math inline">i</span>th column of the matrix.</p>
<p>Note that the system can be written as <span class="math inline">x_1\begin{bmatrix} a_{1, 1} \\ \vdots \\ a_{m, 1} \end{bmatrix} + \ldots + x_n\begin{bmatrix} a_{1, n} \\ \vdots \\ a_{m, n} \end{bmatrix} = \vec{b}</span>.</p>
<p>So we can write the system as <span class="math inline">x_1 \vec{c}_1 + \ldots + x_n \vec{c}_n = \vec{b}</span>.</p>
<p>Now we define matrix-vector multiplication in terms of columns as <span class="math inline">A\vec{x} = x_1 \vec{c}_1 + \ldots + x_n \vec{c}_n</span>, where <span class="math inline">\vec{c}_i</span> represents the <span class="math inline">i</span>th column of <span class="math inline">A</span>.</p>
<p>We define it this way because now we can represent the system with <span class="math inline">A\vec{x} = \vec{b}</span>.</p>
<p>Note that we can use this to select one particular column of the matrix using a standard basis vector (a vector of all zeros except a lone one).</p>
<p>For example, multiplying an <span class="math inline">m \times n</span> matrix by the <span class="math inline">i</span>th standard basis vector in <span class="math inline">\mb{R}^n</span> (the vector with all components set to 0 except for the <span class="math inline">ith</span> component, which is set to 1) will result in the <span class="math inline">i</span>th column of the matrix.</p>
<h3 id="matrix-matrix-multiplication">Matrix-Matrix Multiplication</h3>
<p>Now we can define the product of a matrix and a matrix.</p>
<p>Given an <span class="math inline">m \times n</span> matrix <span class="math inline">A = \begin{bmatrix} \vec{a}_1^T \\ \vdots \\ \vec{a}_n^T \end{bmatrix}</span> and an <span class="math inline">n \times p</span> matrix <span class="math inline">B = \begin{bmatrix} \vec{b}_1 &amp; \ldots &amp; \vec{b}_p \end{bmatrix}</span>, <span class="math inline">AB = A\begin{bmatrix} \vec{b}_1 &amp; \ldots &amp; \vec{b}_p \end{bmatrix} = \begin{bmatrix} A\vec{b}_1 &amp; \ldots &amp; A\vec{b}_p \end{bmatrix}</span> is an <span class="math inline">m \times p</span> matrix.</p>
<p>Basically, when we multiply a matrix by a matrix, we multiply the first matrix by each of the column vectors in the second matrix.</p>
<p>Note that this requires that the number of columns in <span class="math inline">A</span> must be the same as the number of rows in <span class="math inline">B</span>, since the dot product must be defined.</p>
<p>Note that <span class="math inline">(AB)_{i, j} = \vec{a}_i^T \vec{b}_j = \vec{a}_i \cdot \vec{b}_j = \sum_{k = 1}^n (a_i^T)_k (\vec{a}_k)_j = \sum_{k = 1}^n (A)_{i, k} (B)_{k, j}</span>.</p>
<h3 id="theorem-3.1.3-properties-of-matrix-multiplication">Theorem 3.1.3 (Properties of Matrix Multiplication)</h3>
<p>If <span class="math inline">A, B, C</span> are matrices of the required dimensions, <span class="math inline">t \in \mb{R}</span>:</p>
<ul>
<li>Distributivity: <span class="math inline">A(B + C) = AB + AC</span> (left distributive) and <span class="math inline">(A + B)C = AC + BC</span> (right distributive).</li>
<li>Scalar multiplication: <span class="math inline">t(AB) = (tA)B = A(tB)</span>.</li>
<li>Associativity: <span class="math inline">(AB)C = A(BC)</span>.</li>
<li>Transposition distributivity: <span class="math inline">(AB)^T = B^T A^T</span>.</li>
</ul>
<p>Note that matrix multiplication is not commutative over matrices: there are matrices <span class="math inline">A</span> and <span class="math inline">B</span> such that <span class="math inline">AB \ne BA</span>.</p>
<h3 id="theorem-3.1.4-matrix-multiplication-cancellation">Theorem 3.1.4 (Matrix Multiplication Cancellation)</h3>
<p>Note that <span class="math inline">A = B \implies AC = BC \wedge CA = BA</span>. However, <span class="math inline">AC = BC \vee CA = BA</span> <strong>does not imply</strong> <span class="math inline">A = B</span> - there is no cancellation law for matrix multiplication.</p>
<p>That said, given <span class="math inline">A, B \in M_{m, n}</span>, <span class="math inline">(\forall \vec{x} \in \mb{R}^n, A\vec{x} = B\vec{x}) \implies A = B</span>.</p>
<p>In other words, if the product of a matrix with an arbitrary vector are equal to each other, then the matrices are themselves equal to each other.</p>
<p>Proof:</p>
<blockquote>
<p>Let <span class="math inline">A = \begin{bmatrix} \vec{a}_1 &amp; \ldots &amp; \vec{a}_n \end{bmatrix}, B = \begin{bmatrix} \vec{b}_1 &amp; \ldots &amp; \vec{b}_n \end{bmatrix}</span>.<br />
Let <span class="math inline">i \in \mb{N}, i \le n</span>.<br />
Let <span class="math inline">\vec{e}_i</span> be the <span class="math inline">i</span>th standard basis vector in <span class="math inline">\mb{R}^n</span> (the vector with all zeroes except the <span class="math inline">i</span>th element, which is 1).<br />
Let <span class="math inline">\vec{x} \in \mb{R}^n</span>. Construct <span class="math inline">\vec{x} = \vec{e}_i</span>.<br />
Assume <span class="math inline">A\vec{x} = B\vec{x}</span>. Then <span class="math inline">A\vec{e}_i = B\vec{e}_i = \vec{a}_i = \vec{b}_i</span>.<br />
Since <span class="math inline">i</span> is arbitrary, <span class="math inline">\forall 1 \le i \le n, \vec{a}_i = \vec{b}_i</span>.<br />
So <span class="math inline">A = B</span>.</p>
</blockquote>
<h1 id="section-4">3/2/14</h1>
<h2 id="identity-matrix">Identity Matrix</h2>
<p>The identity matrix is the matrix such that, when multiplied with any matrix, or when any matrix is multiplied by it, results in that same matrix.</p>
<p>Formally, the <strong>identity matrix</strong> is the unique matrix <span class="math inline">I \in M_{n, n}</span> such that <span class="math inline">I_{i, j} = \begin{cases} 1 &amp;\text{if } i = j \\ 0 &amp;\text{if } i \ne j \end{cases}</span>.</p>
<p>We denote this with <span class="math inline">I_n</span> to represent the square matrix of dimensions <span class="math inline">n \times n</span>. We can also write it as <span class="math inline">I_n = \begin{bmatrix} \vec{e}_1 &amp; \ldots &amp; \vec{e}_n \end{bmatrix}</span>.</p>
<p>In other words, we want to find <span class="math inline">I</span> such that <span class="math inline">\forall A \in M_{m, n}, AI = IA = A</span>.</p>
<p>Note that this is only possible when <span class="math inline">m = n</span>, since if it wasn't, the product of the matrices would result in a matrix of a different size.</p>
<p>We will now find the identity matrix. We want <span class="math inline">A = AI</span>. So <span class="math inline">\begin{bmatrix} \vec{a}_1 &amp; \ldots &amp; \vec{a}_n \end{bmatrix} = A\begin{bmatrix} \vec{i}_1 &amp; \ldots &amp; \vec{i}_n \end{bmatrix} = \begin{bmatrix} A\vec{i}_1 &amp; \ldots &amp; A\vec{i}_n \end{bmatrix}</span>.</p>
<p>So <span class="math inline">\vec{a}_c = A\vec{i}_c</span> for all <span class="math inline">1 \le c \le n</span>. Recall that <span class="math inline">\vec{i}_c = \vec{e}_c \implies \vec{a}_c = A\vec{i}_c</span>, where <span class="math inline">\vec{e}_c</span> is the <span class="math inline">c</span>th standard basis vector. So <span class="math inline">\vec{i}_c = \vec{e}_c</span>.</p>
<p>Therefore, one possible value of <span class="math inline">I</span> is <span class="math inline">\begin{bmatrix} \vec{e}_1 &amp; \ldots &amp; \vec{e}_n \end{bmatrix}</span>.</p>
<p>We can also construct a similar proof for <span class="math inline">A = IA</span>.</p>
<h3 id="thoerem-3.1.5">Thoerem 3.1.5</h3>
<p>If <span class="math inline">I = \begin{bmatrix} \vec{e}_1 &amp; \ldots &amp; \vec{e}_n \end{bmatrix}</span>, then <span class="math inline">A = AI = IA</span>.</p>
<h3 id="theorem-3.1.6">Theorem 3.1.6</h3>
<p>The multiplicative identity for <span class="math inline">M_{n, n}</span> is unique.</p>
<p>Proof:</p>
<blockquote>
<p>Let <span class="math inline">I_1, I_2</span> be two possibly equal multiplicative identities in <span class="math inline">M_{n, n}</span>.<br />
Since <span class="math inline">I_1</span> is a multiplicative identity, <span class="math inline">I_2 = I_1 I_2</span>.<br />
Since <span class="math inline">I_2</span> is a multiplicative identity, <span class="math inline">I_1 = I_1 I_2</span>.<br />
So <span class="math inline">I_1 = I_2</span> and all multiplicative identities are equal to each other.<br />
Therefore, multiplicative identities are unique.</p>
</blockquote>
<p>For example, for <span class="math inline">I \in M_{4, 4}</span>, <span class="math inline">I = \begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; 1 \end{bmatrix}</span>.</p>
<h2 id="block-matrices">Block Matrices</h2>
<p>Previously, we defined multiplication by splitting up a matrix into its columns. As it turns out, it's also useful to split a matrix up in other ways.</p>
<p>When we write <span class="math inline">A = \begin{bmatrix} \vec{a}_1 &amp; \ldots &amp; \vec{a}_n \end{bmatrix}</span>, we actually split up <span class="math inline">A</span> into <span class="math inline">n</span> different blocks, each one with only one column (so we can represent them using vectors).</p>
<p>A <strong>block matrix</strong> is a matrix that contains <strong>blocks</strong>, which are simply smaller matrices that we build the larger matrix out of.</p>
<p>Formally, if <span class="math inline">A</span> is an <span class="math inline">m \times n</span> matrix, then <span class="math inline">A</span> can be written as a <span class="math inline">u \times v</span> block matrix <span class="math inline">A = \begin{bmatrix} A_{1, 1} &amp; \ldots &amp; A_{1, v} \\ \vdots &amp; \vdots &amp; \vdots \\ A_{u, 1} &amp; \ldots &amp; A_{u, v} \end{bmatrix}</span>, where <span class="math inline">A_{x, y}</span> is a block and all blocks in any given row in <span class="math inline">A</span> have the same number of rows, and all blocks in any given column in <span class="math inline">A</span> have the same number of columns.</p>
<p>For example, given <span class="math inline">A = \begin{bmatrix} 1 &amp; 2 &amp; 3 &amp; 4 \\ 5 &amp; 6 &amp; 7 &amp; 8 \end{bmatrix}</span>, we can split it up to form a block matrix <span class="math inline">A = \begin{bmatrix} B &amp; C \end{bmatrix}</span>, where <span class="math inline">B = \begin{bmatrix} 1 &amp; 2 \\ 5 &amp; 6 \end{bmatrix}</span> and <span class="math inline">C = \begin{bmatrix} 3 &amp; 4 \\ 7 &amp; 8 \end{bmatrix}</span>.</p>
<p>When we multiply block matrices, we can actually treat the blocks as simple values and multiply them using the same rules as we would use for numbers.</p>
<p>For example, <span class="math inline">\begin{bmatrix} A_{1, 1} &amp; A_{1, 2} \end{bmatrix} \begin{bmatrix} B_{1, 1} \\ B_{2, 1} \end{bmatrix} = \begin{bmatrix} A_{1, 1} B_{1, 1} + A_{1, 2} B_{2, 1} \end{bmatrix}</span>.</p>
<h1 id="section-5">8/2/14</h1>
<h2 id="matrix-mappings">Matrix Mappings</h2>
<p>A <strong>function</strong> is a rule that associates <span class="math inline">x \in A</span> to <span class="math inline">f(x) \in B</span>. This is written as <span class="math inline">f: A \to B</span>, where <span class="math inline">A</span> is the <strong>domain</strong> of <span class="math inline">f</span>, and <span class="math inline">B</span> is the <strong>codomain</strong> of <span class="math inline">f</span>.</p>
<p><span class="math inline">f(a)</span> is the &quot;<strong>image</strong> of <span class="math inline">a</span> under <span class="math inline">f</span>&quot;. An <strong>image</strong> is a subset of the codomain corresponding to the function values for a subset of the domain.</p>
<p>Formally, <span class="math inline">I = \set{y \in C : \exists x \in D, f(x) = y}</span> where <span class="math inline">C</span> is the codomain, <span class="math inline">D</span> is a subset of the domain, and <span class="math inline">I</span> is the image,</p>
<p>A <strong>matrix mapping</strong> is a function <span class="math inline">f: \mb{R}^n \to \mb{R}^m</span> where <span class="math inline">f(\vec{x}) = A\vec{x}</span> for some matrix <span class="math inline">A \in M_{m, n}</span>.</p>
<p>In other words, it is a function over vectors that multiplies matrix by the given vector.</p>
<p>We can write this as <span class="math inline">f\left(\begin{bmatrix} x_1 \\ \vdots \\ x_n \end{bmatrix}\right) = \begin{bmatrix} y_1 \\ \vdots \\ y_m \end{bmatrix}</span>, or with the more aesthetically pleasing notation <span class="math inline">f(x_1, \ldots, x_n) = (y_1, \ldots, y_m)</span>.</p>
<p>What is the domain/codomain of <span class="math inline">f(\vec{x}) = A\vec{x}, A = \begin{bmatrix} 2 &amp; 3 \\ -4 &amp; 0 \\ 5 &amp; 1 \end{bmatrix}</span>.</p>
<blockquote>
<p>Clearly, <span class="math inline">f</span> is defined if and only if the matrix multiplication is defined, which is when <span class="math inline">\vec{x}</span> has 2 components.<br />
So <span class="math inline">\vec{x} \in \mb{R}^2</span>.<br />
Since <span class="math inline">A</span> has 3 rows, <span class="math inline">f(\vec{x})</span> also has 3 rows.<br />
So <span class="math inline">f(\vec{x}) \in \mb{R}^3</span>.</p>
</blockquote>
<p>We can also write the function in a simpler form, <span class="math inline">f(\vec{x}) = \begin{bmatrix} 2x_1 + 3x_2 \\ -4x_1 \\ 5x_1 + x_2 \end{bmatrix}</span>.</p>
<h1 id="section-6">10/2/14</h1>
<p>Note that if <span class="math inline">f(\vec{x}) = A\vec{x}</span> is a matrix mapping, then <span class="math inline">A = \begin{bmatrix} f(1, 0, \ldots, 0) &amp; f(0, 1, \ldots, 0) &amp; \ldots &amp; f(0, \ldots, 1, 0) &amp; f(0, \ldots, 0, 1) \end{bmatrix}</span>.</p>
<h3 id="theorem-3.3.1-linearity-of-matrix-mappings">Theorem 3.3.1 (Linearity of Matrix Mappings)</h3>
<p>Given <span class="math inline">A \in M_{m, n}, f(\vec{x}) = A\vec{x}</span>, <span class="math inline">\forall \vec{x}, \vec{y} \in \mb{R}^n, \forall s, t \in \mb{R}, f(s\vec{x} + t\vec{y}) = sf(\vec{x}) + tf(\vec{y})</span>.</p>
<p>In other words, when we multiply a matrix by a linear combination of vectors, it is equivalent to the linear combination of multiplying the matrix by the vectors directly.</p>
<p>This is the <strong>linearity property</strong>. What this theorem states is that all matrix mappings have the linearity property.</p>
<p>Proof:</p>
<blockquote>
<p>Let <span class="math inline">A \in M_{m, n}, f(\vec{x}) = A\vec{x}, \vec{x}, \vec{y} \in \mb{R}^n, s, t \in \mb{R}</span>.<br />
Then <span class="math inline">f(s\vec{x} + t\vec{y}) = A(s\vec{x} + t\vec{y}) = As\vec{x} + At\vec{y} = sA\vec{x} + tA\vec{y} = sf(\vec{x}) + tf(\vec{y})</span>.</p>
</blockquote>
<h2 id="linear-mappings">Linear Mappings</h2>
<p>A <strong>linear mapping/linear transformation/linear operator</strong> is a function <span class="math inline">L: \mb{R}^n \to \mb{R}^m</span> that has the linearity property.</p>
<p>In other words, given <span class="math inline">L: \mb{R}^n \to \mb{R}^m</span>, <span class="math inline">L</span> is a linear mapping if and only if <span class="math inline">L(s\vec{x} + t\vec{y}) = sL(\vec{x}) + tL(\vec{y}), \vec{x}, \vec{y} \in \mb{R}^n, s, t \in \mb{R}</span>.</p>
<p>To prove that a function is a lienar mapping, we simply need to prove that it has the inearity property.</p>
<p>Prove that <span class="math inline">\proj_{\vec{a}}</span> is a linear operator:</p>
<blockquote>
<p><span class="math display">\displaystyle 
\begin{aligned}
\proj_{\vec{a}} (s\vec{x} + t\vec{y}) &amp;= \left((s\vec{x} + t\vec{y}) \cdot \frac{\vec{a}}{\magn{a}}\right) \frac{\vec{a}}{\magn{a}} \\
&amp;= \left(s\vec{x} \cdot \frac{\vec{a}}{\magn{a}}\right) \frac{\vec{a}}{\magn{a}} + \left(t\vec{y} \cdot \frac{\vec{a}}{\magn{a}}\right) \frac{\vec{a}}{\magn{a}} \\
&amp;= s\proj_{\vec{a}} \vec{x} + t\proj_{\vec{a}} \vec{y} \\
\end{aligned}
</span> Therefore, <span class="math inline">\proj_{\vec{a}}</span> has the linearity property and is a linear mapping.</p>
</blockquote>
<h3 id="theorem-3.2.2">Theorem 3.2.2</h3>
<p>If <span class="math inline">L: \mb{R}^n \to \mb{R}^m</span> is a linear mapping, then <span class="math inline">L(\vec{x}) = A\vec{x}</span>, where <span class="math inline">A = \begin{bmatrix} L(\vec{e}_1) &amp; \ldots &amp; L(\vec{e}_n) \end{bmatrix}</span> and <span class="math inline">\vec{e}_i</span> is the <span class="math inline">i</span>th standard basis vector in <span class="math inline">\mb{R}^n</span>.</p>
<p>Proof:</p>
<blockquote>
<p>Let <span class="math inline">\vec{x} \in \mb{R}^n</span> and <span class="math inline">\vec{e}_i</span> be the <span class="math inline">i</span>th standard basis vector. Then <span class="math inline">\vec{x} = x_1 \vec{e}_1 + \ldots + x_n \vec{e}_n</span>.<br />
Then <span class="math inline">L(\vec{x}) = L(x_1 \vec{e}_1 + \ldots + x_n \vec{e}_n) = x_1 L(\vec{e}_1) + \ldots + x_n L(\vec{e}_n) = \begin{bmatrix} L(\vec{e}_1) &amp; \ldots &amp; L(\vec{e}_n) \end{bmatrix} \vec{x}</span>.<br />
Let <span class="math inline">A = \begin{bmatrix} L(\vec{e}_1) &amp; \ldots &amp; L(\vec{e}_n) \end{bmatrix}</span>.<br />
Then <span class="math inline">L(\vec{x}) = A\vec{x}</span>.</p>
</blockquote>
<p>The matrix <span class="math inline">A</span> in the theorem above is called the <strong>standard matrix</strong> of the linear mapping <span class="math inline">L</span>. It is the matrix such that <span class="math inline">L(\vec{x}) = A\vec{x}</span></p>
<p>We denote this as <span class="math inline">A = [L]</span>, where <span class="math inline">L</span> is a linear mapping. Additionally, <span class="math inline">L(\vec{x}) = [L]\vec{x}</span></p>
<p>Find the standard matrix of <span class="math inline">\proj_{\vec{a}}</span>:</p>
<blockquote>
<p><span class="math display">\displaystyle 
\begin{aligned}
\proj_{\vec{a}} \vec{x} &amp;= \left(\vec{x} \cdot \frac{\vec{a}}{\magn{a}}\right) \frac{\vec{a}}{\magn{a}} \\
\proj_{\vec{a}} \vec{e}_i &amp;= \frac{\vec{a}_i}{\magn{a}} \frac{\vec{a}}{\magn{a}} = \vec{a}_i \frac{\vec{a}}{\magn{a}^2} \\
f(\vec{x}) &amp;= A\vec{x} = \proj_{\vec{a}} \\
A &amp;= \begin{bmatrix} \vec{a}_1 \frac{\vec{a}}{\magn{a}^2} &amp; \ldots &amp; \vec{a}_n \frac{\vec{a}}{\magn{a}^2} \end{bmatrix} \\
\end{aligned}
</span></p>
</blockquote>
<h1 id="section-7">11/2/14</h1>
<h3 id="rotations">Rotations</h3>
<p><span class="math inline">R_\theta: \mb{R}^2 \to \mb{R}^2</span> is a linear mapping <span class="math inline">R_\theta(x_1, x_2)</span> that rotates a vector by an angle <span class="math inline">\theta</span> counterclockwise.</p>
<p>We can geometrically determine that the location of <span class="math inline">\vec{x}</span> after rotation is <span class="math inline">R_\theta(x_1, x_2) = \begin{bmatrix} x_1 \cos \theta - x_2 \sin \theta \\ x_1 \sin \theta + x_2 \cos \theta \end{bmatrix}</span>.</p>
<p>Now we can convert it into a matrix mapping: <span class="math inline">R_\theta(\vec{e}_1) = x_1 \begin{bmatrix} \cos \theta \\ \sin \theta \end{bmatrix}, R_\theta(\vec{e}_2) = x_2 \begin{bmatrix} -\sin \theta \\ \cos \theta \end{bmatrix}</span>.</p>
<p>So <span class="math inline">R_\theta(\vec{x}) = \begin{bmatrix} \cos \theta &amp; -\sin \theta \\ \sin \theta &amp; \cos \theta \end{bmatrix}\vec{x}</span>.</p>
<p><span class="math inline">\begin{bmatrix} \cos \theta &amp; -\sin \theta \\ \sin \theta &amp; \cos \theta \end{bmatrix}</span> is called a <strong>rotation matrix</strong>.</p>
<h3 id="reflections">Reflections</h3>
<p><span class="math inline">\refl_P: \mb{R}^n \to \mb{R}^n</span> is a linear mapping <span class="math inline">\refl_P(\vec{x})</span> that reflects <span class="math inline">\vec{x}</span> about the hyperplane <span class="math inline">P</span>.</p>
<p>We geometrically determine that the reflection of a vector about a hyperplane is the projection of the vector on the hyperplane minus the perpendicular.</p>
<p>In other words, <span class="math inline">\refl_P \vec{x} = \proj_P \vec{x} - \prp_P \vec{x} = (\vec{x} - \prp_P \vec{x}) - \prp_P \vec{x} = \vec{x} - 2\prp_P \vec{x} = \vec{x} - 2\proj_{\vec{n}} \vec{x}</span>, where <span class="math inline">\vec{n}</span> is the normal of the hyperplane.</p>
<p>Now we find the standard matrix of this operation. Let <span class="math inline">\vec{n}</span> be the normal of the plane. Then <span class="math inline">\refl_P \vec{e}_i = \vec{e}_i - 2\proj_{\vec{n}} \vec{e}_i</span>.</p>
<p>So <span class="math inline">\refl_P \vec{x} = \begin{bmatrix} \vec{e}_1 - 2\proj_{\vec{n}} \vec{e}_1 &amp; \ldots &amp; \vec{e}_n - 2\proj_{\vec{n}} \vec{e}_n \end{bmatrix} \vec{x}</span>.</p>
<h1 id="section-8">14/2/14</h1>
<h2 id="range">Range</h2>
<p>The <strong>range</strong> of a function <span class="math inline">f: \mb{R}^n \to \mb{R}^m</span> is the set of all images (function values) that <span class="math inline">f</span> produces for any <span class="math inline">\vec{x} \in \mb{R}^n</span>.</p>
<p>Range in mathematics can refer to this set of images, or to the codomain. The difference between the image and the codomain is that the image is always a subset of the codomain - the codomain also may include values that <span class="math inline">f(\vec{x})</span> cannot possibly produce. The codomain is something we define for the function, while the image is a property of the function itself.</p>
<p>For example, the image of <span class="math inline">f(\vec{x}) = \vec{0}</span> (<span class="math inline">f: \mb{R}^2 \to \mb{R}^3</span>) is <span class="math inline">\set{\vec{0}}</span>, while the codomain is <span class="math inline">\mb{R}^3</span>. The domain is <span class="math inline">\mb{R}^2</span>.</p>
<p>Formally, we denote range as <span class="math inline">\range f = \set{f(\vec{x}) \middle| \vec{x} \in \mb{R}^n}</span>. It is basically the set of all values the function can produce.</p>
<p>A value <span class="math inline">\vec{y}</span> is in the range (image) if <span class="math inline">\exists \vec{x} \in \mb{R}^n, f(\vec{x}) = \vec{y}</span>. We simply prove that <span class="math inline">\vec{y} \in \range f</span></p>
<h3 id="theorem-3.3.2">Theorem 3.3.2</h3>
<p>If <span class="math inline">L: \mb{R}^n \to \mb{R}^m</span> is linear, then <span class="math inline">\range L</span> is a subspace of the codomain, <span class="math inline">\mb{R}^m</span>.</p>
<p>Also, if <span class="math inline">L</span> is linear, then <span class="math inline">L(\vec{0}) = \vec{0}</span>.</p>
<p>Proof:</p>
<blockquote>
<p>Assume <span class="math inline">L: \mb{R}^n \to \mb{R}^m</span> is linear.<br />
By definition, <span class="math inline">\range L \subseteq \mb{R}^m</span>.<br />
Since <span class="math inline">L(\vec{0}) = \vec{0}</span>, <span class="math inline">\vec{0} \in \range L</span>.<br />
Let <span class="math inline">\vec{y}, \vec{z} \in \range L</span>.<br />
Then <span class="math inline">\exists \vec{w} \in \mb{R}^n, L(\vec{w}) = \vec{y}</span> and <span class="math inline">\exists \vec{x} \in \mb{R}^n, L(\vec{x}) = \vec{z}</span>.<br />
Clearly, <span class="math inline">\vec{y} + \vec{z} = L(\vec{w}) + L(\vec{x}) = L(\vec{w} + \vec{x})</span> and <span class="math inline">L(\vec{w} + \vec{x}) \in \range L</span>, so <span class="math inline">\vec{y} + \vec{z} \in \range L</span>.<br />
Clearly, <span class="math inline">c\vec{y} = cL(\vec{w}) = L(c\vec{w})</span> and <span class="math inline">L(c\vec{w}) \in \range L</span>, so <span class="math inline">c\vec{y} \in \range L</span>.<br />
So <span class="math inline">\range L</span> satisfies the subspace test, and is a subspace of <span class="math inline">\mb{R}^m</span>.</p>
</blockquote>
<p>Find a basis of <span class="math inline">\range L</span> where <span class="math inline">L(x_1, x_2, x_3) = (x_1 + x_2, 0, x_3)</span>:</p>
<blockquote>
<p>Clearly, <span class="math inline">L\left(\begin{bmatrix} x_1 \\ x_2 \\ x_3 \end{bmatrix}\right) = \begin{bmatrix} x_1 + x_2 \\ 0 \\ x_3 \end{bmatrix} = (x_1 + x_2)\begin{bmatrix} 1 \\ 0 \\ 0 \end{bmatrix} + x_3\begin{bmatrix} 0 \\ 0 \\ 1 \end{bmatrix}</span>.<br />
So <span class="math inline">\set{\begin{bmatrix} 1 \\ 0 \\ 0 \end{bmatrix}, \begin{bmatrix} 0 \\ 0 \\ 1 \end{bmatrix}}</span> spans <span class="math inline">\range L</span>.<br />
Clearly, this set is linearly independent. So it is a basis of <span class="math inline">\range L</span>.</p>
</blockquote>
<h2 id="kernel">Kernel</h2>
<p>The <strong>kernel</strong> of a linear mapping <span class="math inline">L: \mb{R}^n \to \mb{R}^m</span> is the set of all vectors in the domain <span class="math inline">\mb{R}^n</span> that have an image of <span class="math inline">\vec{0}</span> under <span class="math inline">L</span> (<span class="math inline">L(\vec{x}) = 0</span>).</p>
<p>Formally, <span class="math inline">\ker L = \set{\vec{x} \in \mb{R}^n \middle| L(\vec{x}) = \vec{0}}</span>. Basically, it is the set of vectors that make the function produce the zero vector.</p>
<p>A vector <span class="math inline">\vec{x}</span> is in the kernel of the function if and only if <span class="math inline">L(\vec{x}) = \vec{0}</span>.</p>
<h3 id="theorem-3.3.2-1">Theorem 3.3.2</h3>
<p>If <span class="math inline">L: \mb{R}^n \to \mb{R}^m</span> is linear, then <span class="math inline">\ker L</span> is a subspace of the domain, <span class="math inline">\mb{R}^n</span>.</p>
<p>Proof:</p>
<blockquote>
<p>Assume <span class="math inline">L: \mb{R}^n \to \mb{R}^m</span> is linear.<br />
By definition, <span class="math inline">\ker L \subseteq \mb{R}^n</span>.<br />
Since <span class="math inline">L(\vec{0}) = \vec{0}</span>, <span class="math inline">\vec{0} \in \range L</span>.<br />
Let <span class="math inline">\vec{y}, \vec{z} \in \ker L</span>.<br />
Clearly, <span class="math inline">L(\vec{y} + \vec{z}) = L(\vec{y}) + L(\vec{z}) = \vec{0} + \vec{0} = \vec{0}</span>, so <span class="math inline">\vec{y} + \vec{z} \in \ker L</span>.<br />
Clearly, <span class="math inline">L(c\vec{y}) = cL(\vec{y}) = c\vec{0} = \vec{0}</span> and <span class="math inline">\vec{y} \in \ker L</span>, so <span class="math inline">c\vec{y} \in \range L</span>.<br />
So <span class="math inline">\ker L</span> satisfies the subspace test, and is a subspace of <span class="math inline">\mb{R}^n</span>.</p>
</blockquote>
<p>Find a basis of the kernel of <span class="math inline">L(x_1, x_2, x_3) = (x_1 + x_2, 0, x_3)</span>:</p>
<blockquote>
<p>Let <span class="math inline">\vec{x} \in \ker L</span>.<br />
By definition, <span class="math inline">L(x_1, x_2, x_3) = (0, 0, 0)</span>.<br />
So <span class="math inline">\begin{cases} x_1 + x_2 &amp;= 0 \\ x_3 &amp;= 0 \end{cases}</span>, and <span class="math inline">x_2 = -x_1</span>, so <span class="math inline">\vec{x} = \begin{bmatrix} x_1 \\ -x_1 \\ 0 \end{bmatrix} = x_1\begin{bmatrix} 1 \\ -1 \\ 0 \end{bmatrix}</span>.<br />
Clearly, <span class="math inline">\set{\begin{bmatrix} 1 \\ -1 \\ 0 \end{bmatrix}}</span> spans <span class="math inline">\ker L</span> and is linearly independent, so it is a basis.</p>
</blockquote>
<h3 id="theorem-3.3.4">Theorem 3.3.4</h3>
<p>Given a linear mapping <span class="math inline">L: \mb{R}^n \to \mb{R}^m</span> with a standard matrix <span class="math inline">A = [L]</span>, <span class="math inline">\vec{x} \in \ker L \iff A\vec{x} = \vec{0}</span>.</p>
<p>The <strong>nullspace</strong> of a matrix <span class="math inline">A</span> is the set <span class="math inline">\operatorname{Null}(A) = \set{\vec{x} \in \mb{R}^n \middle| A\vec{x} = \vec{0}}</span>. In other words, the set of all vectors that make its corresponding linear mapping produce the zero vector.</p>
<p>Also, if <span class="math inline">L: \mb{R}^n \to \mb{R}^m</span> is linear, then <span class="math inline">\ker L = \operatorname{Null}([L])</span>.</p>
<h3 id="theorem-3.3.5">Theorem 3.3.5</h3>
<p>Given a linear mapping <span class="math inline">L: \mb{R}^n \to \mb{R}^m</span> with a standard matrix <span class="math inline">A = [L] = \begin{bmatrix} \vec{a}_1 &amp; \ldots &amp; \vec{a}_n \end{bmatrix}</span>, then <span class="math inline">\range L = \spn \set{\vec{a}_1, \ldots, \vec{a}_n}</span>.</p>
<p>Proof:</p>
<blockquote>
<p>Let <span class="math inline">\vec{y} \in \range L</span>. Then <span class="math inline">\exists \vec{x} \in \mb{R}^n, L(\vec{x}) = \vec{y}</span>.<br />
Clearly, <span class="math inline">\vec{y} = L(\vec{x}) = A\vec{x} = \begin{bmatrix} \vec{a}_1 &amp; \ldots &amp; \vec{a}_n \end{bmatrix}\vec{x} = x_1\vec{a}_1 + \ldots + x_n\vec{a}_n</span>.<br />
Since <span class="math inline">\vec{x}</span> is arbitrary, <span class="math inline">x_1\vec{a}_1 + \ldots + x_n\vec{a}_n \in \spn \set{\vec{a}_1, \ldots, \vec{a}_n}</span>.<br />
So <span class="math inline">\range L \subseteq \spn \set{\vec{a}_1, \ldots, \vec{a}_n}</span>.<br />
Clearly, <span class="math inline">\spn \set{\vec{a}_1, \ldots, \vec{a}_n} \subseteq \range L</span>, so <span class="math inline">\range L = \spn \set{\vec{a}_1, \ldots, \vec{a}_n}</span>.</p>
</blockquote>
<p>The <strong>column space</strong> of a matrix <span class="math inline">A = \begin{bmatrix} \vec{a}_1 &amp; \ldots &amp; \vec{a}_n \end{bmatrix}</span> is <span class="math inline">\operatorname{Col}(A) = \set{A\vec{x} \middle| \vec{x} \in \mb{R}^n} = \spn \set{\vec{a}_1, \ldots, \vec{a}_n}</span>. It is closely related to the range of the correponding linear mapping.</p>
<p>Also, if <span class="math inline">L: \mb{R}^n \to \mb{R}^m</span> is linear, then <span class="math inline">\range L = \operatorname{Col}([L])</span>.</p>
<p>The <strong>row space</strong> of a matrix <span class="math inline">A</span> is <span class="math inline">\operatorname{Row}(A) = \operatorname{Col}(A^T)</span>. It is simply the span of the rows of the matrix instead of the columns.</p>
<p>The <strong>left nullspace</strong> of a matrix <span class="math inline">A</span> is <span class="math inline">\operatorname{Null}(A^T)</span>. It is simply the nullspace of the matrix transposed.</p>
<h2 id="operations-on-linear-mappings">Operations on Linear Mappings</h2>
<p>Let <span class="math inline">L: \mb{R}^n \to \mb{R}^m</span> and <span class="math inline">M: \mb{R}^n \to \mb{R}^m</span> be linear mappings. Let <span class="math inline">t \in \mb{R}</span>.</p>
<p>Equality: <span class="math inline">L = M \iff \forall \vec{x} \in \mb{R}^n, L(\vec{x}) = M(\vec{x})</span>.</p>
<p>Addition/subtraction: we can define <span class="math inline">L \pm M: \mb{R}^n \to \mb{R}^m</span>, and <span class="math inline">\forall \vec{x} \in \mb{R}^n, (L \pm M)(\vec{x}) = L(\vec{x}) \pm M(\vec{x})</span>.</p>
<p>Scalar multiplication: we can define <span class="math inline">tL: \mb{R}^n \to \mb{R}^m</span>, and <span class="math inline">\forall \vec{x} \in \mb{R}^n, (tL)(\vec{x}) = tL(\vec{x})</span>.</p>
<p>The set <span class="math inline">\mb{L}</span> is the set of all possible linear mappings. The domain is <span class="math inline">\mb{R}^n</span> and the codomain is <span class="math inline">\mb{R}^m</span> for all linear mappings in this set.</p>
<h3 id="theorem-3.4.1">Theorem 3.4.1</h3>
<p><span class="math inline">[L \pm M] = [L] \pm [M]</span> and <span class="math inline">[tL] = t[L]</span>. In other words, addition and scalar multiplication over linear mappings are equivalent to the same operation applied to their standard matrices.</p>
<p>Proof:</p>
<blockquote>
<p>Let <span class="math inline">\vec{x} \in \mb{R}^n</span>.<br />
Clearly, <span class="math inline">(L + M)(\vec{x}) = L(\vec{m}) + M(\vec{x}) = ([L] + [M])\vec{x}</span>.<br />
So <span class="math inline">[L + M] = [L] + [M]</span>.<br />
Clearly, <span class="math inline">(tL)(\vec{x}) = tL(\vec{x}) = (t[L])\vec{x}</span>.<br />
So <span class="math inline">[tL] = t[L]</span>.</p>
</blockquote>
<h3 id="theorem-3.4.2">Theorem 3.4.2</h3>
<p>Let <span class="math inline">L, M, N \in \mb{L}</span> and <span class="math inline">c, d \in \mb{R}</span>:</p>
<ul>
<li>Closure under addition: <span class="math inline">L + M \in \mb{L}</span>.</li>
<li>Associativity: <span class="math inline">(L + M) + N = L + (M + N)</span>.</li>
<li>Commutativity: <span class="math inline">L + M = M + L</span>.</li>
<li>Additive identity: <span class="math inline">\exists O \in \mb{L}, L + O = L</span> (<span class="math inline">O(\vec{x}) = \vec{0} = O\vec{x}</span>).</li>
<li>Additive inverse: <span class="math inline">\exists (-L) \in \mb{L}, L + (-L) = O</span>.</li>
<li>Closure under scalar multiplication: <span class="math inline">cL \in \mb{L}</span>.</li>
<li><span class="math inline">c(dL) = (cd)L</span>.</li>
<li><span class="math inline">(c + d)L = cL + dL</span>.</li>
<li><span class="math inline">c(L + M) = cL + cM</span>.</li>
<li>Multiplicative identity: <span class="math inline">1L = L</span>.</li>
</ul>
<h3 id="composition">Composition</h3>
<p>Given <span class="math inline">L: \mb{R}^n \to \mb{R}^m</span> and <span class="math inline">M: \mb{R}^m \to \mb{R}^p</span>, <span class="math inline">M \circ L = M(L(\vec{x})) = [M][L]\vec{x}</span>.</p>
<p>Also, <span class="math inline">[M \circ L] = [M][L]</span>.</p>
<p>Find the standard matrix of <span class="math inline">R_\frac{\pi}{2} \circ R_\frac{3\pi}{2}</span>:</p>
<blockquote>
<p>Recall that <span class="math inline">R_\theta = \begin{bmatrix} \cos \theta &amp; -\sin \theta \\ \sin \theta &amp; \cos \theta \end{bmatrix}</span>.<br />
So <span class="math inline">R_\frac{\pi}{2} \circ R_\frac{3\pi}{2} = \begin{bmatrix} 0 &amp; -1 \\ 1 &amp; 0 \end{bmatrix} \begin{bmatrix} 0 &amp; 1 \\ -1 &amp; 0 \end{bmatrix}\vec{x} = \begin{bmatrix} 1 &amp; 0 \\ 0 &amp; 1 \end{bmatrix} \vec{x}</span>.<br />
So the standard matrix is <span class="math inline">\begin{bmatrix} 1 &amp; 0 \\ 0 &amp; 1 \end{bmatrix}</span></p>
</blockquote>
<p>Note that this is the identity matrix. This is because we rotated the vector by 90 degrees, then 270 degrees - back to where we started.</p>
<p>The <strong>identity mapping</strong> <span class="math inline">I \in \mb{L}</span> is the mapping such that <span class="math inline">I(\vec{x}) = I\vec{x}</span>. Note that the standard matrix of this linear mapping is the identity matrix.</p>
<p>A linear mapping <span class="math inline">L</span> is <strong>invertible</strong> if and only if there exists another linear mapping <span class="math inline">M</span> such that <span class="math inline">L \circ M = I</span> (<span class="math inline">L(M(\vec{x})) = \vec{x}</span>). If <span class="math inline">M</span> exists, then <span class="math inline">M = L^{-1}</span>.</p>
<p>The standard matrix of <span class="math inline">M</span> is such that <span class="math inline">[L][M] = I</span>. Note that in order for the inverse to exist, <span class="math inline">L</span> must therefore have the same number of rows as <span class="math inline">M</span> has columns.</p>
<p>In summary, the four <strong>fundamental subspaces</strong> of a matrix are:</p>
<ul>
<li>Row space</li>
<li>Column space</li>
<li>Nullspace</li>
<li>Left Nullspace</li>
</ul>
<h1 id="section-9">1/3/14</h1>
<h2 id="vector-spaces">Vector Spaces</h2>
<p>A <strong>real vector space</strong> is a set <span class="math inline">\mb{V}</span> of vectors put together with an addition operation, denoted <span class="math inline">\vec{x} \oplus \vec{y}</span>, and a scalar multiplication operation, denoted <span class="math inline">t \odot \vec{x}</span>, such that for any <span class="math inline">\vec{a}, \vec{b}, \vec{c} \in \mb{V}, u, v \in \mb{V}</span>:</p>
<ul>
<li>Closure under addition: <span class="math inline">\vec{a} \oplus \vec{b} \in \mb{V}</span></li>
<li>Associativity: <span class="math inline">(\vec{a} \oplus \vec{b}) \oplus \vec{c} = \vec{a} \oplus (\vec{b} \oplus \vec{c})</span></li>
<li>Commutativity: <span class="math inline">\vec{a} \oplus \vec{b} = \vec{b} \oplus \vec{a}</span></li>
<li>Additive identity: <span class="math inline">\exists \vec{0} \in \mb{V}, \vec{a} \oplus \vec{0} = \vec{a}</span>. This is known as the zero vector.</li>
<li>Additive inverse: <span class="math inline">\exists (-\vec{a}) \in \mb{V}, \vec{a} \oplus (-\vec{a}) = \vec{0}</span></li>
<li>Closure under scalar multiplication: <span class="math inline">u \odot \vec{a} \in \mb{V}</span></li>
<li><span class="math inline">u \odot (v \odot \vec{a}) = (uv) \odot \vec{a}</span></li>
<li>Scalar distributivity: <span class="math inline">(u + v) \odot \vec{a} = (u \odot \vec{a}) \oplus (v \odot \vec{a})</span></li>
<li>Vector distributivity: <span class="math inline">u \odot (\vec{a} \oplus \vec{b}) = u \odot \vec{a} \oplus u \odot \vec{b}</span></li>
<li>Scalar multiplicative identity: <span class="math inline">1 \odot \vec{a} = \vec{a}</span></li>
</ul>
<p>The &quot;vectors&quot; in a vector space don't necessarily have to be traditional vectors. They can also be things like polynomials or even sets.</p>
<p>Unlike with subspaces, we cannot assume that a few properties will imply the rest. So in order to prove that something is a vector space, we have to prove all ten properties are satisfied.</p>
<p>In this course, a <strong>vector space</strong> is the same thing as a real vector space. In practice, there are other types of vector spaces, like complex vector spaces.</p>
<p>We are using <span class="math inline">\oplus</span> and <span class="math inline">\odot</span> rather than <span class="math inline">+</span> and <span class="math inline">\cdot</span> because it makes it easier to see that the operations are not necessarily the same thing as real addition or multiplication - that they are non-standard.</p>
<p><span class="math inline">+</span> and <span class="math inline">\cdot</span> are called the <strong>standard addition</strong> and <strong>standard multiplication</strong>, respectively.</p>
<p>We can denote the zero vector of a vector space with <span class="math inline">\vec{0}_{\mb{V}}</span> to show which vector space it is in.</p>
<p>An example of a vector space is <span class="math inline">\mb{R}^n</span> with standard addition and standard multiplication. It is obvious that this satisfies all ten properties of the definition of a vector space.</p>
<p>Another example of a vector space is <span class="math inline">P_n(\mb{R})</span>, which is the set of all polynomials with degree less than or equal to <span class="math inline">n</span> with real coefficients. Here, the addition operation is standard addition of polynomials, and the scalar multiplication operation is the standard scalar multiplication of polynomials.</p>
<p>Another example of a vector space is <span class="math inline">M_{m \times n}</span>. Together with standard matrix multiplication and scalar multiplication of the matrices, this forms a vetor space. Likewise with linear mappings, with the standard linear mapping operations.</p>
<h3 id="theorem-4.1.1">Theorem 4.1.1</h3>
<p>If <span class="math inline">\mb{V}</span> is a vector space and <span class="math inline">\vec{v} \in \mb{V}</span>, then <span class="math inline">\vec{0} = 0 \odot \vec{v}</span> and <span class="math inline">-\vec{v} = (-1) \odot \vec{v}</span>.</p>
<p>In other words, scalar multiplication by 0 results in the additive identity and scalar multiplication by -1 results in the additive inverse.</p>
<p>Using this, we can always find the additive identity by picking any vector and multiplying by 0, and always find the additive inverse by multiplying by -1.</p>
<p>Proof:</p>
<blockquote>
<p>Clearly, <span class="math inline">0 \odot \vec{v} = 0 \odot \vec{v} \oplus \vec{0} = 0 \odot \vec{v} \oplus 1\vec{v} \oplus (-\vec{v}) = (0 + 1) \odot \vec{v} \oplus (-\vec{v}) = \vec{v} \oplus (-\vec{v}) = \vec{0}</span>.<br />
Clearly, <span class="math inline">(-1) \odot \vec{v} = \vec{0} \oplus (-1) \odot \vec{v} = 1\vec{v} + (-\vec{v}) \oplus (-1) \odot \vec{v} = 0\vec{v} + (-\vec{v}) = -\vec{1\vec{v} + (-\vec{v}) \oplus (-1) \odot \vec{v}}</span>.</p>
</blockquote>
<p>Prove that <span class="math inline">\vec{D} = \set{\vec{x} \in \mb{R} \middle| x &gt; 0}</span>, <span class="math inline">x, y \in \mb{D}</span>, where <span class="math inline">x \oplus y = xy</span> and <span class="math inline">t \odot x = x^t</span> is a vector space:</p>
<blockquote>
<p>Let <span class="math inline">x, y, z \in \mb{D}, c, d \in \mb{R}</span>.<br />
So <span class="math inline">xy &gt; 0</span>, and <span class="math inline">x \oplus y \in \mb{D}</span>.<br />
So <span class="math inline">(x \oplus y) \oplus z = (xy)z = x(yz) = x \oplus (y \oplus z)</span>.<br />
So <span class="math inline">x \oplus y = xy = yx = y \oplus x</span>.<br />
So <span class="math inline">\vec{0} = 0 \odot x = x^0 = 1</span>, by theorem 4.1.1.<br />
So <span class="math inline">-x = -1 \odot x = x^{-1} = \frac{1}{x}</span>, by theorem 4.1.1.<br />
So <span class="math inline">x^c &gt; 0</span>, and <span class="math inline">c \odot x \in \mb{D}</span>.<br />
So <span class="math inline">c \odot (d \cdot x) = x^{cd} = x^{dc} = d \odot (c \odot x)</span>.<br />
So <span class="math inline">(c + d) \odot x = x^{c + d} = x^c x^d = c \odot x \oplus d \odot x</span>.<br />
So <span class="math inline">c \odot (x \oplus y) = (xy)^c = x^c y^c = c \odot x \oplus c \odot y</span>.<br />
So <span class="math inline">1 \odot x = x^1 = x</span>.<br />
So all ten properties are satisfied and this is a vector space.</p>
</blockquote>
<p>For example, the empty set is not a vector space because all vector spaces must have at least one element, the zero vector.</p>
<h3 id="subspaces-1">Subspaces</h3>
<p>A set <span class="math inline">\mb{S}</span> is a <strong>subspace</strong> of <span class="math inline">\mb{V}</span> if and only if:</p>
<ul>
<li><span class="math inline">\mb{S}</span> is a subset of <span class="math inline">\mb{V}</span>.</li>
<li><span class="math inline">\mb{S}</span> is a vector space using the same operations as <span class="math inline">\mb{V}</span>.</li>
</ul>
<p>Since we know more about <span class="math inline">\mb{S}</span> and <span class="math inline">\mb{V}</span> - that they are vector spaces - we can again use the <strong>subspace test</strong> to see if some <span class="math inline">\mb{S}</span> is a subspace of some <span class="math inline">\mb{V}</span>.</p>
<h3 id="theorem-4.1.2-subspace-test-for-vector-spaces">Theorem 4.1.2 (Subspace Test for Vector Spaces)</h3>
<p>A set <span class="math inline">\mb{S}</span> is a subspace of a set <span class="math inline">\mb{V}</span> if and only if, for any <span class="math inline">\vec{x}, \vec{y} \in \mb{S}, t \in \mb{R}</span>:</p>
<ul>
<li><span class="math inline">\mb{S}</span> is not empty (this allows us to instantiate <span class="math inline">\vec{x}</span> and <span class="math inline">\vec{y}</span>). We can test this by checking if the zero vector is in the set.</li>
<li><span class="math inline">\mb{S}</span> is a subset of <span class="math inline">\mb{V}</span>.</li>
<li>Closure under addition: <span class="math inline">\vec{x} \oplus \vec{y} \in \mb{S}</span>.</li>
<li>Closure under scalar multiplication: <span class="math inline">t \odot \vec{x} \in \mb{S}</span>.</li>
</ul>
<p>The proof of this is roughly the same as the proof for the subspace test for <span class="math inline">\mb{R}^n</span>.</p>
<p>The rest of the properties follow from these, so if we show that these are true, the rest are automatically true.</p>
<h3 id="spanning">Spanning</h3>
<p>Let <span class="math inline">\mathcal{B} = \set{\vec{v}_1, \ldots, \vec{v}_k}</span> where <span class="math inline">\vec{v}_1, \ldots, \vec{v}_k \in \mb{V}</span> and <span class="math inline">\mb{V}</span> is a vector space.</p>
<p>The <strong>span</strong> of <span class="math inline">\mathcal{B}</span> is <span class="math inline">\spn \mathcal{B} = \set{c_1 \vec{v}_1 + \ldots + c_k \vec{v}_k \middle| c_1, \ldots, c_k \in \mb{R}}</span>.</p>
<p>This is just a generalization of the concept of a span in <span class="math inline">\mb{R}^n</span> into all vector spaces.</p>
<p>As with subspaces in <span class="math inline">\mb{R}^n</span>, we can prove a vector <span class="math inline">\vec{x}</span> is in a span if the linear system <span class="math inline">\vec{x} = c_1 \vec{v}_1 + \ldots + c_k \vec{v}_k</span> has at least one solution. Here, we are solving for <span class="math inline">c_1, \ldots, c_k</span>.</p>
<h3 id="linear-independence-1">Linear Independence</h3>
<p>If <span class="math inline">c_1 \odot \vec{v}_1 \oplus \ldots \oplus c_k \odot \vec{v}_k = \vec{0}_{\mb{V}}</span> has any solutions where <span class="math inline">c_i \ne 0</span> for <span class="math inline">1 \le i \le k</span>, then <span class="math inline">\mathcal{B}</span> is <strong>linearly dependent</strong>. Otherwise, it is <strong>linearly independent</strong>.</p>
<p>This is just a generalization of the concept of linear independence in <span class="math inline">\mb{R}^n</span> into all vector spaces.</p>
<p>Also, we can find which vectors are causing the set to be linearly dependent by following these steps:</p>
<ol type="1">
<li>Write the solution to <span class="math inline">c_1 \odot \vec{v}_1 \oplus \ldots \oplus c_k \odot \vec{v}_k = \vec{0}_{\mb{V}}</span> as a vector equation in the form of <span class="math inline">\begin{bmatrix} c_1 \\ \vdots \\ c_n \end{bmatrix} = a_1 \vec{v}_1 + \ldots + a_n \vec{v}_n, a_1, \ldots, a_n \in \mb{R}</span>.</li>
<li>For each <span class="math inline">\vec{v}_i, 1 \le i \le n</span>, let <span class="math inline">\vec{c} = \vec{v}_i</span> by setting <span class="math inline">a_i = 1</span> and all other coefficients to 0.</li>
<li>For each <span class="math inline">\vec{c} = \vec{v}_i</span>, write each <span class="math inline">c_i, 1 \le i \le n</span> as a linear equation. For example, <span class="math inline">c_4 = c_1 + c_2 + c_3</span>.</li>
<li>Find the vectors that can be written in terms of the other vectors.</li>
<li>These are the vectors that can be removed to make the set linearly independent.</li>
</ol>
<h3 id="theorem-4.1.3">Theorem 4.1.3</h3>
<p>Then <span class="math inline">\spn \mathcal{B}</span> is a subspace of <span class="math inline">\mb{V}</span>.</p>
<p>In other words, the span of a set of vectors that are in a vector space is always a subspace of that vector space.</p>
<h3 id="theorem-4.1.4">Theorem 4.1.4</h3>
<p>Then if for some <span class="math inline">1 \le i \le k</span>, <span class="math inline">v_i \in \spn \set{\vec{v}_1, \ldots, \vec{v}_{i - 1}, \vec{v}_{i + 1}, \ldots, \vec{v}_k}</span>, <span class="math inline">\spn \mathcal{B} = \spn \set{\vec{v}_1, \ldots, \vec{v}_{i - 1}, \vec{v}_{i + 1}, \ldots, \vec{v}_k}</span>.</p>
<p>In other words, if a vector in a set can be written as a linear combination of the other vectors in the set (it is in the span), then the span of the set is the same as the span of the set without that vector.</p>
<h3 id="theorem-4.1.5">Theorem 4.1.5</h3>
<p>Then <span class="math inline">\mathcal{B}</span> is linearly dependent if and only if for some <span class="math inline">1 \le i \le k</span>, <span class="math inline">v_i \in \spn \set{\vec{v}_1, \ldots, \vec{v}_{i - 1}, \vec{v}_{i + 1}, \ldots, \vec{v}_k}</span>.</p>
<h3 id="theorem-4.1.6">Theorem 4.1.6</h3>
<p>If <span class="math inline">\vec{0}_{\mb{V}} \in \mathcal{B}</span>, then <span class="math inline">\mathcal{B}</span> is linearly dependent.</p>
<h3 id="bases-1">Bases</h3>
<p>A set <span class="math inline">\mathcal{B}</span> is a <strong>basis</strong> for the vector space <span class="math inline">\mb{V}</span> if and only if it spans <span class="math inline">\mb{V}</span> and is linearly independent.</p>
<p>In other words, given <span class="math inline">\mathcal{B} = \set{\vec{v}_1, \ldots, \vec{v}_n}</span> and a vector space <span class="math inline">\mb{V}</span>, <span class="math inline">\mathcal{B}</span> is a basis for <span class="math inline">\mb{V}</span> if and only if <span class="math inline">\spn \mathcal{B} = \mb{V}</span> and <span class="math inline">\mb{B}</span> is linearly independent.</p>
<p>Also, the span of an empty set (an empty vector space) is <span class="math inline">\set{\vec{0}_\mb{V}}</span>.</p>
<p>The vectors in a set that is a basis are known as <strong>basis vectors</strong> and are often given by <span class="math inline">\vec{e}_i</span> where <span class="math inline">` \le i \le n</span>. The basis can then be represented as <span class="math inline">\mathcal{B} = \set{\vec{e}_1, \ldots, \vec{e}_n}</span>.</p>
<p>We can find bases given a vector space <span class="math inline">\mb{V}</span> by using the following steps:</p>
<ol type="1">
<li>Write <span class="math inline">\vec{v} \in \mb{V}</span> in its most general form.</li>
<li>Write <span class="math inline">\vec{v}</span> as a linear combination of vectors <span class="math inline">\vec{v}_1, \ldots, \vec{v}_n \in \mb{V}</span>.</li>
<li>Let <span class="math inline">\mathcal{B} = \set{\vec{v}_1, \ldots, \vec{v}_n}</span>.</li>
<li>Remove vectors from <span class="math inline">\mathcal{B}</span> until it is linearly independent.</li>
<li><span class="math inline">\mathcal{B}</span> is now a basis for <span class="math inline">\mb{V}</span>.</li>
</ol>
<p>Find a basis for the subspace <span class="math inline">\mb{P} = \set{a + bx + cx^2 \in P_2(\mb{R}) \middle| a + c = b}</span>:</p>
<blockquote>
<p>Let <span class="math inline">f(x) \in \mb{P}</span>. Clearly, <span class="math inline">f(x) = a + bx + cx^2</span> where <span class="math inline">a + c = b</span>.<br />
Then <span class="math inline">f(x) = a + (a + c)x + cx^2 = a(1 + x) + c(x + x^2)</span>.<br />
So <span class="math inline">f(x)</span> is a linear combination of <span class="math inline">1 + x \in \mb{P}</span> and <span class="math inline">x + x^2 \in \mb{P}</span>, so <span class="math inline">\spn \set{1 + x, x + x^2} = \mb{P}</span>.<br />
Clearly, <span class="math inline">\set{1 + x, x + x^2}</span> is lienarly independent, so it is a basis for <span class="math inline">\mb{P}</span>.</p>
</blockquote>
<h3 id="unique-representation-theorem">Unique Representation Theorem</h3>
<p>Let <span class="math inline">\mathcal{B} = \set{\vec{v}_1, \ldots, \vec{v}_k}</span> be a basis for vector space <span class="math inline">\mb{V}</span>.</p>
<p>Then for every <span class="math inline">\vec{v} \in \mb{V}</span> can be written as a unique linear combination of <span class="math inline">\vec{v}_1, \ldots, \vec{v}_k</span> (there is only one of these linear combinations).</p>
<blockquote>
<p>Since <span class="math inline">\mathcal{B}</span> spans <span class="math inline">\mb{V}</span>, by definition <span class="math inline">\exists c_1, \ldots, c_n \in \mb{R}, c_1 \vec{v}_1 + \ldots + c_n \vec{v}_n = \vec{v}</span>.<br />
Let <span class="math inline">\vec{v} = d_1 \vec{v}_1 + \ldots + d_n \vec{v}_n</span> for some <span class="math inline">d_1, \ldots, d_n \in \mb{R}</span>.<br />
Clearly, <span class="math inline">d_1 \vec{v}_1 + \ldots + d_n \vec{v}_n = c_1 \vec{v}_1 + \ldots + c_n \vec{v}_n</span>, so <span class="math inline">(c_1 - d_1) \vec{v}_1 + \ldots + (c_n - d_n) \vec{v}_n = \vec{0}_\mb{V}</span>.<br />
Since <span class="math inline">\mathcal{B}</span> is linearly independent, the only solution is <span class="math inline">c_1 - d_1 = \ldots = c_n - d_n = 0</span>, the trivial solution.<br />
So <span class="math inline">c_i = d_i</span> for all <span class="math inline">1 \le i \le n</span> and the linear combination is unique.</p>
</blockquote>
<p>For example, consider the polynomials vector space, <span class="math inline">P_n(\mb{R})</span>:</p>
<blockquote>
<p>Let <span class="math inline">\mathcal{B} = \set{1, x, x^2, \ldots, x^n}</span>.<br />
Clearly, <span class="math inline">\spn \mathcal{B} = P_n(\mb{R})</span>.<br />
Clearly, <span class="math inline">\mathcal{B}</span> is linearly independent because <span class="math inline">c_1 1 + \ldots c_n x^n = 0 + \ldots + 0x^n \implies c_1 = \ldots = c_n = 0</span>.<br />
In fact, we define the standard basis of <span class="math inline">P_n(\mb{R})</span> to be <span class="math inline">\set{1, x, x^2, \ldots, x^n}</span>.</p>
</blockquote>
<p>In other words, every vector in a vector space has a unique coordinate.</p>
<h2 id="dimension">Dimension</h2>
<h3 id="theorem-4.2.1">Theorem 4.2.1</h3>
<p>Let <span class="math inline">\mathcal{B} = \set{\vec{v}_1, \ldots, \vec{v}_n}</span> be a basis for vector space <span class="math inline">\mb{V}</span>.</p>
<p>If <span class="math inline">\set{\vec{w}_1, \ldots, \vec{w}_k}</span> is linearly independent, then <span class="math inline">k \le n</span>.</p>
<p>Proof:</p>
<blockquote>
<p>Suppose <span class="math inline">k &gt; n</span>.<br />
Clearly, <span class="math inline">\forall 1 \le i \le k, \vec{w}_i = a_{i, 1} \vec{v}_1 + \ldots + a_{i, n} \vec{v}_n</span>.<br />
So <span class="math inline">c_1 \vec{w}_1 + \ldots + c_k \vec{w}_k = \vec{0}_\mb{V} = c_1 (a_{1, 1} \vec{v}_1 + \ldots + a_{1, n}) + \ldots + c_k (a_{k, 1} \vec{v}_1 + \ldots + a_{k, n}) = (c_1 a_{1, 1} + \ldots + c_k a_{k, 1}) \vec{v}_1 + \ldots + (c_1 a_{1, n} + \ldots + c_k a_{k, n}) \vec{v}_1</span>.<br />
Since <span class="math inline">\mathcal{B}</span> is linearly independent, the only solution for the coefficients of <span class="math inline">\vec{v}_i</span> is the trivial solution, which is <span class="math inline">\begin{cases} c_1 a_{1, 1} + \ldots + c_k a_{k, 1} &amp;= 0 \\ \vdots \\ c_1 a_{1, n} + \ldots + c_k a_{k, n} &amp;= 0 \end{cases}</span>.<br />
This solution is itself a system of linear equations that we can represent with the <span class="math inline">n \times k</span> coefficient matrix <span class="math inline">A</span>.<br />
Clearly, <span class="math inline">\rank A \le n</span> since <span class="math inline">A</span> has <span class="math inline">k</span> columns, so there are infinite solutions.<br />
So <span class="math inline">c_1 \vec{w}_1 + \ldots + c_k \vec{w}_k = \vec{0}_\mb{V}</span> has infinite solutions.<br />
This is a contradiction because the set is linearly independent. Therefore, <span class="math inline">k &lt; n</span>.</p>
</blockquote>
<h3 id="theorem-4.2.2">Theorem 4.2.2</h3>
<p>If <span class="math inline">\set{\vec{v}_1, \ldots, \vec{v}_n}</span> and <span class="math inline">\set{\vec{w}_1, \ldots, \vec{w}_k}</span> are both bases in <span class="math inline">\mb{V}</span>, then <span class="math inline">k = n</span>.</p>
<p>This is easily proved using theorem 4.2.1.</p>
<h3 id="definition">Definition</h3>
<p>The <strong>dimension</strong> of a vector space <span class="math inline">\mb{V}</span> is the number of elements in a basis <span class="math inline">\mathcal{B}</span> of <span class="math inline">\mb{V}</span>.</p>
<p>In other words, if <span class="math inline">\mathcal{B} = \set{\vec{v}_1, \ldots, \vec{v}_n}</span> is a basis for <span class="math inline">\mb{V}</span>, then the dimension of <span class="math inline">\mb{V}</span> is <span class="math inline">\dim \mb{V} = n</span> and <span class="math inline">\mb{V}</span> is an <span class="math inline">n</span>-dimensional vector space.</p>
<p>Since <span class="math inline">\set{\vec{0}_\mb{V}}</span> has only the basis <span class="math inline">\emptyset</span>, the dimension is <span class="math inline">\dim \set{\vec{0}_\mb{V}} = 0</span>.</p>
<p>If the basis of a vector space has infinite elements, then the vector space is <strong>infinite-dimensional</strong>.</p>
<p>For example:</p>
<ul>
<li><span class="math inline">\dim \mb{R}^n = n</span> since the standard basis is <span class="math inline">\set{\begin{bmatrix} 1 \\ 0 \\ \vdots \\ 0 \end{bmatrix}, \ldots, \begin{bmatrix} 0 \\ \vdots \\ 0 \\ 1 \end{bmatrix}}</span>.</li>
<li><span class="math inline">\dim M_{m \times n}(\mb{R}) = mn</span> since the standard basis is a set of <span class="math inline">mn</span> matrices with a single 1 in unique locations.</li>
<li><span class="math inline">\dim P_n(\mb{R}) = n + 1</span> since the standard basis is <span class="math inline">\set{1, x, x^2, \ldots, x^n}</span>.</li>
</ul>
<p>To find the dimension of a vector space, we simply need to find a basis.</p>
<p>For example, find <span class="math inline">\dim \mb{W}</span> where <span class="math inline">\mb{W} = \set{p(x) \in P_3(\mb{R}) \middle| p(3) = 0}</span>:</p>
<blockquote>
<p>Let <span class="math inline">p(x) \in \mb{W}</span>. Then <span class="math inline">p(3) = 0</span>.<br />
So <span class="math inline">\exists a, b, c \in \mb{R}, p(x) = (x - 3)(a + bx + cx^2) = a(x - 3) + bx(x - 3) + cx^2(x - 3) = a(x - 3) + b(x^2 - 3x) + c(x^3 - 3x^2)</span>.<br />
So <span class="math inline">\set{x - 3, x^2 - 3x, x^3 - 3x^2}</span> spans <span class="math inline">\mb{W}</span> and since it is linearly independent, it is a basis for <span class="math inline">\mb{W}</span>.<br />
So <span class="math inline">\dim \mb{W} = 3</span>, since there are 3 elements in the basis.</p>
</blockquote>
<h3 id="theorem-4.2.3">Theorem 4.2.3</h3>
<p>Given a vector space <span class="math inline">\mb{V}</span>:</p>
<ul>
<li>A set with more than <span class="math inline">\dim \mb{V}</span> elements is linearly dependent.</li>
<li>A set with less than <span class="math inline">\dim \mb{V}</span> elements cannot span <span class="math inline">\mb{V}</span>.</li>
<li>Given a set <span class="math inline">\mathcal{B}</span> with <span class="math inline">\dim \vec{V}</span> elements, <span class="math inline">\mathcal{B}</span> spans <span class="math inline">\mb{V}</span> if and only if <span class="math inline">\mathcal{B}</span> is linearly independent.</li>
</ul>
<p>;wip: prove this</p>
<p>Let <span class="math inline">A_1 = \begin{bmatrix} 1 &amp; 1 \\ 0 &amp; 0 \end{bmatrix}, A_2 = \begin{bmatrix} 1 &amp; 1 \\ 1 &amp; -1 \end{bmatrix}</span>. Extend <span class="math inline">\set{A_1, A_2}</span> to be a basis for <span class="math inline">M_{2 \times 2}(\mb{R})</span>:</p>
<blockquote>
<p>Clearly, <span class="math inline">\set{E_1, E_2, E_3, E_4}</span> spans <span class="math inline">M_{2 \times 2}(\mb{R})</span> where <span class="math inline">E_i</span> is the <span class="math inline">i</span>th standard basis matrix (the <span class="math inline">i</span>th entry in the matrix is 1 and all the others are 0).<br />
So <span class="math inline">\set{A_1, A_2, E_1, \ldots, E_4}</span> spans <span class="math inline">M_{2 \times 2}(\mb{R})</span>. Let <span class="math inline">A \in M_{2 \times 2}(\mb{R})</span>.<br />
Then <span class="math inline">\exists c_1, c_2, c_3, c_4, c_5, c_6 \in \mb{R}, A = c_1 \begin{bmatrix} 1 &amp; 1 \\ 0 &amp; 0 \end{bmatrix} + c_2 \begin{bmatrix} 1 &amp; 1 \\ 1 &amp; -1 \end{bmatrix} + c_3 \begin{bmatrix} 1 &amp; 0 \\ 0 &amp; 0 \end{bmatrix} + c_4 \begin{bmatrix} 0 &amp; 1 \\ 0 &amp; 0 \end{bmatrix} + c_5 \begin{bmatrix} 0 &amp; 0 \\ 1 &amp; 0 \end{bmatrix} + c_6 \begin{bmatrix} 0 &amp; 0 \\ 0 &amp; 1 \end{bmatrix}</span>.<br />
The augmented matrix for this system is <span class="math inline">\left[\begin{array}{cccccc|c} 1 &amp; 1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\ 1 &amp; 1 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 \\ 0 &amp; -1 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 \end{array}\right]</span>, and the RREF is <span class="math inline">\left[\begin{array}{cccccc|c} 1 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 1 &amp; 0 \\ 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; -1 &amp; 0 \\ 0 &amp; 0 &amp; 1 &amp; -1 &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 1 &amp; 0 \end{array}\right]</span>.<br />
So <span class="math inline">\begin{bmatrix} c_1 \\ c_2 \\ c_3 \\ c_4 \\ c_5 \\ c_6 \end{bmatrix} = s\begin{bmatrix} -1 \\ 0 \\ 1 \\ 1 \\ 0 \\ 0 \end{bmatrix} + t\begin{bmatrix} -1 \\ 1 \\ 0 \\ 0 \\ -1 \\ 1 \end{bmatrix}, s, t \in \mb{R}</span>.<br />
So <span class="math inline">\begin{bmatrix} c_1 \\ c_2 \\ c_3 \\ c_4 \\ c_5 \\ c_6 \end{bmatrix} = \begin{bmatrix} -1 \\ 0 \\ 1 \\ 1 \\ 0 \\ 0 \end{bmatrix}</span> and <span class="math inline">\begin{bmatrix} c_1 \\ c_2 \\ c_3 \\ c_4 \\ c_5 \\ c_6 \end{bmatrix} = \begin{bmatrix} -1 \\ 1 \\ 0 \\ 0 \\ -1 \\ 1 \end{bmatrix}</span> are solutions.<br />
From this we determine that <span class="math inline">E_2 = A_1 - E_1</span> and <span class="math inline">E_4 = A_1 - A_2 + E_3</span>.<br />
So <span class="math inline">\spn \set{A_1, A_2, E_1, E_2, E_3, E_4} = \spn \set{A_1, A_2, E_1, E_3}</span>.<br />
Clearly, <span class="math inline">\set{A_1, A_2, E_1, E_3}</span> is linearly independent and spans <span class="math inline">M_{2 \times 2}(\mb{R})</span>, so it is a basis.</p>
</blockquote>
<h3 id="theorem-4.2.4">Theorem 4.2.4</h3>
<p>Given <span class="math inline">n</span>-dimensional vector space <span class="math inline">\mb{V}</span> and linearly independent set <span class="math inline">\set{\vec{v}_1, \ldots, \vec{v}_k}</span> where <span class="math inline">k \le n</span>, then there exist <span class="math inline">n - k</span> vectors <span class="math inline">\vec{w}_{k + 1}, \ldots, \vec{w}_n</span> such that <span class="math inline">\set{\vec{v}_1, \ldots, \vec{v}_k, \vec{w}_{k + 1}, \ldots, \vec{w}_n}</span> is a basis for <span class="math inline">\mb{V}</span>.</p>
<p>Also, all linearly independent sets of vectors in <span class="math inline">\mb{V}</span> with <span class="math inline">n</span> elements are bases for <span class="math inline">\mb{V}</span>.</p>
<p>Also, if <span class="math inline">\mb{S}</span> is a subspace of <span class="math inline">\mb{V}</span>, then <span class="math inline">\dim \mb{S} \le \dim \mb{V}</span>.</p>
<h2 id="coordinates">Coordinates</h2>
<p>Let <span class="math inline">\mb{V}</span> be an <span class="math inline">n</span>-dimensional vector space. Let <span class="math inline">\mathcal{B} = \set{\vec{e}_1, \ldots, \vec{e}_n}</span> be a basis for <span class="math inline">\mb{V}</span>. Let <span class="math inline">\vec{x} \in \mb{V}</span>.</p>
<p>Then <span class="math inline">\exists c_1, \ldots, c_n \in \mb{R}, \vec{x} = c_1 \vec{e}_1 + \ldots + c_n \vec{e}_n</span>.</p>
<p>The <strong>coordinate vector</strong> of <span class="math inline">\vec{x}</span> with respect to <span class="math inline">\mathcal{B}</span> is defined as <span class="math inline">\coord{\vec x}{\mathcal{B}} = \begin{bmatrix} c_1 \\ \vdots \\ c_n \end{bmatrix}</span>. The <strong>coordinates</strong> are simply <span class="math inline">c_1, \ldots, c_n</span> - the values of the variables themselves.</p>
<p>A <span class="math inline">\mathcal{B}</span>-coordinate is a coordinate with respect to <span class="math inline">\mathcal{B}</span>.</p>
<p>Note that <span class="math inline">\vec{x}</span> is not the same thing as <span class="math inline">\coord{\vec x}{\mathcal{B}}</span> - <span class="math inline">\vec{x}</span> can be whatever type of thing exists in the vector space, like polynomials or matrices, but <span class="math inline">\coord{\vec x}{\mathcal{B}}</span> is always a vector in <span class="math inline">\mb{R}^n</span>.</p>
<p>Note that all coordinates are relative to a vector, so it is important to know which basis a coordinate vector is written with respect to. The order of the basis vectors matter too, so we must give the vectors in the basis a particular fixed order.</p>
<h3 id="theorem-4.3.2">Theorem 4.3.2</h3>
<p>For all <span class="math inline">\vec{v}, \vec{w} \in \mb{V}, s, t \in \mb{R}</span>, <span class="math inline">[s\vec{v} + t\vec{w}]_\mathcal{B} = s\coord{\vec v}{\mathcal{B}} + t\coord{\vec w}{\mathcal{B}}</span>.</p>
<p>Proof:</p>
<blockquote>
<p>Clearly, <span class="math inline">\exists b_1, \ldots, b_n \in \mb{R}, \vec{v} = b_1 \vec{e}_1 + \ldots + b_n \vec{e}_n</span>.<br />
Clearly, <span class="math inline">\exists c_1, \ldots, c_n \in \mb{R}, \vec{w} = c_1 \vec{e}_1 + \ldots + c_n \vec{e}_n</span>.<br />
So <span class="math inline">s\vec{v} + t\vec{w} = (sb_1 + tc_1)\vec{e}_1 + \ldots + (sb_n + tc_n)\vec{e}_n</span>.<br />
So <span class="math inline">[s\vec{v} + t\vec{w}]_\mathcal{B} = \begin{bmatrix} sb_1 + tc_1 \\ \vdots \\ sb_n + tc_n \end{bmatrix}_\mathcal{B} = \begin{bmatrix} sb_1 + tc_1 \\ \vdots \\ sb_n + tc_n \end{bmatrix}_\mathcal{B} = s\begin{bmatrix} b_1 \\ \vdots \\ b_n \end{bmatrix}_\mathcal{B} + t\begin{bmatrix} c_1 \\ \vdots \\ c_n \end{bmatrix}_\mathcal{B} = s\coord{\vec v}{\mathcal{B}} + t\coord{\vec w}{\mathcal{B}}</span>.</p>
</blockquote>
<h3 id="change-of-coordinates">Change of Coordinates</h3>
<p>Sometimes, we want to change coordinates from one basis to another.</p>
<p>Let <span class="math inline">\mb{V}</span> be an <span class="math inline">n</span>-dimensional vector space. Let <span class="math inline">\vec{x} \in \mb{V}</span>.</p>
<p>Let <span class="math inline">\mathcal{B} = \set{\vec{v}_1, \ldots, \vec{v}_n}</span> and <span class="math inline">\mathcal{C}</span> be bases for <span class="math inline">\mb{V}</span>.</p>
<p>Given a coordinate with respect to <span class="math inline">\mathcal{B}</span>, we want to find a coordinate with respect to <span class="math inline">\mathcal{C}</span> that represents the same vector. In other words, given <span class="math inline">\coord{\vec x}{\mathcal{B}}</span>, we want to quickly find <span class="math inline">\coord{\vec x}{\mathcal{C}}</span>.</p>
<p>Clearly, <span class="math inline">\exists b_1, \ldots, b_n \in \mb{R}, \vec{x} = b_1 \vec{v}_1 + \ldots + b_n \vec{v}_n</span>. So <span class="math inline">\coord{\vec x}{\mathcal{B}} = \begin{bmatrix} b_1 \\ \vdots \\ b_n \end{bmatrix}</span>.</p>
<p>Clearly, <span class="math inline">\coord{\vec x}{\mathcal{C}} = \coord{b_1 \vec{v}_1 + \ldots + b_n \vec{v}_n}{\mathcal{C}} = b_1 \coord{\vec{v}_1}{\mathcal{C}} + \ldots + b_n \coord{\vec{v}_n}{\mathcal{C}}</span>.</p>
<p>We can write this using vectors or matrices: <span class="math inline">\coord{\vec x}{\mathcal{C}} = \begin{bmatrix} \coord{\vec{v}_1}{\mathcal{C}} \\ \vdots \\ \coord{\vec{v}_n}{\mathcal{C}} \end{bmatrix} \cdot \coord{\vec x}{\mathcal{B}} = \begin{bmatrix} \coord{\vec{v}_1}{\mathcal{C}} &amp; \ldots &amp; \coord{\vec{v}_n}{\mathcal{C}} \end{bmatrix} \coord{\vec x}{\mathcal{B}}</span>.</p>
<p>The matrix <span class="math inline">{}_\mathcal{C}P_\mathcal{B} = \begin{bmatrix} \coord{\vec{v}_1}{\mathcal{C}} &amp; \ldots &amp; \coord{\vec{v}_n}{\mathcal{C}} \end{bmatrix}</span> is called the <strong>change of coordinates matrix/change of basis matrix</strong> from <span class="math inline">\mathcal{B}</span> to <span class="math inline">\mathcal{C}</span>.</p>
<p>This is the matrix that satisfies <span class="math inline">\coord{\vec x}{\mathcal{C}} = {}_\mathcal{C}P_\mathcal{B} \coord{\vec x}{\mathcal{B}}</span>.</p>
<h3 id="theorem-4.3.3">Theorem 4.3.3</h3>
<p><span class="math inline">{}_\mathcal{C}P_\mathcal{B} {}_\mathcal{B}P_\mathcal{C} = I = {}_\mathcal{B}P_\mathcal{C} {}_\mathcal{C}P_\mathcal{B}</span>.</p>
<p>Proof:</p>
<blockquote>
<p>Let <span class="math inline">A = {}_\mathcal{B}P_\mathcal{C} {}_\mathcal{C}P_\mathcal{B}</span>.<br />
Then <span class="math inline">A\coord{\vec x}{\mathcal{B}} = {}_\mathcal{B}P_\mathcal{C} {}_\mathcal{C}P_\mathcal{B} \coord{\vec x}{\mathcal{B}} = {}_\mathcal{B}P_\mathcal{C} \coord{\vec x}{\mathcal{C}} = \coord{\vec x}{\mathcal{B}}</span>.<br />
So <span class="math inline">A\coord{\vec x}{\mathcal{B}} = \coord{\vec x}{\mathcal{B}}</span>, so <span class="math inline">A = {}_\mathcal{B}P_\mathcal{C} {}_\mathcal{C}P_\mathcal{B} = I</span>.<br />
The same technique can be used for for <span class="math inline">{}_\mathcal{C}P_\mathcal{B} {}_\mathcal{B}P_\mathcal{C} = I</span>.</p>
</blockquote>
<p>Given <span class="math inline">\mathcal{B} = \set{1, x, x^2}, \mathcal{C} = \set{1, x + 1, (x + 1)^2}</span>, find <span class="math inline">{}_\mathcal{C}P_\mathcal{B}</span>:</p>
<blockquote>
<p>Clearly, <span class="math inline">{}_\mathcal{C}P_\mathcal{B} = \begin{bmatrix} \coord{1}{\mathcal{C}} &amp; \coord{x}{\mathcal{C}} &amp; \coord{x^2}{\mathcal{C}} \end{bmatrix}</span>.<br />
Clearly, <span class="math inline">\coord{1}{\mathcal{C}} = \begin{bmatrix} 1 \\ 0 \\ 0 \end{bmatrix}, \coord{x}{\mathcal{C}} = \begin{bmatrix} -1 \\ 1 \\ 0 \end{bmatrix}, \coord{x^2}{\mathcal{C}} = \begin{bmatrix} 1 \\ -2 \\ 1 \end{bmatrix}</span>, by inspection.<br />
So <span class="math inline">{}_\mathcal{C}P_\mathcal{B} = \begin{bmatrix} 1 &amp; -1 &amp; 1 \\ 0 &amp; 1 &amp; -2 \\ 0 &amp; 0 &amp; 1 \end{bmatrix}</span>.</p>
</blockquote>
<h1 id="section-10">17/3/14</h1>
<h2 id="inverses-of-matrices">Inverses of Matrices</h2>
<p>Let <span class="math inline">A \in M_{m \times n}, B, C \in M_{n \times m}</span>.</p>
<p>Then <span class="math inline">B</span> is the <strong>right inverse</strong> of <span class="math inline">A</span> if and only if <span class="math inline">AB = I_m</span>.</p>
<p>Then <span class="math inline">C</span> is the <strong>left inverse</strong> of <span class="math inline">A</span> if and only if <span class="math inline">CA = I_n</span></p>
<p>A matrix <span class="math inline">A</span> is <strong>square</strong> if and only if <span class="math inline">A \in M_{n \times n}</span>.</p>
<h3 id="finding-inverses">Finding Inverses</h3>
<p>Suppose we wanted to find the right inverse <span class="math inline">B</span> of <span class="math inline">A</span>.</p>
<p>Clearly, <span class="math inline">B = \begin{bmatrix} \vec{b}_1 &amp; \ldots &amp; \vec{b}_n \end{bmatrix}</span>. So <span class="math inline">AB = I = A\begin{bmatrix} \vec{b}_1 &amp; \ldots &amp; \vec{b}_n \end{bmatrix} = \begin{bmatrix} A\vec{b}_1 &amp; \ldots &amp; A\vec{b}_n \end{bmatrix}</span>.</p>
<p>So <span class="math inline">A\vec{b}_i = \vec{e}_i</span>. We can then find the inverse by solving each system.</p>
<p>There is also a faster way. We can simply put the system <span class="math inline">\sys{A}{I}</span> into RREF to get <span class="math inline">\sys{I}{A^{-1}}</span> and read off the inverse directly from the resulting matrix. If the left side is not <span class="math inline">I</span>, then the inverse does not exist.</p>
<p>In fact, this is a useful method for solving multiple systems of linear equations with the same coefficients but different right hand sides at the same time.</p>
<p>For example, find the right inverse of <span class="math inline">A = \begin{bmatrix} 1 &amp; 2 \\ 3 &amp; 4 \end{bmatrix}</span>:</p>
<blockquote>
<p>Clearly, <span class="math inline">\left[\begin{array}{cc|cc} 1 &amp; 2 &amp; 1 &amp; 0 \\ 3 &amp; 4 &amp; 0 &amp; 1 \end{array}\right]</span> in RREF is <span class="math inline">\left[\begin{array}{cc|cc} 1 &amp; 0 &amp; -2 &amp; 1 \\ 0 &amp; 1 &amp; \frac{3}{2} &amp; -\frac{1}{2} \end{array}\right]</span>.<br />
So the inverse is <span class="math inline">\begin{bmatrix} -2 &amp; 1 \\ \frac{3}{2} &amp; -\frac{1}{2} \end{bmatrix}</span>. To verify, <span class="math inline">\begin{bmatrix} 1 &amp; 2 \\ 3 &amp; 4 \end{bmatrix} \begin{bmatrix} -2 1 \\ \frac{3}{2} &amp; -\frac{1}{2} \end{bmatrix} = \begin{bmatrix} 1 &amp; 0 \\ 0 &amp; 1 \end{bmatrix}</span>, as required.</p>
</blockquote>
<h3 id="theorem-5.1.1">Theorem 5.1.1</h3>
<p>If <span class="math inline">A \in M_{m \times n}</span> where <span class="math inline">m &gt; n</span>, then <span class="math inline">A</span> cannot have a right inverse.</p>
<p>If <span class="math inline">A \in M_{m \times n}</span> where <span class="math inline">m &lt; n</span>, then <span class="math inline">A</span> cannot have a left inverse.</p>
<p>Proof:</p>
<blockquote>
<p>Let <span class="math inline">A \in M_{m \times n}</span> where <span class="math inline">m &gt; n</span>.<br />
Suppose <span class="math inline">B = \begin{bmatrix} \vec{b}_1 &amp; \ldots &amp; \vec{b}_m \end{bmatrix}</span> is a right inverse for <span class="math inline">A</span>.<br />
Then <span class="math inline">I_m = \begin{bmatrix} \vec{e}_1 &amp; \ldots &amp; \vec{e}_m \end{bmatrix} = AB = \begin{bmatrix} A\vec{b}_1 &amp; \ldots &amp; A\vec{b}_m \end{bmatrix}</span>.<br />
Then <span class="math inline">\forall 1 \le i \le m, A\vec{b}_i = \vec{e}_i</span>.<br />
Let <span class="math inline">\vec{y} \in \mb{R}^m</span>. Then <span class="math inline">\vec{y} = y_1\vec{e}_1 + \ldots y_m\vec{e}_m = y_1A\vec{b}_1 + \ldots y_mA\vec{b}_m = A(y_1\vec{b}_1 + \ldots y_m\vec{b}_m)</span>.<br />
So <span class="math inline">A\vec{x} = \vec{y}</span> has a solution <span class="math inline">\vec{x} = y_1\vec{b}_1 + \ldots y_m\vec{b}_m</span> for all <span class="math inline">\vec{y} \in \mb{R}^m</span>, and <span class="math inline">\rank A = m</span> by theorem 2.2.4.<br />
So <span class="math inline">n \ge m</span>, a contradiction. Therefore, <span class="math inline">A</span> has no right inverse.</p>
</blockquote>
<h3 id="inverse-matrices">Inverse Matrices</h3>
<p>Let <span class="math inline">A</span> be an <span class="math inline">n \times n</span> square matrix. If <span class="math inline">BA = I = AB</span>, then <span class="math inline">B</span> is the <strong>inverse</strong> of <span class="math inline">A</span>. If <span class="math inline">B</span> exists, then <span class="math inline">A</span> is <strong>invertible</strong>.</p>
<p>The inverse of the matrix is denoted <span class="math inline">B = A^{-1}</span> - <span class="math inline">B</span> is the inverse of <span class="math inline">A</span>.</p>
<p>In other words, only square matrices can have inverses, but not all of them do.</p>
<h3 id="theorem-5.1.3">Theorem 5.1.3</h3>
<p>The inverse of a matrix is always unique.</p>
<p>Proof:</p>
<blockquote>
<p>Let <span class="math inline">B, C</span> be inverses of <span class="math inline">A</span>. Then <span class="math inline">B = BI = B(AC) = (BA)C = IC = C</span>.</p>
</blockquote>
<h3 id="theorem-5.1.4">Theorem 5.1.4</h3>
<p>If <span class="math inline">A, B \in M_{n \times n}</span> such that <span class="math inline">AB = I</span>, then <span class="math inline">A = B^{-1}</span> and <span class="math inline">B = A^{-1}</span>, and <span class="math inline">BA = I</span>.</p>
<p>Also, the RREF of <span class="math inline">A</span>, and the RREF of <span class="math inline">B</span>, are both <span class="math inline">I</span>.</p>
<p>Basically, if a square matrix has a left or right inverse, then this is also the right or left inverse, and is simply the inverse of the matrix.</p>
<p>So we can simply use the algorithm for finding left or right inverses to find the multiplciative inverse of the matrix.</p>
<p>So if <span class="math inline">A</span> is invertible, then row reducing <span class="math inline">\sys{A}{I}</span> results in <span class="math inline">\sys{I}{A^{-1}}</span> - <strong>row reducing the system results in the inverse</strong>.</p>
<p>Proof:</p>
<blockquote>
<p>Let <span class="math inline">B\vec{x} = \vec{0}</span>. Then <span class="math inline">\vec{0} = A\vec{0} = A(B\vec{x}) = (AB)\vec{x} = I\vec{x} = \vec{x}</span>.<br />
So <span class="math inline">\vec{x} = 0</span> and <span class="math inline">\vec{x}</span> is the unique solution to the system. Since the solution is unique, <span class="math inline">\rank B = n</span> and there is a leading one in each column of the RREF, so the RREF of <span class="math inline">B</span> is <span class="math inline">I</span>.<br />
Since the RREF of <span class="math inline">B</span> is <span class="math inline">I</span>, <span class="math inline">\forall \vec{b} \in \mb{R}^n, \exists \vec{x} \in \mb{R}^n, B\vec{x} = \vec{b}</span>.<br />
Clearly, <span class="math inline">BA\vec{b} = (BA)(B\vec{x}) = B(AB)\vec{x} = BI\vec{x} = B\vec{x} = \vec{b}</span>.<br />
By theorem 3.1.4, <span class="math inline">BA = I</span>.<br />
Now the same argument used to prove <span class="math inline">\rank B = n</span> can then be used to probe <span class="math inline">\rank A = n</span>.</p>
</blockquote>
<h3 id="theorem-5.1.5">Theorem 5.1.5</h3>
<p>If the RREF of a matrix <span class="math inline">A \in M_{n \times n}</span> is <span class="math inline">I</span>, then the <span class="math inline">A^{-1}</span> exists.</p>
<p>In other words, if <span class="math inline">\rank A = n</span>, then <span class="math inline">A</span> is invertible.</p>
<p>Find the general inverse of <span class="math inline">A = \begin{bmatrix} a &amp; b \\ c &amp; d \end{bmatrix}</span>:</p>
<blockquote>
<p>Clearly, <span class="math inline">\left[\begin{array}{cc|cc} a &amp; b &amp; 1 &amp; 0 \\ c &amp; d &amp; 0 &amp; 1 \end{array}\right]</span> in RREF is <span class="math inline">\left[\begin{array}{cc|cc} 1 &amp; 0 &amp; \frac{d}{ad - bc} &amp; -\frac{b}{ad - bc} \\ 0 &amp; 1 &amp; -\frac{c}{ad - bc} &amp; \frac{a}{ad - bc} \end{array}\right]</span>.<br />
So <span class="math inline">A^{-1} = \begin{bmatrix} \frac{d}{ad - bc} &amp; -\frac{b}{ad - bc} \\ -\frac{c}{ad - bc} &amp; \frac{a}{ad - bc} \end{bmatrix} = \frac{1}{ad - bc}\begin{bmatrix} d &amp; -b \\ -c &amp; a \end{bmatrix}</span>, and is defined whenever <span class="math inline">ad - bc \ne 0</span>.</p>
</blockquote>
<h3 id="theorem-5.1.6">Theorem 5.1.6</h3>
<p>Given <span class="math inline">A, B \in M_{n \times n}</span> such that <span class="math inline">A</span> and <span class="math inline">B</span> are invertible and <span class="math inline">k \in \mb{R}</span>:</p>
<ul>
<li><span class="math inline">(kA)^{-1} = \frac{1}{k}A^{-1}</span></li>
<li><span class="math inline">(AB)^{-1} = B^{-1}A^{-1}</span></li>
<li><span class="math inline">(A^T)^{-1} = (A^{-1})^T</span></li>
</ul>
<p>Proof:</p>
<p>(proof of first)</p>
<blockquote>
<p>Clearly, <span class="math inline">kA(kA)^{-1} = I</span>. Clearly, <span class="math inline">kA\frac{1}{k}A^{-1} = AA^{-1} = I</span>.<br />
Since the inverse is always unique, <span class="math inline">(kA)^{-1} = \frac{1}{k}A^{-1}</span>.</p>
</blockquote>
<p>(proof of second)</p>
<blockquote>
<p>Clearly, <span class="math inline">AB(AB)^{-1} = I</span>. Clearly, <span class="math inline">ABB^{-1}A^{-1} = AIA^{-1} = I</span>.<br />
Since the inverse is always unique, <span class="math inline">(AB)^{-1} = B^{-1}A^{-1}</span>.</p>
</blockquote>
<p>(proof of third)</p>
<blockquote>
<p>Clearly, <span class="math inline">A^T(A^T)^{-1} = I</span>. Clearly, <span class="math inline">A^T(A^{-1})^T = (A^{-1}A)^T = I^T = I</span>.<br />
Since the inverse is always unique, <span class="math inline">(A^T)^{-1} = (A^{-1})^T</span>.</p>
</blockquote>
<h3 id="theorem-5.1.7-invertible-matrix-theorem">Theorem 5.1.7 (Invertible Matrix Theorem)</h3>
<p>Given <span class="math inline">A \in M_{n \times n}</span>, all of the following are equivalent:</p>
<ul>
<li><span class="math inline">A</span> is invertible.</li>
<li>The RREF of <span class="math inline">A</span> is <span class="math inline">I</span>.</li>
<li><span class="math inline">\rank A = n</span>.</li>
<li>The system of equations <span class="math inline">A\vec{x} = \vec{b}</span> has unique solutions for all <span class="math inline">\vec{b} \in \mb{R}^n</span>.</li>
<li>The nullspace of <span class="math inline">A</span> is <span class="math inline">\set{\vec{0}}</span>.</li>
<li>The rows or columns of <span class="math inline">A</span> form a basis for <span class="math inline">\mb{R}^n</span>.</li>
<li><span class="math inline">A^T</span> is invertible.</li>
</ul>
<p>Proof of third:</p>
<blockquote>
<p>Clearly, <span class="math inline">\rank A = n</span> if and only if the RREF of <span class="math inline">A</span> is <span class="math inline">I</span>.<br />
Clearly, the RREF is <span class="math inline">I</span> if and only if there are a sequence of elementary matrices <span class="math inline">E_k \cdots E_1</span> such that <span class="math inline">E_k \cdots E_1 = I</span>.<br />
Clearly, this the inverse, so <span class="math inline">A</span> is invertible if and only if <span class="math inline">E_k \cdots E_1</span> exist.</p>
</blockquote>
<p>Proof of fourth:</p>
<blockquote>
<p>Clearly, <span class="math inline">A\vec{x} = \vec{b}</span> has unique solutions for all <span class="math inline">\vec{b} \in \mb{R}^n</span> if and only if there are no free variables.<br />
Clearly, there are no free variables if and only if <span class="math inline">\rank A = n</span>.</p>
</blockquote>
<p>Proof of fifth:</p>
<blockquote>
<p>Clearly, the nullspace of <span class="math inline">A</span> is <span class="math inline">\set{\vec{0}}</span> if and only if <span class="math inline">\vec{x} = \vec{0}</span> is the only solution to <span class="math inline">A\vec{x} = \vec{0}</span>.<br />
Clearly, the solution is unique if and only if <span class="math inline">\rank A = n</span>.</p>
</blockquote>
<p>Proof of sixth:</p>
<blockquote>
<p>Clearly, the rows or columns of $A form a basis for <span class="math inline">\mb{R}^n</span> if and only if they span <span class="math inline">\mb{R}^n</span> and are linearly independent.<br />
Since the rows or columns are linearly independent, the nullspace of <span class="math inline">A</span> is <span class="math inline">\set{\vec{0}}</span>.</p>
</blockquote>
<p>Proof of seventh:</p>
<blockquote>
<p>Clearly, <span class="math inline">A^T</span> is invertible if and only if <span class="math inline">(A^{-1})^T</span> exists.<br />
CLearly, <span class="math inline">(A^{-1})^T</span> exists if and only if <span class="math inline">A</span> is invertible.</p>
</blockquote>
<p>Also note that <span class="math inline">A\vec{x} = \vec{b}</span> if and only if <span class="math inline">A^{-1}A\vec{x} = \vec{x} = A^{-1}\vec{b}</span>.</p>
<p>In other words, we can solve linear systems this way. If <span class="math inline">A\vec{x} = \vec{b}</span>, then <span class="math inline">\vec{x} = A^{-1}\vec{b}</span>, and the converse is true as well.</p>
<h2 id="elementary-matrices">Elementary Matrices</h2>
<p>When we found the inverse of a matrix <span class="math inline">A</span> by row reducing <span class="math inline">\sys{A}{I}</span> into <span class="math inline">\sys{I}{A^{-1}}</span>, we applied a series of elementary row operations in a certain order, and this resulted in finding <span class="math inline">A^{-1}</span>.</p>
<p>Since we can, given any <span class="math inline">A^{-1}</span>, find <span class="math inline">A</span> again, those elementary row operations must have been encoded in <span class="math inline">A^{-1}</span> somehow.</p>
<p>An <strong>elementary matrix</strong> is the result of applying an elementary row operation on the <span class="math inline">n \times n</span> identity matrix. The elementary matrix <strong>represents</strong> the elementary row operation.</p>
<h3 id="theorem-5.2.1-5.2.3">Theorem 5.2.1-5.2.3</h3>
<p>Let <span class="math inline">A \in M_{m \times n}</span>. Let <span class="math inline">E</span> be the elementary matrix representing an elementary row operation (<span class="math inline">R_i + cR_j, c \ne 0, i \ne j</span>, <span class="math inline">cR_i, c \ne 0</span>, or <span class="math inline">R_i \leftrightarrow R_j, i \ne j</span>).</p>
<p>Then <span class="math inline">EA</span> is the matrix obtained by applying that elementary row operation directly onto <span class="math inline">A</span>.</p>
<p>In other words, applying elementary row operations to identity matrices allow the matrices to represent the operations in a more elegant way.</p>
<p>Since the elementary row operations cannot change the rank of the matrix, <span class="math inline">\rank EA = \rank A</span></p>
<p>Since all elementary row operations are reversible, likewise all elementary matrices are invertible. In fact, the inverse of an elementary matrix is the reverse of the elementar row operation applied to the identity matrix.</p>
<p>For example, <span class="math inline">E = \begin{bmatrix} 1 &amp; 0 \\ 0 &amp; -5 \end{bmatrix}</span> represents the elementary row operation <span class="math inline">-5R_2</span>, and the reverse operation is <span class="math inline">-\frac{1}{5}R_2</span>. So <span class="math inline">E^{-1} = \begin{bmatrix} 1 &amp; 0 \\ 0 &amp; -\frac{1}{5} \end{bmatrix}</span>.</p>
<h2 id="matrix-decomposition">Matrix Decomposition</h2>
<h3 id="theorem-5.2.5">Theorem 5.2.5</h3>
<p>If <span class="math inline">A</span> is an <span class="math inline">m \times n</span> matrix, and <span class="math inline">R</span> is the RREF of <span class="math inline">A</span>, then <span class="math inline">\exists E_1, \ldots, E_k \in M_{m \times m}, E_k \cdots E_1 A = R</span>.</p>
<p>Also, if <span class="math inline">E_k \cdots E_1 A = R</span>, then <span class="math inline">A = E_1^{-1} \cdots E_k^{-1} R</span>.</p>
<p>This can be derived by multiplying both sides of <span class="math inline">E_k \cdots E_1 A = R</span> by <span class="math inline">E_k^{-1}, \ldots, E_1^{-1}</span> repeatedly.</p>
<p>Convert <span class="math inline">A = \begin{bmatrix} 1 &amp; 2 \\ 3 &amp; 4 \end{bmatrix}</span> into RREF using elementary matrices:</p>
<blockquote>
<p>Clearly, <span class="math inline">A = \begin{bmatrix} 1 &amp; 0 \\ -3 &amp; 1 \end{bmatrix} \begin{bmatrix} 1 &amp; 2 \\ 3 &amp; 4 \end{bmatrix} = \begin{bmatrix} 1 &amp; 2 \\ 0 &amp; -2 \end{bmatrix}</span> Clearly, <span class="math inline">\begin{bmatrix} 1 &amp; 1 \\ 0 &amp; 1 \end{bmatrix} \begin{bmatrix} 1 &amp; 2 \\ 0 &amp; -2 \end{bmatrix} = \begin{bmatrix} 1 &amp; 0 \\ 0 &amp; -2 \end{bmatrix}</span> Clearly, <span class="math inline">\begin{bmatrix} 1 &amp; 1 \\ 0 &amp; -\frac{1}{2} \end{bmatrix} \begin{bmatrix} 1 &amp; 0 \\ 0 &amp; -2 \end{bmatrix} = \begin{bmatrix} 1 &amp; 0 \\ 0 &amp; 1 \end{bmatrix}</span>.<br />
So <span class="math inline">\begin{bmatrix} 1 &amp; 0 \\ 0 &amp; 1 \end{bmatrix} = \begin{bmatrix} 1 &amp; 1 \\ 0 &amp; -\frac{1}{2} \end{bmatrix} \begin{bmatrix} 1 &amp; 1 \\ 0 &amp; 1 \end{bmatrix} \begin{bmatrix} 1 &amp; 0 \\ -3 &amp; 1 \end{bmatrix} \begin{bmatrix} 1 &amp; 2 \\ 3 &amp; 4 \end{bmatrix}</span>.</p>
</blockquote>
<h3 id="inverses">Inverses</h3>
<p>If <span class="math inline">A</span> is invertible, then the RREF <span class="math inline">R</span> is <span class="math inline">I</span>, so <span class="math inline">E_k \cdots E_1 A = I</span> and <span class="math inline">A = E_1^{-1} \cdots E_k^{-1} I</span>.</p>
<p>So <span class="math inline">E_k \cdots E_1</span> is the left inverse of <span class="math inline">A</span>, by definition. So <span class="math inline">A^{-1} = E_k \cdots E_1</span>. Also, <span class="math inline">A = E_1^{-1} \cdots E_k^{-1}</span>.</p>
<p>In other words, if <span class="math inline">A</span> is invertible, then <span class="math inline">A</span> and <span class="math inline">A^{-1}</span> can both be written as products of elementary matrices.</p>
<p>Also, since we are row reducing <span class="math inline">\sys{A}{I}</span> into <span class="math inline">\sys{I}{A^{-1}}</span>, and <span class="math inline">I = E_k \cdots E_1 A</span>, then <span class="math inline">A^{-1} = E_k \cdots E_1 I = E_k \cdots E_1</span>. In other words, we are getting the inverse written in terms of elementary row operations.</p>
<h2 id="matrix-determinants">Matrix Determinants</h2>
<p>The <strong>determinant</strong> of a matrix is an expression over the values within the matrix. The value of the determinant provides important information about it, such as whether it is invertible. The determinant <strong>only applies to square matrices</strong>.</p>
<p>We denote the determinant of a matrix <span class="math inline">A</span> as <span class="math inline">\det A = \begin{vmatrix} A \end{vmatrix} = \begin{vmatrix} a_{1, 1} &amp; \ldots &amp; a_{1, n} \\ \vdots &amp; \vdots &amp; \vdots \\ a_{m, 1} &amp; \ldots &amp; a_{m, n} \end{vmatrix}</span>. Importantly, <span class="math inline">A</span> is invertible if and only if <span class="math inline">\det A \ne 0</span>.</p>
<p>The determinant of a <span class="math inline">1 \times 1</span> matrix <span class="math inline">A = \begin{bmatrix} a \end{bmatrix}</span> is <span class="math inline">\det A = a</span>. This is because the matrix is invertible if and only if <span class="math inline">a \ne 0</span>.</p>
<p>The determinant of the <span class="math inline">2 \times 2</span> matrix <span class="math inline">A = \begin{bmatrix} a &amp; b \\ c &amp; d \end{bmatrix}</span> is <span class="math inline">\det A = ad - bc</span>. Note that this is equal to <span class="math inline">a\begin{vmatrix} d \end{vmatrix} - b\begin{vmatrix} c \end{vmatrix}</span>.</p>
<p>The determinant of the <span class="math inline">3 \times 3</span> matrix <span class="math inline">A = \begin{bmatrix} a &amp; b &amp; c \\ d &amp; e &amp; f \\ g &amp; h &amp; i \end{bmatrix}</span> is <span class="math inline">\det A = aei - afh + bfg - bdi + cdh - cei</span>. Note that this is equal to <span class="math inline">a\begin{vmatrix} e &amp; f \\ h &amp; i \end{vmatrix} - b\begin{vmatrix} d &amp; f \\ g &amp; i \end{vmatrix} + c\begin{bmatrix} d &amp; e \\ g &amp; h \end{bmatrix}</span>.</p>
<p>For systems of linear equations, the determinant is 0 if and only if the system has one and only one solution.</p>
<h3 id="cofactors">Cofactors</h3>
<p>Let <span class="math inline">A \in M_{m \times n}</span>, <span class="math inline">n \ge 2</span>. Let <span class="math inline">A(i, j) \in M_{m - 1, n - 1}</span> be <span class="math inline">A</span>, except without row <span class="math inline">i</span> and column <span class="math inline">j</span>.</p>
<p>The <strong>cofactor</strong> of <span class="math inline">A_{i, j}</span> is <span class="math inline">C_{i, j} = (-1)^{i + j} \det A(i, j)</span>. <span class="math inline">A</span> therefore has <span class="math inline">mn</span> cofactors.</p>
<p>Using this notation, we can write the determinant of a <span class="math inline">3 \times 3</span> matrix as <span class="math inline">\det A = a_{1, 1}C_{1, 1} + a_{1, 2}C_{1, 2} + a_{1, 3}C_{1, 3}</span> or <span class="math inline">a_{2, 1}C_{2, 1} + a_{2, 2}C_{2, 2} + a_{2, 3}C_{2, 3}</span> or even <span class="math inline">a_{1, 1}C_{1, 1} + a_{2, 2}C_{2, 2} + a_{3, 3}C_{3, 3}</span>.</p>
<p>The cofactors of <span class="math inline">A</span> are denoted <span class="math inline">\cof A = \begin{bmatrix} C_{1, 1} &amp; \ldots &amp; C_{1, n} \\ \vdots &amp; \vdots &amp; \vdots \\ C_{n, 1} &amp; \ldots &amp; C_{n, n} \end{bmatrix}</span>.</p>
<h3 id="theorem-5.3.1">Theorem 5.3.1</h3>
<p>Given <span class="math inline">A \in M_{n \times n}, n \ge 2</span>, for all <span class="math inline">1 \le i \le n</span>, <span class="math inline">\det A = a_{i, 1}C_{i, 1} + \ldots + a_{i, n}C_{i, n} = \sum_{j = 1}^n a_{i, j}C_{i, j}</span> and <span class="math inline">\det A = a_{1, i}C_{1, i} + \ldots + a_{n, i}C_{n, i} = \sum_{j = 1}^n a_{j, i}C_{j, i}</span>.</p>
<p><span class="math inline">\sum_{j = 1}^n a_{i, j}C_{i, j}</span> is the <strong>cofactor expansion</strong> across the <span class="math inline">i</span>th row.</p>
<p><span class="math inline">\sum_{j = 1}^n a_{j, i}C_{j, i}</span> is called the <strong>cofactor expansion</strong> across the <span class="math inline">i</span>th column.</p>
<p>In other words, we can calculate the determinant of any <span class="math inline">n \times n</span> matrix using a simple formula based on the determinant of <span class="math inline">n</span> <span class="math inline">(n - 1) \times (n - 1)</span> matrices.</p>
<p>We can choose any <span class="math inline">i</span> we want. Often, the best choice has a lot of zeros in <span class="math inline">a_{i, j}</span>, so that we can avoid as much computation as possible.</p>
<p>For example, find <span class="math inline">\det \begin{bmatrix} 3 &amp; -2 &amp; 8 &amp; 2 \\ 0 &amp; -2 &amp; 5 &amp; 5 \\ 0 &amp; 0 &amp; 4 &amp; 8 \\ 0 &amp; 0 &amp; 0 &amp; 7 \end{bmatrix}</span>:</p>
<blockquote>
<p>We choose to cofactor expand across the fourth row, since there are a lot of zeros there.<br />
Clearly, <span class="math inline">\det \begin{bmatrix} 3 &amp; -2 &amp; 8 &amp; 2 \\ 0 &amp; -2 &amp; 5 &amp; 5 \\ 0 &amp; 0 &amp; 4 &amp; 8 \\ 0 &amp; 0 &amp; 0 &amp; 7 \end{bmatrix} = a_{4, 1}C_{4, 1} + a_{4, 2}C_{4, 2} + a_{4, 3}C_{4, 3} + a_{4, 4}C_{4, 4} = 0C_{4, 1} + 0C_{4, 2} + 0C_{4, 3} + 7C_{4, 4} = 7C_{4, 4}</span>.<br />
So <span class="math inline">7C_{4, 4} = 7(-1)^{4 + 4}\det \begin{bmatrix} 3 &amp; -2 &amp; 8 \\ 0 &amp; -2 &amp; 5 \\ 0 &amp; 0 &amp; 4 \end{bmatrix}</span>.<br />
We choose to cofactor expand across the third row, since there are a lot of zeroes there.<br />
So <span class="math inline">7C_{4, 4} = 7(b_{3, 1}C_{3, 1} + b_{3, 2}C_{3, 2} + b_{3, 3}C_{3, 3}) = 7(0C_{3, 1} + 0C_{3, 2} + 4C_{3, 3}) = 28C_{3, 3}</span>.<br />
So <span class="math inline">28C_{3, 3} = 28(-1)^{3 + 3}\det \begin{bmatrix} 3 &amp; -2 \\ 0 &amp; -2 \end{bmatrix} = 28(3 \cdot -2 - (-2) \cdot 0) = 28 \cdot -6</span>.<br />
So <span class="math inline">\det \begin{bmatrix} 3 &amp; -2 &amp; 8 &amp; 2 \\ 0 &amp; -2 &amp; 5 &amp; 5 \\ 0 &amp; 0 &amp; 4 &amp; 8 \\ 0 &amp; 0 &amp; 0 &amp; 7 \end{bmatrix} = -168</span>.</p>
</blockquote>
<h3 id="triangular-matrices">Triangular Matrices</h3>
<p>Let <span class="math inline">1 \le i \le n</span> and <span class="math inline">1 \le j \le n</span>.</p>
<p>A matrix <span class="math inline">A</span> is <strong>upper triangular</strong> if and only if <span class="math inline">i &gt; j \implies A_{i, j} = 0</span>. In other words, it is upper triangular if it has a triangle of zeroes on the bottom left up to but not including the biggest diagonal.</p>
<p>A matrix <span class="math inline">A</span> is <strong>lower triangular</strong> if and only if <span class="math inline">i &lt; j \implies A_{i, j} = 0</span>. In other words, it is lower triangular if it has a triangle of zeroes on the top right down to but not including the biggest diagonal.</p>
<p><span class="math inline">\begin{bmatrix} 0 &amp; 0 &amp; 1 \\ 0 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 1 \end{bmatrix}</span> is upper triangular but not lower triangular. The identity matrix is both.</p>
<h3 id="theorem-5.3.2">Theorem 5.3.2</h3>
<p>Given <span class="math inline">A \in M_{n \times n}</span>, if <span class="math inline">A</span> is upper or lower triangular, then <span class="math inline">\det A = a_{1, 1} \cdots a_{n, n} = \sum_{i = 1}^n a_{i, i}</span>.</p>
<p>In other words, a triangular matrix allows us to calculate the determinant simply by multiplying the elements in the diagonal together.</p>
<p>As a result, if <span class="math inline">A</span> is triangular, then <span class="math inline">A</span> is invertible if and only if there are no zero elements in the diagonal.</p>
<h1 id="section-11">22/3/14</h1>
<p>It is easy to get the determinant of a triangular matrix - all we have to do is multiply the elements along the diagonal.</p>
<p>We can actually use elementary row operations to put a matrix into diagonal form, to make it easier to calculate the determinant.</p>
<h3 id="theorem-5.3.3-5.3.5">Theorem 5.3.3-5.3.5</h3>
<p>Let <span class="math inline">A \in M_{m, n}</span>. Let <span class="math inline">c \in \mb{R}, c \ne 0</span>.</p>
<p>If <span class="math inline">B</span> is <span class="math inline">A</span> with one row multiplied by <span class="math inline">c</span>, then <span class="math inline">\det B = c \det A</span>. This corresponds to the operation <span class="math inline">cR_i</span>, and we just multiply the determinant by that value.</p>
<p>If <span class="math inline">B</span> is <span class="math inline">A</span> with two different rows swapped, then <span class="math inline">\det B = -\det A</span>. This corresponds to the operation <span class="math inline">R_i \leftrightarrow R_j</span>, and we just invert the determinant.</p>
<p>If <span class="math inline">B</span> is <span class="math inline">A</span> with <span class="math inline">c</span> times one row being added to another row, then <span class="math inline">\det B = \det A</span>. This corresponds to the operation <span class="math inline">R_i + cR_j</span>, and the determinant is unaffected.</p>
<p>Find <span class="math inline">\begin{vmatrix} 3 &amp; 4 &amp; 3 &amp; -1 \\ 1 &amp; 0 &amp; -2 &amp; 2 \\ -2 &amp; 1 &amp; 1 &amp; 4 \\ 1 &amp; 2 &amp; 1 &amp; 1 \end{vmatrix}</span> using elementary matrices:</p>
<blockquote>
<p>Clearly, <span class="math inline">\det \begin{bmatrix} 3 &amp; 4 &amp; 3 &amp; -1 \\ 1 &amp; 0 &amp; -2 &amp; 2 \\ -2 &amp; 1 &amp; 1 &amp; 4 \\ 1 &amp; 2 &amp; 1 &amp; 1 \end{bmatrix} = \det \begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; -\frac{21}{9} \\ 0 &amp; 1 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; 1 \end{bmatrix}\begin{bmatrix} 1 &amp; 0 &amp; -4 &amp; 0 \\ 0 &amp; 1 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; 1 \end{bmatrix}\begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; -2 &amp; 1 \end{bmatrix}\begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 1 &amp; 0 \\ 0 &amp; -1 &amp; 0 &amp; 1 \end{bmatrix}\begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 &amp; 0 \\ 0 &amp; 2 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; 1 \end{bmatrix}\begin{bmatrix} 1 &amp; -3 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; 1 \end{bmatrix}\begin{bmatrix} 3 &amp; 4 &amp; 3 &amp; -1 \\ 1 &amp; 0 &amp; -2 &amp; 2 \\ -2 &amp; 1 &amp; 1 &amp; 4 \\ 1 &amp; 2 &amp; 1 &amp; 1 \end{bmatrix} = \det \begin{bmatrix} 0 &amp; 0 &amp; 0 &amp; \frac{2}{3} \\ 1 &amp; 0 &amp; -2 &amp; 2 \\ 0 &amp; 1 &amp; -3 &amp; 8 \\ 0 &amp; 0 &amp; 9 &amp; -17 \end{bmatrix}</span>.<br />
Clearly, <span class="math inline">\det \begin{bmatrix} 0 &amp; 0 &amp; 0 &amp; \frac{2}{3} \\ 1 &amp; 0 &amp; -2 &amp; 2 \\ 0 &amp; 1 &amp; -3 &amp; 8 \\ 0 &amp; 0 &amp; 9 &amp; -17 \end{bmatrix} = -\det \begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; 1 \\ 0 &amp; 0 &amp; 1 &amp; 0 \end{bmatrix}\begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 1 &amp; 0 \\ 0 &amp; 1 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; 1 \end{bmatrix}\begin{bmatrix} 0 &amp; 1 &amp; 0 &amp; 0 \\ 1 &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; 1 \end{bmatrix}\begin{bmatrix} 0 &amp; 0 &amp; 0 &amp; \frac{2}{3} \\ 1 &amp; 0 &amp; -2 &amp; 2 \\ 0 &amp; 1 &amp; -3 &amp; 8 \\ 0 &amp; 0 &amp; 9 &amp; -17 \end{bmatrix} = -\det \begin{bmatrix} 1 &amp; 0 &amp; -2 &amp; 2 \\ 0 &amp; 1 &amp; -3 &amp; 8 \\ 0 &amp; 0 &amp; 9 &amp; -17 \\ 0 &amp; 0 &amp; 0 &amp; \frac{2}{3} \end{bmatrix}</span>.<br />
Clearly, the matrix is now in upper triangular form, so <span class="math inline">-\det \begin{bmatrix} 1 &amp; 0 &amp; -2 &amp; 2 \\ 0 &amp; 1 &amp; -3 &amp; 8 \\ 0 &amp; 0 &amp; 9 &amp; -17 \\ 0 &amp; 0 &amp; 0 &amp; \frac{2}{3} \end{bmatrix} = -(1 \cdot 1 \cdot 9 \cdot \frac{2}{3}) = -6</span>, by theorem 5.3.2.</p>
</blockquote>
<p>The basic technique is using elementary row operations to reduce the matrix into upper triangular form, and then using theorem 5.3.2 to easily calculate the determinant just by multiplying the numbers in the diagonal together.</p>
<h3 id="theorem-5.3.6">Theorem 5.3.6</h3>
<p>Given <span class="math inline">A \in M_{n \times n}</span>, <span class="math inline">\det A = \det A^T</span>. In other words, the determinant of a matrix is the same as the determinant of its inverse.</p>
<p>This allows us to do &quot;column operations&quot; to simplify the determinant - row operations, but applied to the transpose of the matrix.</p>
<p>When we perform a row operation on a matrix, the determinant is always multiplied by a scalar <span class="math inline">c \ne 0</span> (this is <span class="math inline">c</span>, 1, or -1 for multiplying a row, adding a scalar multiple, and swapping, respectively).</p>
<p>As a result, the determinant of an elementary matrix is never 0, since the row operation always multiplies the determinant by a nonzero value, and the determinant of the identity is 1.</p>
<p>Let <span class="math inline">E</span> be an elementary matrix. Since <span class="math inline">E</span> is <span class="math inline">I</span> with a row operation applied to it, and <span class="math inline">\det I = 1</span>, <span class="math inline">\det E = c</span>.</p>
<p>Since <span class="math inline">EA</span> is equivalent to applying that same row operation to <span class="math inline">A</span>, <span class="math inline">\det EA = c \det A</span>.</p>
<p>So <span class="math inline">\det EA = \det E \det A</span>.</p>
<h3 id="theorem-5.3.8">Theorem 5.3.8</h3>
<p>This theorem extends theorem 5.1.7 - the Invertible Matrix Theorem.</p>
<p>Given <span class="math inline">A \in M_{n \times n}</span>, <span class="math inline">A^{-1}</span> exists if and only if <span class="math inline">\det A \ne 0</span>.</p>
<p>Proof:</p>
<blockquote>
<p>Let <span class="math inline">R</span> be the RREF of <span class="math inline">A</span>. Clearly, there exists a sequence of elementary matrices <span class="math inline">E_k, \ldots, E_1</span>, such that <span class="math inline">E_k \cdots E_1 A = R</span>.<br />
So <span class="math inline">\det R = \det E_k \cdots \det E_1 \det A</span>.<br />
Since the determinant of any elementary matrix is always nonzero, <span class="math inline">\det R \ne 0 \iff \det A \ne 0</span>.<br />
Since <span class="math inline">R</span> is in RREF and is square, <span class="math inline">\det R \ne 0 \iff R = I</span>.<br />
Clearly, <span class="math inline">R = I</span> if and only if <span class="math inline">E_k \cdots \det E_1</span> is the inverse of <span class="math inline">A</span>.<br />
So <span class="math inline">A</span> is invertible if and only if <span class="math inline">\det A \ne 0</span>.</p>
</blockquote>
<h3 id="theorem-5.3.9">Theorem 5.3.9</h3>
<p>Let <span class="math inline">A, B \in M_{n \times n}</span>. Then <span class="math inline">\det AB = \det A \det B</span>.</p>
<p>In other words, the determinant of two square matrices of the same dimensions is the same as the product of their individual determinants.</p>
<p>Proof:</p>
<blockquote>
<p>Let <span class="math inline">R</span> be the RREF of <span class="math inline">A</span>.<br />
Clearly, there exists a sequence of elementary matrices <span class="math inline">E_k, \ldots, E_1</span>, such that <span class="math inline">E_k \cdots E_1 A = R</span> and <span class="math inline">A = E_1^{-1} \cdots E_k^{-1} R</span>.<br />
Clearly, <span class="math inline">\det AB = \det E_1^{-1} \cdots E_k^{-1} RB = \det E_1^{-1} \cdots E_k^{-1} \det RB</span>, and <span class="math inline">\det E_1^{-1} \cdots E_k^{-1} \ne 0</span> since they are all elementary matrices.<br />
Assume <span class="math inline">\det A = 0</span>. Then <span class="math inline">R \ne I</span>, and since <span class="math inline">R</span> is in RREF, the last row is all zeros, and <span class="math inline">\det R = 0</span>.<br />
Clearly, the last row of <span class="math inline">RB</span> is 0 since the last row of <span class="math inline">R</span> is all zeroes.<br />
So <span class="math inline">\det AB = 0 = \det A \det B</span>.<br />
Assume <span class="math inline">\det A \ne 0</span>. Then <span class="math inline">A</span> is invertible, and by theorem 5.1.7, <span class="math inline">R = I</span>.<br />
So <span class="math inline">\det AB = \det E_1^{-1} \cdots E_k^{-1} B = \det E_1^{-1} \cdots E_k^{-1} \det B = \det E_1^{-1} \cdots E_k^{-1} R \det B = \det A \det B</span>.<br />
So in all cases, <span class="math inline">\det AB = \det A \det B</span>.</p>
</blockquote>
<p>As a result, if we use <span class="math inline">B = A^{-1}</span>, then <span class="math inline">\det AA^{-1} = \det I = 1 = \det A \det A^{-1}</span>, and <span class="math inline">\det A = \frac{1}{\det A^{-1}}</span>.</p>
<h1 id="section-12">22/3/14</h1>
<h2 id="adjugates">Adjugates</h2>
<p>Let <span class="math inline">A \in M_{n \times n}</span>.</p>
<p>The *<strong>adjugate</strong> of <span class="math inline">A</span> is <span class="math inline">\begin{bmatrix} C_{1, 1} &amp; \ldots &amp; C_{n, 1} \\ \vdots &amp; \vdots &amp; \vdots \\ C_{1, n} &amp; \ldots &amp; C_{n, n} \end{bmatrix}</span>.</p>
<p>In other words, the adjugate of a matrix is the transpose of cofactors matrix - <span class="math inline">\adj A = (\cof A)^T</span>. So <span class="math inline">(\adj A)_{i, j} = C_{j, i}</span>.</p>
<p>For example, <span class="math inline">\cof \begin{bmatrix} a &amp; b \\ c &amp; d \end{bmatrix} = \begin{bmatrix} a &amp; -c \\ -b &amp; a \end{bmatrix}</span>, so <span class="math inline">\adj A = \begin{bmatrix} d &amp; -b \\ -c &amp; a \end{bmatrix}</span>.</p>
<p>The adjugate is extremely useful because it allows us to find the inverse of any matrix.</p>
<p>This is because <span class="math inline">A^{-1} = \frac{1}{\det A} \adj A</span>.</p>
<p>In addition, this means that <span class="math inline">A_{i, j}^{-1} = \frac{1}{\det A} C_{j, i} = \frac{1}{\det A} (-1)^{i + j} \det A(i, j)</span>, where <span class="math inline">A(i, j)</span> is <span class="math inline">A</span> with row <span class="math inline">i</span> and column <span class="math inline">j</span> removed so it is <span class="math inline">(n - 1) \times (n - 1)</span>.</p>
<p>We have also seen the elementary matrix method of finding the inverse, where we row reduce the matrix while keeping track of all the elementary matrices along the way, then invert these elementary matrices and multiply them.</p>
<h3 id="cramers-rule">Cramer's Rule</h3>
<p>Consider the linear system <span class="math inline">A\vec{x} = \vec{b}</span>. The solution is <span class="math inline">\vec{x} = A^{-1}\vec{b}</span>, so we know that it has a solution if and only if <span class="math inline">A</span> is invertible.</p>
<p>Since <span class="math inline">A^{-1} = \frac{1}{\det A} \adj A</span>, then <span class="math inline">\vec{x} = \frac{1}{\det A} \adj A \vec{b} = \frac{1}{\det A} \begin{bmatrix} b_1C_{1, 1} + \ldots + b_nC_{n, 1} \\ \vdots \\ b_1C_{1, n} + \ldots + b_nC_{n, n} \end{bmatrix}</span>.</p>
<p>Let <span class="math inline">A_i = \begin{bmatrix} a_{1, 1} &amp; \ldots &amp; a_{1, i - 1} &amp; b_1 &amp; a_{1, i + 1} &amp; \ldots &amp; a_{1, n} \\ \vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots \\ a_{n, 1} &amp; \ldots &amp; a_{n, i - 1} &amp; b_n &amp; a_{n, i + 1} &amp; \ldots &amp; a_{n, n} \end{bmatrix}</span>. This is basically <span class="math inline">A</span> with the <span class="math inline">i</span>th column replaced by <span class="math inline">\vec{b}</span>.</p>
<p>Note that <span class="math inline">\det A_i = \det \begin{bmatrix} b_1C_{1, i} &amp; \ldots &amp; b_nC_{n, i} \end{bmatrix}</span>. As a result, <span class="math inline">\vec{x} = \frac{1}{\det A} \begin{bmatrix} \det A_1 \\ \vdots \\ \det A_n \end{bmatrix}</span>.</p>
<p><strong>Cramer's rule</strong> states that given <span class="math inline">A \in M_{n \times n}</span> such that <span class="math inline">A</span> is invertible, the solution to <span class="math inline">A\vec{x} = \vec{b}</span> is <span class="math inline">\vec{x}_i = \frac{\det A_i}{\det A}</span> for <span class="math inline">1 \le i \le n</span>.</p>
<p>This allows us to solve linear systems using determinants. However, it is not as practical as simply row reducing the matrices to obtain the values of <span class="math inline">\vec{x}</span>.</p>
<p>Cramer's rule explicitly gives the solution to a system of linear equations as a mathematical equation. This can be contrasted with row reducing, which does not have a formula.</p>
<h2 id="areavolumes">Area/Volumes</h2>
<p>Let <span class="math inline">\vec{u}, \vec{v} \in \mb{R}^2</span>. Geometrically, <span class="math inline">\vec{u} + \vec{v}</span> forms a parallelogram with <span class="math inline">\vec{u}</span> and <span class="math inline">\vec{v}</span> as the sides. We then say that <span class="math inline">\vec{u}</span> and <span class="math inline">\vec{v}</span> <strong>induce</strong> a parallelogram.</p>
<p>This parallelogram has points <span class="math inline">\vec{0}, \vec{u}, \vec{v}, \vec{u} + \vec{v}</span>.</p>
<p>Clearly, the area of the parallelogram is the length times the height. The length of the parallelogram can be set as <span class="math inline">\magn{\vec{u}}</span>, in which case the height would be <span class="math inline">\magn{\prp_{\vec{u}} \vec{v}}</span>, so the area would be <span class="math inline">\magn{\vec{u}} \magn{\prp_{\vec{u}} \vec{v}}</span>.</p>
<p>Using trigonometry, we find that <span class="math inline">\magn{\prp_{\vec{u}} \vec{v}} = \magn{\vec{v}} \sin \theta</span>, where <span class="math inline">\theta</span> is the angle between <span class="math inline">\vec{u}</span> and <span class="math inline">\vec{v}</span>.</p>
<p>So <span class="math inline">A = \magn{\vec{u}} \magn{\vec{v}} \sin \theta</span>.</p>
<p>Recall that <span class="math inline">\cos \theta = \frac{\vec{u} \cdot \vec{v}}{\magn{\vec{u}} \magn{\vec{v}}}</span>.</p>
<p>Clearly, <span class="math inline">A^2 = \magn{\vec{u}}^2 \magn{\vec{v}}^2 \sin^2 \theta = \magn{\vec{u}}^2 \magn{\vec{v}}^2 (1 - \cos^2 \theta) = (u_1^2 + u_2^2) (v_1^2 + v_2^2) - (\magn{\vec{u}} \magn{\vec{v}} \cos \theta)^2 = (u_1^2 + u_2^2) (v_1^2 + v_2^2) - (\magn{\vec{u}} \magn{\vec{v}} \cos \theta)^2 = u_1^2 v_1^2 + u_1^2 v_2^2 + u_2^2 v_1^2 + u_2^2 v_2^2 - (u_1 v_1 + u_2 v_2)^2 = u_1^2 v_1^2 + u_1^2 v_2^2 + u_2^2 v_1^2 + u_2^2 v_2^2 - (u_1^2 v_1^2 + 2 u_1 v_1 u_2 v_2 + u_2^2 v_2^2) = u_1^2 v_2^2 - 2 u_1 v_1 u_2 v_2 + u_2^2 v_1^2 = (u_1 v_2 - u_2 v_1)^2</span>.</p>
<p>Clearly, <span class="math inline">\det \begin{bmatrix} \vec{u} &amp; \vec{v} \end{bmatrix} = \det \begin{bmatrix} u_1 &amp; v_1 \\ u_2 &amp; v_2 \end{bmatrix} = u_1 v_2 - v_1 u_2</span>.</p>
<p>So <span class="math inline">A^2 = (\det \begin{bmatrix} \vec{u} &amp; \vec{v} \end{bmatrix})^2</span>. Since <span class="math inline">A</span> is always non-negative, <span class="math inline">A = \abs{\det \begin{bmatrix} \vec{u} &amp; \vec{v} \end{bmatrix}}</span>.</p>
<p>So in <span class="math inline">\mb{R}^2</span>, the area of the parallelogram is <span class="math inline">\abs{u_1 v_2 - v_1 u_2}</span>.</p>
<p>In <span class="math inline">\mb{R}^3</span>, three vectors <span class="math inline">\vec{u}, \vec{v}, \vec{w}</span> induce a <strong>parallelepiped</strong> - a sort of slanted cube.</p>
<p>Without loss of generality, assume <span class="math inline">\vec{u}, \vec{v}</span> form the base of the parallelepiped.</p>
<p>The volume of the parallelepiped is the area of the base times the height, or <span class="math inline">\magn{\vec{u} \times \vec{v}} \magn{\proj_{\vec{u} \times \vec{v}} \vec{w}}</span>. Recall that <span class="math inline">\vec{u} \times \vec{v}</span> is the cross product, and results in a vector perpendicular to <span class="math inline">\vec{u}</span> and <span class="math inline">\vec{v}</span> with the magnitude of the parallelogram they form.</p>
<p>Using a similar process as before, we find that <span class="math inline">V = \abs{\det \begin{bmatrix} \vec{u} &amp; \vec{v} &amp; \vec{w} \end{bmatrix}}</span>.</p>
<p>In fact, this works in any number of dimensions. A <strong>parallelotope</strong> is an <span class="math inline">n</span> dimensional solid induced by <span class="math inline">n</span> vectors <span class="math inline">\vec{v}_1, \ldots, \vec{v}_n \in \mb{R}^n</span>.</p>
<p>The <strong>n-volume</strong> of an <span class="math inline">n</span> dimensional solid is an extension to the concept of a volume to any number of dimensions. The n-volume of the parallelotope is <span class="math inline">V = \abs{\det \begin{bmatrix} \vec{v}_1 &amp; \ldots &amp; \vec{v}_n \end{bmatrix}}</span>.</p>
<h1 id="section-13">31/3/14</h1>
<h2 id="linear-mappings-and-bases">Linear Mappings and Bases</h2>
<p>Let <span class="math inline">L: \mb{R}^n \to \mb{R}^n</span> be a linear operator. Let <span class="math inline">\mathcal{B} = \set{\vec{v}_1, \ldots, \vec{v}_n}</span> be a basis for <span class="math inline">\mb{R}^n</span>. Let <span class="math inline">\vec{x} \in \mb{R}^n</span>.</p>
<p>Then <span class="math inline">\exists b_1, \ldots, b_n \in \mb{R}, \vec{x} = b_1 \vec{v}_1 + \ldots + b_n \vec{v}_n</span>. So <span class="math inline">\coord{\vec x}{\mathcal{B}} = \begin{bmatrix} b_1 \\ \vdots \\ b_n \end{bmatrix}</span>.</p>
<p>We want to determine a matrix <span class="math inline">A</span> such that <span class="math inline">[L(\vec{x})]_\mathcal{B} = [[L]\vec{x}]_\mathcal{B} = A\vec{x}_\mathcal{B}</span> - a linear operator that works the same as <span class="math inline">L</span>, but accepts and produces <span class="math inline">\mathcal{B}</span>-coordinates.</p>
<p>Clearly, <span class="math inline">[L(\vec{x})]_\mathcal{B} = [b_1 L(\vec{v}_1) + \ldots + b_n L(\vec{v}_n)]_\mathcal{B} = b_1 [L(\vec{v}_1)]_\mathcal{B} + \ldots + b_n [L(\vec{v}_n)]_\mathcal{B} = \begin{bmatrix} [L(\vec{v}_n)]_\mathcal{B} &amp; \ldots &amp; [L(\vec{v}_n)]_\mathcal{B} \end{bmatrix} \begin{bmatrix} b_1 \\ \vdots \\ b_n \end{bmatrix}</span>.</p>
<p>So <span class="math inline">[L(\vec{x})]_\mathcal{B} = [L]_\mathcal{B} \coord{\vec x}{\mathcal{B}}</span>.</p>
<p>In other words, the matrix of <span class="math inline">L</span> with respect to <span class="math inline">\mathcal{B}</span>, is <span class="math inline">[L]_\mathcal{B} = \begin{bmatrix} [L(\vec{v}_n)]_\mathcal{B} &amp; \ldots &amp; [L(\vec{v}_n)]_\mathcal{B} \end{bmatrix} \coord{\vec x}{\mathcal{B}}</span>. We call <span class="math inline">[L]_\mathcal{B}</span> the <span class="math inline">\mathcal{B}</span>-matrix of <span class="math inline">L</span>.</p>
<p>Given <span class="math inline">L(\vec{x}) = A\vec{x} = \begin{bmatrix} 1 &amp; 2 &amp; 2 \\ 2 &amp; 1 &amp; 2 \\ 2 &amp; 2 &amp; 1 \end{bmatrix} \vec{x}</span> and <span class="math inline">\mathcal{B} = \set{\begin{bmatrix} 1 \\ 1 \\ 1 \end{bmatrix}, \begin{bmatrix} 1 \\ -1 \\ 0 \end{bmatrix}, \begin{bmatrix} 1 \\ 0 \\ -1 \end{bmatrix}} = \set{\vec{v}_1, \vec{v}_2, \vec{v}_3}</span>, find <span class="math inline">[L]_\mathcal{B}</span>:</p>
<blockquote>
<p>Clearly, <span class="math inline">[L]_\mathcal{B} = \begin{bmatrix} [A\vec{v}_1]_\mathcal{B} &amp; [A\vec{v}_2]_\mathcal{B} &amp; [A\vec{v}_3]_\mathcal{B} \end{bmatrix}</span>.<br />
So <span class="math inline">[L]_\mathcal{B} = \begin{bmatrix} 5 &amp; 0 &amp; 0 \\ 0 &amp; -1 &amp; 0 \\ 0 &amp; 0 &amp; -1 \end{bmatrix}</span>.</p>
</blockquote>
<h3 id="diagonal-matrices">Diagonal Matrices</h3>
<p>A matrix <span class="math inline">D \in M_{n \times n}</span> is <strong>diagonal</strong> if and only if for all <span class="math inline">1 \le i \le n</span> and <span class="math inline">1 \le j \le n</span>, <span class="math inline">i \ne j \implies D_{i, j} = 0</span>.</p>
<p>In other words, a diagonal matrix is all zero, except possibly along the diagonal. For example, <span class="math inline">\begin{bmatrix} 1 &amp; 1 \\ 0 &amp; 1 \end{bmatrix}</span> is not diagonal, while <span class="math inline">\begin{bmatrix} 1 &amp; 0 \\ 0 &amp; 0 \end{bmatrix}</span> is.</p>
<p>A diagonal matrix is, by definition, both upper and lower triangular.</p>
<p>We can construct diagonal matrices using <span class="math inline">D = \diag(D_{1, 1}, \ldots, D_{n, n}) = \begin{bmatrix} D_{1, 1} &amp; 0 &amp; \ldots &amp; 0 \\ \vdots &amp; \vdots &amp; \vdots &amp; \vdots \\ 0 &amp; \ldots &amp; 0 &amp; D_{n, n} \end{bmatrix}</span>.</p>
<p>We often want to determine if there is a basis in which a matrix converted into that basis would be diagonal.</p>
<p>Let <span class="math inline">\mathcal{B} = \set{\vec{v}_1, \ldots, \vec{v}_n}</span>.</p>
<p>Also, <span class="math inline">\mathcal{S} = \set{\vec{e}_1, \ldots, \vec{e}_n}</span> is the standard basis for <span class="math inline">\mb{R}^n</span>.</p>
<p>Clearly, <span class="math inline">[L]_\mathcal{B} = {}_\mathcal{B}P_\mathcal{S} [L] {}_\mathcal{S}P_\mathcal{B}</span>. Basically, we convert it into standard coordinates, apply the linear operator, and then convert it back into <span class="math inline">\mathcal{B}</span> coordinates.</p>
<p>Also, <span class="math inline">{}_\mathcal{S}P_\mathcal{B} = \begin{bmatrix} \vec{v}_1, \ldots, \vec{v}_n \end{bmatrix}</span> and <span class="math inline">{}_\mathcal{B}P_\mathcal{S} = ({}_\mathcal{S}P_\mathcal{B})^{-1}</span>.</p>
<p>Given <span class="math inline">A \in M_{n \times n}</span>, the <strong>trace</strong> of <span class="math inline">A</span> is <span class="math inline">\operatorname{tr}(A) = \sum_{i = 1}^n A_{i, i}</span>. In other words, it is the sum of all the diagonal values.</p>
<p>The <span class="math inline">[L]_\mathcal{B}</span> notation is similar to the <span class="math inline">\coord{\vec v}{\mathcal{B}}</span>, except instead of a coordinate being put into another basis, it is a linear mapping converted to work in another basis.</p>
<h3 id="theorem-6.1.1">Theorem 6.1.1</h3>
<p>Given <span class="math inline">P, A, B \in M_{n \times n}</span> such that <span class="math inline">P^{-1}AP = B</span>, then <span class="math inline">\rank A = \rank B</span>, <span class="math inline">\det A = \det B</span>, and <span class="math inline">\operatorname{tr}(A) = \operatorname{tr}(B)</span>.</p>
<p>Also, if <span class="math inline">P</span> and <span class="math inline">P^{-1}</span> exist satisfying the above conditions, then <span class="math inline">A</span> and <span class="math inline">B</span> are <strong>similar</strong>. We can think of this as meaning that they represent the same linear operation, but in different bases.</p>
<h2 id="eigenvalueseigenvectors">Eigenvalues/Eigenvectors</h2>
<p>Pronounced &quot;Eye-gan values&quot; and &quot;Eye-gan vectors&quot;.</p>
<p>Let <span class="math inline">L</span> be a linear operator and <span class="math inline">\mathcal{B} = \set{\vec{v}_1, \ldots, \vec{v}_n}</span> be a basis for <span class="math inline">\mb{R}^n</span>.</p>
<p>We want to determine if <span class="math inline">\mathcal{B}</span> exists such that the matrix of <span class="math inline">L</span> with respect to <span class="math inline">\mathcal{B}</span> is diagonal.</p>
<p>Clearly, <span class="math inline">\mathcal{B}</span> exists if and only if <span class="math inline">[L]_\mathcal{B} = \diag(\lambda_1, \ldots, \lambda_n)</span> for some <span class="math inline">\lambda_1, \ldots, \lambda_n \in \mb{R}</span>.</p>
<p>Clearly, <span class="math inline">[L]_\mathcal{B} = {}_\mathcal{B}P_\mathcal{S} [L] {}_\mathcal{S}P_\mathcal{B}</span>.</p>
<p>So <span class="math inline">{}_\mathcal{B}P_\mathcal{S} [L] {}_\mathcal{S}P_\mathcal{B} = \diag(\lambda_1, \ldots, \lambda_n)</span> and <span class="math inline">[L] {}_\mathcal{S}P_\mathcal{B} = {}_\mathcal{S}P_\mathcal{B} \diag(\lambda_1, \ldots, \lambda_n)</span>.</p>
<p>Clearly, <span class="math inline">{}_\mathcal{S}P_\mathcal{B} = \begin{bmatrix} \vec{v}_1 &amp; \ldots &amp; \vec{v}_n \end{bmatrix}</span>. So <span class="math inline">\begin{bmatrix} [L]\vec{v}_1 &amp; \ldots &amp; [L]\vec{v}_n \end{bmatrix} = \begin{bmatrix} \lambda_1\vec{v}_1 &amp; \ldots &amp; \lambda_n\vec{v}_n \end{bmatrix}</span>.</p>
<p>So <span class="math inline">[L]\vec{v}_i = \lambda_i \vec{v}_i</span> for <span class="math inline">1 \le i \le n</span> if and only if <span class="math inline">\mathcal{B}</span> exists.</p>
<p><span class="math inline">\lambda_1, \ldots, \lambda_n</span> are <strong>eigenvalues</strong> of <span class="math inline">[L]</span>, and <span class="math inline">\vec{v}_1, \ldots, \vec{v}_n</span> are <strong>eigenvectors</strong> of <span class="math inline">[L]</span>.</p>
<p>This shows that the eigenvalues and eigenvectors are those values satisfying <span class="math inline">[L]_\mathcal{B} = \diag(\lambda_1, \ldots, \lambda_n)</span> and <span class="math inline">[L]\vec{v}_i = \lambda_i \vec{v}_i</span> where <span class="math inline">\mathcal{B} = \set{\vec{v}_1, \ldots, \vec{v}_n}</span>.</p>
<h3 id="definition-1">Definition</h3>
<p>Let <span class="math inline">A \in M_{n \times n}</span>. Let <span class="math inline">\vec{v} \ne \vec{0}</span>. Let <span class="math inline">\lambda \in \mb{R}</span>. Then <span class="math inline">\lambda</span> is an <strong>eigenvalue</strong> of <span class="math inline">A</span> and <span class="math inline">\vec{v}</span> is an <strong>eigenvector</strong> of <span class="math inline">A</span> if and only if <span class="math inline">A\vec{v} = \lambda \vec{v}</span>.</p>
<p>In other words, the eigenvector can be multiplied by <span class="math inline">A</span> to give a scalar multiple of itself, and the eigenvalue is the scalar that it is multiplied by.</p>
<p>Also, the eigenvector and eigenvalue of a linear operator is the eigenvector and eigenvalue of the standard matrix of the linear operator.</p>
<p>Every eigenvalue is associated with eigenvectors. An eigenvalue can have a whole set of associated eigenvectors.</p>
<p>Find the eigenvectors and eigenvalues of <span class="math inline">L(\vec{x}) = \proj_{\vec{a}} \vec{x}</span>:</p>
<blockquote>
<p>Let <span class="math inline">\lambda \in \mb{R}</span>. Clearly, <span class="math inline">L(\vec{x}) = \lambda \vec{x}</span> if <span class="math inline">\vec{x}</span> lies along <span class="math inline">\vec{a}</span> - <span class="math inline">\vec{x}</span> is already a scalar multiple of <span class="math inline">\vec{a}</span>.<br />
So the eigenvectors of <span class="math inline">L(\vec{x})</span> include <span class="math inline">k\vec{a}, k \ne 0</span> with the eigenvalue <span class="math inline">\lambda = 1</span>.<br />
Clearly, <span class="math inline">L(\vec{x}) = \lambda \vec{x}</span> if <span class="math inline">\vec{x}</span> is perpendicular to <span class="math inline">\vec{a}</span> - the projection is <span class="math inline">\vec{0}</span>.<br />
So the eigenvectors of <span class="math inline">L(\vec{x})</span> include <span class="math inline">k\vec{a}, k \ne 0</span> with tje eigenvalue <span class="math inline">\lambda = 0</span>.<br />
Note that there are infinite combinations of eigenvalues and eigenvectors.</p>
</blockquote>
<p>Determine if <span class="math inline">\vec{v} = \begin{bmatrix} 1 \\ 1 \\ 1 \end{bmatrix}</span> is an eigenvector of <span class="math inline">A = \begin{bmatrix} 3 &amp; 6 &amp; 7 \\ 3 &amp; 3 &amp; 7 \\ 5 &amp; 6 &amp; 5 \end{bmatrix}</span> and if <span class="math inline">\lambda = 1</span> is an eigenvalue:</p>
<blockquote>
<p>CLearly, <span class="math inline">A\vec{v} = \begin{bmatrix} 16 \\ 13 \\ 16 \end{bmatrix}</span>.<br />
Clearly, <span class="math inline">\begin{bmatrix} 16 \\ 13 \\ 16 \end{bmatrix}</span> is not a scalar multiple of <span class="math inline">\vec{v}</span>.<br />
So <span class="math inline">\vec{v}</span> is not an eigenvector of <span class="math inline">A</span>.<br />
We need to see if there is a non-zero vector such that <span class="math inline">A\vec{v} = \vec{v}</span>.<br />
So <span class="math inline">\begin{bmatrix} 3v_1 + 6v_2 + 7v_3 \\ 3v_1 + 3v_2 + 7v_3 \\ 5v_1 + 6v_2 + 5v_3 \end{bmatrix} = \begin{bmatrix} v_1 \\ v_2 \\ v_3 \end{bmatrix}</span>.<br />
Solving, we find that <span class="math inline">\vec{v} = \vec{0}</span> is the only solution.<br />
Since eigenvectors must be non-zero, there are no eigenvectors such that <span class="math inline">\lambda = 1</span> is the corresponding eigenvalue.<br />
So <span class="math inline">\lambda = 1</span> is not an eigenvalue.</p>
</blockquote>
<h3 id="calculation">Calculation</h3>
<p>Whenever we are finding eigenvalues, we are solving <span class="math inline">A\vec{v} = \lambda \vec{v}</span> such that <span class="math inline">\vec{v} \ne 0</span>.</p>
<p>So <span class="math inline">A\vec{v} - \lambda I \vec{v} = \vec{0}</span> and <span class="math inline">(A - \lambda I)\vec{v} = \vec{0}</span>.</p>
<p>Clearly, <span class="math inline">\vec{v} = \vec{0}</span> is always a solution for this linear system. However, we are only interested in the nontrivial solutions - we want more than one solution.</p>
<p>So we want <span class="math inline">\rank A &lt; n</span>, and by theorem 5.3.8, <span class="math inline">\det (A - \lambda I) = 0</span>.</p>
<p>So <span class="math inline">\lambda</span> is an eigenvalue if and only if <span class="math inline">\det (A - \lambda I) = 0</span>.</p>
<p>Given <span class="math inline">A = \begin{bmatrix} 2 &amp; -12 \\ 1 &amp; -5 \end{bmatrix}</span>, find the eigenvalues and eigenvectors:</p>
<blockquote>
<p>Assume <span class="math inline">(A - \lambda I)\vec{v} = \vec{0}</span> and <span class="math inline">\det (A - \lambda I) = 0</span>.<br />
Clearly, <span class="math inline">\det (A - \lambda I) = (\lambda + 2)(\lambda + 1)</span>. So <span class="math inline">\lambda = -2, -1</span>. We can also write this as <span class="math inline">\lambda_1 = -2, \lambda_2 = -1</span>.<br />
Assume <span class="math inline">\lambda = -2</span>. Then <span class="math inline">(A - \lambda I)\vec{v} = \begin{bmatrix} 4 &amp; -12 \\ 1 &amp; -3 \end{bmatrix}\vec{v} = \vec{0}</span>.<br />
Solving, we get <span class="math inline">\vec{v} = k\begin{bmatrix} 3 \\ 1 \end{bmatrix}, k \ne 0</span>.<br />
So the eigenvalue <span class="math inline">\lambda = -2</span> is associated with the eigenvector <span class="math inline">\vec{v} = k\begin{bmatrix} 3 \\ 1 \end{bmatrix}, k \ne 0</span>.<br />
Assume <span class="math inline">\lambda = -1</span>. Then <span class="math inline">(A - \lambda I)\vec{v} = \begin{bmatrix} 3 &amp; -12 \\ 1 &amp; -4 \end{bmatrix}\vec{v} = \vec{0}</span>.<br />
Solving, we get <span class="math inline">\vec{v} = k\begin{bmatrix} 4 \\ 1 \end{bmatrix}, k \ne 0</span>.<br />
So the eigenvalue <span class="math inline">\lambda = -1</span> is associated with the eigenvector <span class="math inline">\vec{v} = k\begin{bmatrix} 4 \\ 1 \end{bmatrix}, k \ne 0</span>.</p>
</blockquote>
<h3 id="related-concepts">Related Concepts</h3>
<p>Note that when we calculated <span class="math inline">\det (A - \lambda I)</span>, we always got an <span class="math inline">n</span>th degree polynomial in terms of <span class="math inline">\lambda</span> - a polynomial of degree equal to the number of rows and columns.</p>
<p>We call this the <strong>characteristic polynomial</strong> of <span class="math inline">A</span>. It is defined as <span class="math inline">C(\lambda) = \det (A - \lambda I)</span>.</p>
<p>Clearly, <span class="math inline">\lambda</span> is an eigenvalue if and only if <span class="math inline">C(\lambda) = 0</span>, since it is the solution to <span class="math inline">\det (A - \lambda I) = 0</span>.</p>
<p>The <strong>algebraic multiplicity</strong> <span class="math inline">a_\lambda</span> of an eigenvalue <span class="math inline">\lambda</span> is the number of times <span class="math inline">\lambda</span> appears as a root of <span class="math inline">C(\lambda)</span> - it is the degree of the root <span class="math inline">\lambda</span>.</p>
<p>Note that the eigenvectors <span class="math inline">\vec{v}</span> associated with a given eigenvalue are the solutions to <span class="math inline">(A - \lambda I)\vec{v} = \vec{0}</span> (rearranged from <span class="math inline">A\vec{v} = \lambda \vec{v}</span>). Therefore, <span class="math inline">\vec{v} \in \operatorname{Null}(A - \lambda I)</span>.</p>
<p>In other words, the eigenvector is one of the elements of the nullspace of <span class="math inline">A - \lambda I</span>, excluding <span class="math inline">\vec{0}</span>.</p>
<p>The <strong>eigenspace</strong> of the eigenvalue <span class="math inline">\lambda</span> is <span class="math inline">E_\lambda = \operatorname{Null}(A - \lambda I)</span>, or the space containing the eigenvectors.</p>
<p>The <strong>geometric multiplicity</strong> of an eigenvalue <span class="math inline">\lambda</span> is the dimension of its eigenspace, and is defined as <span class="math inline">g_\lambda = \dim E_\lambda</span>.</p>
<h3 id="theorem-6.2.3">Theorem 6.2.3</h3>
<p>For any eigenvalue <span class="math inline">\lambda</span>, <span class="math inline">1 \le g_\lambda \le a_\lambda</span>.</p>
<p>In other words, the geometric muliplicity is always less than or equal to the algebraic multpicity, and all multiplicities are positive integers.</p>
<p>Any eigenvalue <span class="math inline">\lambda</span> where <span class="math inline">g_\lambda &lt; a_\lambda</span> is known as <strong>deficient</strong>.</p>
<h3 id="theorem-6.2.4">Theorem 6.2.4</h3>
<p>Given eigenvalues <span class="math inline">\lambda_1, \ldots, \lambda_n</span> of <span class="math inline">A \in M_{n \times n}</span>, <span class="math inline">\det A = \lambda_1 \cdots \lambda_n</span> and <span class="math inline">\operatorname{tr}(A) = \lambda_1 + \ldots + \lambda_n = \sum_{i = 1}^n \lambda_i</span>.</p>
<p>In other words, the determinant of a matrix is the product of all its eigenvalues and the trace is the sum of all the eigenvalues.</p>
<p>This is because the eigenvalues form the elements of a diagonal matrix, so the determinant is the product of the diagonal elements, and the trace is the sum of them.</p>
<h1 id="section-14">6/4/14</h1>
<h2 id="diagonalization">Diagonalization</h2>
<p>A matrix <span class="math inline">A \in M_{n \times n}</span> is <strong>diagonalizable</strong> if and only if there exists <span class="math inline">P</span> such that <span class="math inline">P^{-1}AP = D</span> is diagonal. If <span class="math inline">P</span> exists, then <span class="math inline">P</span> <strong>diagonalizes</strong> <span class="math inline">A</span>.</p>
<p>Basically, <span class="math inline">A</span> is diagonalizable if it is similar to a diagonal matrix.</p>
<p>Let <span class="math inline">\vec{v}_1, \ldots, \vec{v}_n</span> be eigenvectors of <span class="math inline">A</span> and <span class="math inline">\lambda_1, \ldots, \lambda_n</span> be the corresponding eigenvalues of <span class="math inline">A</span>.</p>
<p>We showed earlier that if <span class="math inline">P^{-1}AP = \diag(x_1, \ldots, x_n)</span>, then <span class="math inline">x_1, \ldots, x_n = \lambda_1, \ldots, \lambda_n</span> and a possible value of <span class="math inline">P</span> is <span class="math inline">P = \begin{bmatrix} \vec{v}_1, \ldots, \vec{v}_n \end{bmatrix}</span>.</p>
<h3 id="theorem-6.3.1">Theorem 6.3.1</h3>
<p>Given distinct eigenvalues of <span class="math inline">A</span>, <span class="math inline">\lambda_1, \ldots, \lambda_k</span>, <span class="math inline">\set{\vec{v}_1, \ldots, \vec{v}_k}</span> is linearly independent.</p>
<p>This can be proven via induction. Clearly, <span class="math inline">\set{\vec{v}_1}</span> is linearly independent since <span class="math inline">\vec{v}_1 \ne \vec{0}</span>, by definition.</p>
<p>Assume that <span class="math inline">\set{\vec{v}_1, \ldots, \vec{v}_k}</span> is linearly independent for some <span class="math inline">k \ge 1</span>. Then <span class="math inline">c_1 \vec{v}_1 + \ldots + c_k\vec{v}_k = \vec{0}</span> has only the trivial solution.</p>
<p>Clearly, <span class="math inline">\set{\vec{v}_1, \ldots, \vec{v}_k, \vec{v}_{k + 1}}</span> is linearly independent if <span class="math inline">c_1 \vec{v}_1 + \ldots + c_k\vec{v}_k + c_{k + 1}\vec{v}_{k + 1} = \vec{0}</span> only has a trivial solution.</p>
<p>Since for all <span class="math inline">1 \le i \le k, 1 \le j \le k</span>, <span class="math inline">A\vec{v}_i = \lambda_i\vec{v}_i</span>, <span class="math inline">(A - \lambda_i)\vec{v}_j = A\vec{v}_j - \lambda_i\vec{v}_j = \lambda_j\vec{v}_j - \lambda_i\vec{v}_j = (\lambda_j - \lambda_i)\vec{v}_j</span>.</p>
<p>Assume <span class="math inline">c_1 \vec{v}_1 + \ldots + c_k\vec{v}_k + c_{k + 1}\vec{v}_{k + 1} = \vec{0}</span>.</p>
<p>Clearly, <span class="math inline">(A - \lambda_{k + 1})\vec{v}_j \ne 0</span>, so <span class="math inline">c_1(A - \lambda_{k + 1})\vec{v}_1 + \ldots + c_k(A - \lambda_{k + 1})\vec{v}_k + c_{k + 1}(A - \lambda_{k + 1})\vec{v}_{k + 1} = \vec{0} = c_1(\lambda_1 - \lambda_{k + 1})\vec{v}_1 + \ldots + c_k(\lambda_k - \lambda_{k + 1})\vec{v}_k + c_{k + 1}(\lambda_{k + 1} - \lambda_{k + 1})\vec{v}_{k + 1} = c_1(\lambda_1 - \lambda_{k + 1})\vec{v}_1 + \ldots + c_k(\lambda_k - \lambda_{k + 1})\vec{v}_k</span>.</p>
<p>Clearly, this only has the trivial solution. So by the principle of simple induction, <span class="math inline">\set{\vec{v}_1, \ldots, \vec{v}_k}</span> is linearly independent for all <span class="math inline">k \ge 1</span>.</p>
<h3 id="theorem-6.3.2">Theorem 6.3.2</h3>
<p>Given bases for the eigenspace of <span class="math inline">\lambda_i</span>, <span class="math inline">\mathcal{B}_i = \set{\vec{v}_{i, 1}, \ldots, \vec{v}_{i, g_{\lambda_i}}}</span>, <span class="math inline">\mathcal{B}_1 \cup \ldots \cup \mathcal{B}_k</span> is a linearly independent set.</p>
<p>In other words, the set of all the basis vectors for the eigenspaces of each eigenvalue is linearly independent.</p>
<p>The sum of all algebraic multiplicities of all the eigenvalues must be <span class="math inline">n</span>, since that is the degree of the characteristic polynomial.</p>
<h3 id="theorem-6.3.3-diagonalization-theorem">Theorem 6.3.3 (Diagonalization Theorem)</h3>
<p><span class="math inline">A</span> is diagonalizable if and only if <span class="math inline">g_{\lambda_i} = a_{\lambda_i}</span> for all <span class="math inline">1 \le i \le k</span>.</p>
<p>In other words, a matrix is diagonalizable if and only if the geometric multiplicity is the same as the algebraic multiplicity - if none of its eigenvalues are deficient.</p>
<p>In order to get <span class="math inline">n</span> linearly independent eigenvectors, the geometric multiplicity of each eigenvalue must be the same as the algebraic multiplicity.</p>
<p>Also, if we have <span class="math inline">n</span> linearly independent eigenvectors, the set of them is a basis for <span class="math inline">\mb{R}^n</span>.</p>
<p>Since the multiplicities must be at least 1, and they must add up to <span class="math inline">n</span>, if <span class="math inline">A</span> has <span class="math inline">n</span> eigenvalues, then each one has an algebraic and geometric multiplicity of 1 and is diagonalizable.</p>
<p>An application of this theorem can be used when <span class="math inline">A</span> is a matrix of real values. In this case, <span class="math inline">A</span> is diagonalizable if and only if all the eigenvalues are real and <span class="math inline">g_{\lambda_i} = a_{\lambda_i}</span>. This is useful because any matrix with complex eigenvalues can immediately be known not be diagonalizable.</p>
<p>A matrix can be <strong>factorized</strong> via diagonalization. If <span class="math inline">P^{-1}AP = D</span> is diagonal, then <span class="math inline">A = PDP^{-1}</span>.</p>
<h3 id="algorithm">Algorithm</h3>
<p>To <strong>diagonalize</strong> a matrix <span class="math inline">A \in M_{n \times n}</span> over a set <span class="math inline">\mb{S}</span> means to convert it into the form <span class="math inline">P^{-1}AP = D</span> for some <span class="math inline">P \in M_{n \times n}(\mb{S})</span> where <span class="math inline">D</span> is diagonal.</p>
<p>For example, diagonalizing a matrix over <span class="math inline">\mb{R}</span> means to find <span class="math inline">P \in M_{n \times n}</span> such that <span class="math inline">D</span> is diagonal.</p>
<p>To diagonalize a matrix <span class="math inline">A</span>:</p>
<ol type="1">
<li>Find the characteristic polynomial <span class="math inline">C(\lambda) = \det (A - \lambda I)</span> and factor it.</li>
<li>Solve for <span class="math inline">\lambda</span> where <span class="math inline">C(\lambda) = 0</span> - find the distinct roots of the c haracteristic polynomial to find the eigenvalues <span class="math inline">\lambda_1, \ldots, \lambda_n</span>, with duplicates allowed.</li>
<li>If any of the eigenvalues are not real, then <span class="math inline">A</span> is not diagonalizable over <span class="math inline">\mb{R}</span>.</li>
<li>Find a basis for the eigenspace of each <span class="math inline">\lambda_i</span> by solving <span class="math inline">(A - \lambda_i I)\vec{v} = \vec{0}</span> for <span class="math inline">\vec{v}</span> and finding the general solution.</li>
<li>These bases allow us to find the geometric multiplicities <span class="math inline">g_{\lambda_i}</span>.</li>
<li>If <span class="math inline">g_{\lambda_i} &lt; a_{\lambda_i}</span> for any <span class="math inline">1 \le i \le n</span>, then <span class="math inline">A</span> is not diagonalizable.</li>
<li>Combine all the basis vectors together to get a set of <span class="math inline">n</span> linearly, independent vectors <span class="math inline">\mathcal{B} = \set{\vec{v}_1, \ldots, \vec{v}_n}</span>, which is automatically a basis for <span class="math inline">\mb{R}^n</span>.</li>
<li>Let <span class="math inline">P = \begin{bmatrix} \vec{v}_1 &amp; \ldots &amp; \vec{v}_n \end{bmatrix}</span>. Then <span class="math inline">P^{-1}AP = A_\mathcal{B} = \diag(\lambda_1, \ldots, \lambda_n)</span>.</li>
</ol>
<h2 id="matrix-exponentiation">Matrix Exponentiation</h2>
<p>We define matrix exponentiation with <span class="math inline">A^k = AA^{k - 1} = A^{k - 1}A, k \in \mb{N}</span> and <span class="math inline">A^0 = I</span>. However, how do we calculate larger powers more easily?</p>
<p>Obviously, the answer is diagonalization, somehow. Don't ask me how I know.</p>
<h3 id="theorem-6.4.1">Theorem 6.4.1</h3>
<p>For <span class="math inline">k \ge 0</span>, <span class="math inline">\diag(d_1, \ldots, d_n)^k = \diag(d_1^k, \ldots, d_n^k)</span>. In other words, we simply exponentiate the individual elements in the matrix.</p>
<p>Proof:</p>
<blockquote>
<p>Clearly, <span class="math inline">\diag(d_1, \ldots, d_n)^0 = I = \diag(d_1^0, \ldots, d_n^0)</span>.<br />
Assume <span class="math inline">\diag(d_1, \ldots, d_n)^k = \diag(d_1^k, \ldots, d_n^k)</span> for some <span class="math inline">k \ge 0</span>.<br />
Then <span class="math inline">\diag(d_1, \ldots, d_n)^{k + 1} = \diag(d_1, \ldots, d_n)^k \diag(d_1, \ldots, d_n)^k = \diag(d_1, \ldots, d_n) \diag(d_1^k, \ldots, d_n^k) = \diag(d_1^{k + 1}, \ldots, d_n^{k + 1})</span>.<br />
So by induction, <span class="math inline">\diag(d_1, \ldots, d_n)^k = \diag(d_1^k, \ldots, d_n^k)</span>.</p>
</blockquote>
<h3 id="theorem-6.4.2">Theorem 6.4.2</h3>
<p>If <span class="math inline">P^{-1}AP = B</span>, then <span class="math inline">A^k = PB^kP^{-1}</span>. In other words, we can change the basis to whatever we want and still exponentiate.</p>
<p>Proof:</p>
<blockquote>
<p>Clearly, <span class="math inline">A^0 = I = PP^{-1} = PB^0P^{-1}</span>.<br />
Assume for some <span class="math inline">k \ge 0</span> that <span class="math inline">A^k = PB^kP^{-1}</span>. Then <span class="math inline">A^{k + 1} = AA^k = PBP^{-1}PB^kP^{-1} = PBB^kP^{-1} = PB^{k + 1}P^{-1}</span>.<br />
So by induction, <span class="math inline">A^k = PB^kP^{-1}</span>.</p>
</blockquote>
<p>This is useful if <span class="math inline">B</span> is diagonal, because combined with theorem 6.4.1, we can easily exponentiate matrices to large powers.</p>
<p>However, if we're too lazy to diagonalize, then it might just be faster to use the old multiply and square algorithm.</p>
<p>Given <span class="math inline">A = \begin{bmatrix} 1 &amp; 2 \\ -1 &amp; 4 \end{bmatrix}</span>, find <span class="math inline">A^{1000}</span>:</p>
<blockquote>
<p>Clearly, <span class="math inline">C(\lambda) = \det \begin{bmatrix} 1 - \lambda &amp; 2 \\ -1 &amp; 4 - \lambda \end{bmatrix} = (\lambda - 2)(\lambda - 3)</span>.<br />
So the eigenvalues are 2 and 3 with algebraic multiplicities 1.<br />
Assume <span class="math inline">(A - 2I)\vec{v} = 0</span>. Then row reducing, we get <span class="math inline">\begin{array}{cc|c} 1 &amp; -2 &amp; 0 \\ 0 &amp; 0 &amp; 0 \end{array}</span> and <span class="math inline">\set{\begin{bmatrix} 2 \\ 1 \end{bmatrix}}</span> is a basis for the eigenspace of <span class="math inline">\lambda = 2</span>.<br />
Assume <span class="math inline">(A - 3I)\vec{v} = 0</span>. Then row reducing, we get <span class="math inline">\begin{array}{cc|c} 1 &amp; -1 &amp; 0 \\ 0 &amp; 0 &amp; 0 \end{array}</span> and <span class="math inline">\set{\begin{bmatrix} 1 \\ 1 \end{bmatrix}}</span> is a basis for the eigenspace of <span class="math inline">\lambda = 3</span>.<br />
So <span class="math inline">P = \begin{bmatrix} 2 &amp; 1 \\ 1 &amp; 1 \end{bmatrix}</span> and <span class="math inline">P^{-1}AP = D</span> where <span class="math inline">D</span> is diagonal, and <span class="math inline">D = \begin{bmatrix} 2 &amp; 0 \\ 0 &amp; 3 \end{bmatrix}</span>.<br />
Clearly, <span class="math inline">P^{-1} = \begin{bmatrix} 1 &amp; -1 \\ -1 &amp; 2 \end{bmatrix}</span>.<br />
Clearly, <span class="math inline">A^{1000} = \begin{bmatrix} 2 &amp; 1 \\ 1 &amp; 1 \end{bmatrix}\diag(2, 3)^{1000}\begin{bmatrix} 1 &amp; -1 \\ -1 &amp; 2 \end{bmatrix} = \begin{bmatrix} 2^{1001} - 3^{1000} &amp; -2^{1001} + 2 \cdot 3^{1000} \\ 2^{1000} - 3^{1000} &amp; -2^{1000} + 2 \cdot 3^{1000} \end{bmatrix}</span>.</p>
</blockquote>
<div class="license">
  <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-nc-sa/4.0/80x15.png" /></a> This work by <a xmlns:cc="http://creativecommons.org/ns#" href="https://uberi.github.io/" property="cc:attributionName" rel="cc:attributionURL">Anthony Zhang</a> is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>.
  Copyright 2013-2017 Anthony Zhang.
</div>
</body>
</html>
