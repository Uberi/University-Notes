<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <title>CO487 | Anthony Zhang</title>
  <link rel="stylesheet" href="../css/base.css" type="text/css">
  <link rel="stylesheet" href="../css/note.css" type="text/css">
  <link rel="stylesheet" href="../highlight/styles/default.css">
  <link rel="stylesheet" href="../highlight/styles/paraiso-light.css">
  <script src="../highlight/highlight.pack.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>
  <style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
  </style>
  <script src="../katex/katex.min.js" type="text/javascript"></script>
  <link rel="stylesheet" href="../katex/katex.min.css" />
  <script type="text/javascript">
  window.onload = function() {
    document.getElementsByClassName("status-banner")[0].style.display = "block";
    setTimeout(function() {
      renderMathElements(document.getElementsByClassName("math"));
      document.getElementsByClassName("status-banner")[0].style.display = "none";
    }, 50); // delay to allow status banner to show
  }

  function renderMathElements(mathElements) {
    var mathOptions = {
      macros: {
        "\\set": "\\left\\{ #1 \\right\\}",
        "\\tup": "\\left\\langle #1 \\right\\rangle",
        "\\abs": "\\left\\lvert #1 \\right\\rvert",
        "\\floor": "\\left\\lfloor #1 \\right\\rfloor",
        "\\ceil": "\\left\\lceil#1 \\right\\rceil",
        "\\mb": "\\mathbb{#1}",
        "\\rem": "\\operatorname{rem}",
        "\\ord": "\\operatorname{ord}",
        "\\sign": "\\operatorname{sign}",
        "\\imag": "\\bm{i}",
        "\\dee": "\\mathop{}\\!\\mathrm{d}",
        "\\lH": "\\overset{\\text{l'H}}{=}",
        "\\evalat": "\\left.\\left(#1\\right)\\right|",
        "\\sech": "\\operatorname{sech}",
        "\\spn": "\\operatorname{Span}",
        "\\proj": "\\operatorname{proj}",
        "\\prp": "\\operatorname{perp}",
        "\\refl": "\\operatorname{refl}",
        "\\magn": "\\left\\lVert #1 \\right\\rVert",
        "\\rank": "\\operatorname{rank}",
        "\\sys": "\\left[ #1 \\mid #2\\space \\right]",
        "\\range": "\\operatorname{Range}",
        "\\adj": "\\operatorname{adj}",
        "\\cof": "\\operatorname{cof}",
        "\\coord": "{\\left\\lbrack #1 \\right\\rbrack}_{#2}",
        "\\diag": "\\operatorname{diag}",
        "\\formlp": "\\operatorname{Form}(\\mathcal{L}^P)",

        // not yet available in KaTeX
        "\\operatorname": "\\mathop{\\text{#1}}\\nolimits", //wip: spacing is slightly off
        "\\not": "\\rlap{\\kern{7.5mu}/}", //wip: slash angle is slightly off
        "\\bm": "\\mathbf", //wip: should be italic, but isn't
      },
      throwOnError: false,
    };
    for (var i=0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      katex.render(texText.data, mathElements[i], mathOptions);
    }
  }
  </script>
</head>
<body>
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-68271407-1', 'auto');
    ga('send', 'pageview');

  </script>
  <h1>Lecture Notes by <a href="/">Anthony Zhang</a>.</h1>
  <ul class="site_links">
    <li><a href="/blog/" class="page">blog</a></li>
    <span class="divider"></span>
    <li><a href="http://uberi.github.io/University-Notes" class="page">notes</a></li>
    <span class="divider"></span>
    <li><a href="/resume.pdf" class="page">résumé</a></li>
    <span class="divider"></span>
    <li><a href="https://github.com/Uberi" class="contact">github</a></li>
    <span class="divider"></span>
    <li><a href="https://www.linkedin.com/in/uberi/" class="contact">linkedin</a></li>
    <span class="divider"></span>
    <li><a href="mailto:me@anthonyz.ca" class="contact">email</a></li>
    <span class="divider"></span>
    <li><a href="https://www.facebook.com/anthony.zhang.user" class="contact">facebook</a></li>
    <span class="divider"></span>
    <li><a href="https://twitter.com/anthony926535" class="contact">twitter</a></li>
    <span class="divider"></span>
    <li><a href="https://keybase.io/uberi" class="info">public key</a></li>
  </ul>
<h1 id="co487">CO487</h1>
<p>Applied cryptography.</p>
<pre><code>Alfred Menezes
Section 001
Email: ajmeneze@uwaterloo.ca
Office hours: Mondays 3:00pm-5:00pm, Fridays 1:00pm-3:00pm in MC 5026
Mondays/Wednesdays/Fridays 11:30pm-12:20pm</code></pre>
<h1 id="section">4/3/16</h1>
<p>All course resources are on LEARN. Course has 5 assignments (summing up to 20% of final grade), midterm (worth 30%), and a final exam (worth 50%). Midterm is March 8, 2017 at 7-9pm.</p>
<p>Cryptography is a tool we can use to secure communications when there are malicious adversaries. Some of the fundamental goals of cyptography are:</p>
<ul>
<li>Confidentiality - data is secret to all people except authorized parties.</li>
<li>Data Integrity - data is unaltered.</li>
<li>Origin authentication - data can be confirmed to be from a particular source.</li>
<li>Non-repudiation - once a statement is published, it is not possible to deny/revoke it later.</li>
</ul>
<p>Different applications might require different subsets of these goals.</p>
<p>In this course, we use Bob and Alice as placeholders to represent two parties trying to communicate securely. We use Eve or Mallory as a placeholder to represent one or more adversaries trying to attack the communications (by injecting data, eavesdropping, or other approaches, depending on the threat model)</p>
<p>Some famous early practical uses of cryptography was the enigma machine, which used multiple spinning rotors to encrypt/decrypt messages. In that cryptosystem, the German military leadership and the German U-boats had the role of Bob and Alice, trying to communicate confidentially, and Alan Turing and his team had the role of Eve, trying to break the confidentiality guarantees and read those communications.</p>
<p>However, cryptosystems like Enigma and its successor, Lorenz, are a far cry from modern cryptosystems, which have a much more mathematically sound foundation. Modern cryptography is what makes online banking, online shopping, and cellular networks possible.</p>
<p>For example, SSL, the protocol that makes online security possible, ensures origin authentication and confidentiality between a user and a website. SSL uses symmetric-key encryption to implement confidentiality, and a MAC scheme (HMAC) to implement origin authentication. Of course, to make this possible they both need to have the same secret key for the symmetric-key cryptography, and for the MAC. To confidentially and authentically share this secret key, we use public-key encryption. Of course, to make this possible we need a way to obtain authentic copies of the public keys of both parties - this is implemented using digital signatures, where trusted third parties known as certificate authorities keep track of public keys that are considered authentic (generally by vetting the site owner in the real world), and digitally sign them to confirm that they're authentic. The certificate authorities' public keys are pre-installed in the browser, which acts as a root of trust - if you trust that certificate authority's public key is authentic, and that the certificate authorities are correctly keeping track of websites' authentic public keys, then you can also trust that the website's presented public key is also authentic.</p>
<p>SSL is one of the most successful cryptosystems ever deployed, used for tons of sites on the internet. However, there are a number of possible weaknesses that might result in the guarantees being broken:</p>
<ul>
<li>The crypotgraphy itself might be weak (e.g., previously, SSL supportied using RC4 for the symmetric-key encryption, which is nowadays easy to break).</li>
<li>Quantum computers can attack public-key encryption systems like RSA.</li>
<li>Random number generators used in the protocol might have weaknesses (e.g., Netscape's flawed RNG, NSA's backdoored EC-DRBG CSPRNG).</li>
<li>Certificate authorities (e.g., social engineering resulting in Verisign digitally signing non-authentic certificates).</li>
<li>Bugs in crpytographic code (e.g., Heartbleed)</li>
<li>Misuse of cryptography, like encrypting the wrong thing.</li>
<li>Phishing/social engineering attacks on users themselves.</li>
<li>The server itself leaking data - SSL only protects data in transit, not when it's on the server.</li>
</ul>
<p>This demonstrates some useful concepts:</p>
<ul>
<li>Symmetric-key encryption is used to ensure confidentiality.</li>
<li>MAC schemes are used to ensure authentication.</li>
<li>Public-key encryption is used to ensure confidentiality, integrity, authentication, and non-repudiation.</li>
<li>Digital signatures is used to ensure integrity, authentication, and non-repudiation.</li>
</ul>
<p>Cryptography is only one part of a large information security ecosystem, including other things like secure operating systems, auditing mechanisms, trusted computing, and risk analysis. In this context, cryptography provides a lot of useful and essential tools, but it's not all there is to security. Attackers will generally target the weakest part of the system, and if that link fails, it is possible that the entire system will.</p>
<p>This course focuses on breadth over depth. For depth, take CO 485 and try out the readings for this course as well.</p>
<h1 id="section-1">6/1/17</h1>
<p>SSL in more detail:</p>
<ol type="1">
<li>Client makes request to the website (not the full URL, just the host).</li>
<li>Server responds with the website's certificate, which contains the website identification info (like host and etc.).</li>
</ol>
<h2 id="symmetric-key-encryption">Symmetric-key encryption</h2>
<p>A <strong>symmetric key encryption scheme</strong> (SKES) is a definition containing:</p>
<ul>
<li>A plaintext space <span class="math inline">M</span>.</li>
<li>A ciphertext space <span class="math inline">C</span>.</li>
<li>A key space <span class="math inline">K</span>.</li>
<li>A family of encryption functions <span class="math inline">E_k: M \to C, \forall k \in K</span>.</li>
<li>A family of decryption functions <span class="math inline">D_k: C \to M, \forall k \in K</span>.</li>
</ul>
<p>To use one of these to implement message confidentiality, Alice and Bob agree on a particular <span class="math inline">k \in K</span> over a secure channel (so that nobody else knows about the value of <span class="math inline">k</span>). Then, Alice can compute <span class="math inline">c = E_k(m)</span>, where <span class="math inline">m</span> is the message, and then sends <span class="math inline">c</span> to Bob over an unsecure channel. Bob can read the message by then computing <span class="math inline">m = D_k(c)</span>, while anyone without <span class="math inline">k</span> would have to figure out the correct <span class="math inline">D_k</span> function to use. An SKES can be designed such that finding the correct <span class="math inline">D_k</span> would be very difficult.</p>
<p>A substitution cipher is an SKES, under this definition: <span class="math inline">M</span> is the set of all English messages, <span class="math inline">C</span> is the set of all encrypted messages, <span class="math inline">K</span> is the set of all permutations of the English alphabet, <span class="math inline">E_k</span> maps letters onto <span class="math inline">k</span> by index, and <span class="math inline">D_k</span> does the inverse mapping.</p>
<p>A <strong>security model</strong> defines what the adversay is capable of, like what they can do to the communicating parties. For example, there are passive attacks like <strong>ciphertext attacks</strong> (attacker can obtain ciphertext, like if they're listening on the same network), <strong>known-plaintext attacks</strong> (attacker knows some of the plaintext as well the resulting ciphertext, like knowing that someone always signs off their message with their name). There are also active attacks, like <strong>chosen-plaintext attacks</strong> (attacker can choose some part of the plaintext), <strong>clandestine attacks</strong> (attacker is willing to use bribery/blackmail/etc.), and <strong>side-channel attacks</strong> (power analysis, RF emissions analysis, timing attacks).</p>
<p>The security model also includes the attacker's computational abilities. For example, information-theoretic security models assume the attacker has infinite computational resources, complexity-theoretic security models assume the attacker has a turing machine capable of computing any polynomial-time algorithm efficiently, and computational-theoretic security models assume the attacker simply has a lot of computers available.</p>
<p>The attacker's goals, in decreasing order of priority: recovering the secret key, recovering plaintext from ciphertext (without the key), or learn some characteristics of the plaintext given ciphertext (besides message length).</p>
<p>An SKES is <strong>secure</strong> if and only if it is resistant to a chosen-plaintext attack by a computationally bounded adversary (computational-theoretic security). A secure SKES is necessary but not sufficient to guarantee confidentiality.</p>
<p>Additionally, we also assume that the adversaries know everything about the cryptosystem except the plaintext and the key, including the all the algorithms and communicatoin mechanisms.</p>
<p>An SKES should ideally be efficient (for encryption/decryption), have small keys (but not so small that it makes brute force attacks possible), and be secure (even against the designer of the SKES).</p>
<p>A <strong>work factor</strong> measures how hard a task is, in terms of how many computations are needed to complete it: <span class="math inline">2^{56}</span> operations is easy, <span class="math inline">2^{64}</span> is feasible, <span class="math inline">2^{80}</span> operations is barely feasible, and <span class="math inline">2^{128}</span> operations is infeasible. This will change as our computers get faster. For example, the entire bitcoin network is doing <span class="math inline">2^{61}</span> hashes per second. However, we use <span class="math inline">2^{128}</span> operations as infeasible because the Landauer limit tells us doing that many operations on a classical computer would take a significant fraction of the world's power output for a year.</p>
<p>For example, a substitution cipher is not secure, because it's trivially broken by a chosen-plaintext attack. By choosing the alphabet as the message, the ciphertext is just the secret key. In fact, it's not even secure against ciphertext-only messages. Although it's infeasible to exhaustively try every permutation of the alphabet for valid-looking decrypted messages, we can simply use letter frequency analysis to try likely candidates first.</p>
<p>A polyalphabetic cipher uses multiple alphabet permutations for substitution, choosing between them with a particular algorithm. For example, a Vigenere cipher has a word with non-repeated letters added to the message's letters mod 26. This is nice because as the message grows longer, the letter frequency graph is a bit flatter. We can break that by using a chosen-plaintext attack with plaintext &quot;AAAAAAAAAAAAAAAAAA&quot;, so if &quot;A&quot; corresponds to the numerical value 0, the ciphertext is just the key. Therefore, the Vigenere cipher is not secure.</p>
<p>From now on, we assume plaintext, keys, and ciphertext are all binary strings. What happens when we apply the concept of a Vignere cipher, but with a uniformly random key that's as long as the entire message?</p>
<p>A <strong>one time pad</strong> XORs a random key with the plaintext (the key is as long as the message). However, it's important to not reuse keys, because <span class="math inline">c_1 = m_1 \oplus k_1</span> and <span class="math inline">c_2 = m_2 \oplus k_2</span> implies that <span class="math inline">c_1 \oplus c_2 = m_1 \oplus m_2</span>, which leaks a lot of information about the plaintext. If the key is uniformly randomly selected, every key is equally likely, so every ciphertext is also equally likely. This is information-theoretically secure - a one-time pad provably cannot be broken by a ciphertext-only attack even by attackers with infinite computational resources. While one-time pads have these nice properties, in practice they are hard to use because the key has to be as long as the ciphertext, making sharing keys a pain.</p>
<p>A <strong>stream cipher</strong> is like a one-time pad, but instead of a truly random key, we use a pseudorandom bit generator, where the PRBG's seed is the secret key. This is no longer perfectly secure because it depends on the quality of the randomness of the PRBG, but it's often a practical compromise since the key can be a lot shorter. Like with a one-time pad, we also shouldn't re-use keys, since that would give us some of the values of the PRBG's output by XORing the ciphertexts, which would make it easier to learn the PRBG's state. One example of a stream cipher is RC4.</p>
<h1 id="section-2">9/1/17</h1>
<p>The PRBG should satisfy two requirements:</p>
<ul>
<li>Indistinguishability requirement - the output should be indistinguishable from a random sequence.</li>
<li>Unpredictability requirement - the output should not be predictable even if some of the previous outputs are known.</li>
</ul>
<p>Most random number generators built into programming languages are not cryptographically secure - they're only intended to satisfy the indistinguishability requirement. For example, <code>rand</code> in UNIX uses a linear congruential generator with known constants, which is straightforward to predict.</p>
<p>RC4 was designed by Rivest, the R in RSA. It was used in everything from SSL/TLS to Adobe Acrobat, as one of the most popular stream ciphers around. It's nice because it's very fast and has variable key sizes, but was proprietary for a long time has a lot of weaknesses, even though none of them are catastrophic. It consists of a key scheduling algorithm, and a keystream generator.</p>
<p>The key scheduling algorithm generates a random-looking permutation of <span class="math inline">0, \ldots, 255</span>. It starts by initializing <span class="math inline">S</span> with <span class="math inline">0, \ldots, 255</span>, and <span class="math inline">\overline{K}</span> to the key repeated over and over until it fills 256 entries in the array. Then, the array entries in <span class="math inline">S</span> are swapped based on <span class="math inline">j_{i + 1} = (S[i] + \overline{K} + j_i) \mod 256</span>. The keystream generator just applies that permutation to generate the keystream, which is then XORed with the plaintext to get the ciphertext.</p>
<p>Wireless networks lose the physical security of wired networks, and security for those networks is a lot harder because attackers can do it from a distance with no physical evidence. The original WiFi standard, IEEE 802.11, includes the Wireless Equivalent Privacy (WEP) protocol for protecting link-level data in transit between clients and access points. WEP was intended to provide confidentiality using RC4, data integrity using a checksum, and access control by rejecting improperly encrypted packets.</p>
<p>In WEP, the client first shares a 40-bit or 104-bit key with the access point (this was because the US classified cryptography as munitions, disallowing most cryptography with keys greater than 40 bits for export; and keys greater than 104 bits for domestic use). Messages are then divided into fixed-size packets, which are then each encrypted with a per-packet initialization vector (IV). The issue is that WEP didn't specify certain things, like how the key should be distributed, and how IVs should be managed.</p>
<p>Implementations ended up using one shared key per LAN, infrequently changed, and just generating random IVs or using consecutive integers as IVs. To send a packet, a party would select a 24-bit IV, compute the CRC-32 checksum of the message, the checksum is then appended to the plaintext, and then XORed with the RC4 keystream to get the ciphertext, where the key is the IV the shared WEP key appended to the IV. The sender then transmits the IV and the ciphertext. The receiver then, having the IV and the WEP shared key, gets gets the plaintext concatenated with the checksum, and then verifies that the checksum is correct, rejecting the packet if the checksum doesn't validate.</p>
<p>Turns out, WEP implements none of its goals. One problem is IV collision - since the IV is only 24-bits, the birthday paradox means that only about <span class="math inline">2^{12}</span> packets are needed for a collision. If two packets have the same IV, then <span class="math inline">c_1 \oplus c_2 = m_1 \oplus m_2</span> - the ciphertexts can be XORed to get the plaintexts, which can then be analyzed using known plaintexts or statistical analysis, so confidentiality is broken. Another problem is that the checksum is linear - we can make certain changes to the ciphertext while still ensuring that the checksum will verify correctly, so data integrity is broken. Finally, the checksum is unkeyed, so knowing the plaintext for one encrypted packet is enough to get the RC4 keystream and encrypt messages properly themselves.</p>
<p>Shamir and co. (the S in RSA) came up with an even better attack in 2001, based on the fact that 104-bit keys were infrequently changed, the IV is very predictable/randomly selected, and we know the first byte of the plaintext (the protocols add known headers to the plaintext before encrypting). With 5 million packets, the secret key itself can be recovered. Modern techniques can recover the key in just 40000 packets.</p>
<p>IEEE published updated wireless security standards. For example, WPA was a temporary replacement, and WPA2 came out afterwards, using AES instead of RC4, and designed to be much stronger.</p>
<h1 id="section-3">11/1/17</h1>
<p>Assignment 1 should be started now.</p>
<p>Attacks only get better over time, never worse. In WEPs case, it wasn't the security of the algorithms like RC4 that were broken, it was the use of those algorithms that was incorrect. In the future, thanks to upcoming advances related to Moore's law and quantum computing, currently secure cryptosystems could easily become broken.</p>
<p>A <strong>block cipher</strong> is the other common type of SKES. While a stream cipher encrypts one bit at a time, a block cipher breaks up the plaintext into fixed-length blocks, encrypting the message one block at a time. Some commonly known block ciphers are DES and AES.</p>
<p>DES has a 56-bit key length (which is now relatively easy to break), and a block size of 64 bits. AES (Rijndael, &quot;rined'all&quot;) is its successor, and has a 128/192/256 bit key length. There are currently no significant known attacks on AES.</p>
<p>Clause Shannon gave a couple of principles for designing good block ciphers:</p>
<ul>
<li>Diffusion - each ciphertext bit should depend as many plaintext bits as possible.</li>
<li>Confusion - there should be a complicated relatoinship between key and ciphertext bit.</li>
<li>Key size - keys should be large enough to prevent exhaustive search.</li>
</ul>
<p>Also, block ciphers should be fast, so we can use them in more applications.</p>
<p>A Feistel cipher is a class of ciphers, including DES. Feistel ciphers are parameterized based on <span class="math inline">n</span> (half of block size), <span class="math inline">h</span> (number of rounds), and <span class="math inline">l</span> (key size). It generates <span class="math inline">h</span> subkeys from the cipher key, one for each round. Each key is used to generate a component function <span class="math inline">f_1, \ldots, f_h</span> that &quot;scrambles&quot; its input.</p>
<p>Each block <span class="math inline">m</span> is then broken into two halves, <span class="math inline">\tup{m_0, m_1}</span>. Then, we perform <span class="math inline">h</span> encryption rounds: at each round <span class="math inline">i</span>, the right half gets moved into the left half, and the original left half is XORed with <span class="math inline">f_i</span> applied to the original right half, and that becomes the right half. After those <span class="math inline">h</span> rounds, the two halves are the ciphertext.</p>
<h1 id="section-4">13/1/17</h1>
<p>;wip: feistel ciphers</p>
<p>The New Data Seal cipher was invented by IBM as the predecessor to DES. It's a Feistel cipher, and is a relatively complicated one, but it also happens to be completely broken today to chosen-plaintext attacks. It has a block size of 64 bits, and uses 16 rounds, so <span class="math inline">n = 64, h = 16</span>. Here's the basic idea:</p>
<ol type="1">
<li>Let <span class="math inline">S_k: \set{0, 1}^8 \to \set{0, 1}^8</span> be the secret key - an arbitrary function of a byte.</li>
<li>Let <span class="math inline">f: \set{0, 1}^{64} \to \set{0, 1}^{64}</span>, the component function, be defined as follows:
<ol type="1">
<li>Since the input is 64-bits, let <span class="math inline">z = \tup{z^1, \ldots, z^8}</span> be the 8 bytes of the input.</li>
<li>For each byte <span class="math inline">z^j</span>, let <span class="math inline">n_1^j = z^j[7:4]</span> and <span class="math inline">n_2^j = z^j[3:0]</span> - the two nibbles of the byte.</li>
<li>Let <span class="math inline">t = S_k(z^1[7] \ldots z^8[7])</span> - the key function applied to the byte obtained by taking the first bit of each byte in <span class="math inline">z</span>.</li>
<li>For each byte <span class="math inline">z_j</span>, if bit <span class="math inline">j</span> of <span class="math inline">t</span> is 1, let <span class="math inline">p_1^j = S_1(n_2^j)</span> and <span class="math inline">p_2^j = S_0(n_1^j)</span>, otherwise let <span class="math inline">p_1^j = S_0(n_1^j)</span> and <span class="math inline">p_2^j = S_1(n_2^j)</span>. <span class="math inline">S_0, S_1</span> are functions defined by the NDS specification (that are too long to list here), and we're swapping the nibbles if the corresponding bit of the key function <span class="math inline">S_k</span> is true.</li>
<li>Output <span class="math inline">P(p_1^1 p_2^1 \ldots p_1^8 p_2^8)</span> - a scrambling function <span class="math inline">P</span> applied to all of the scrambled and permuted nibbles concatenated together. <span class="math inline">P</span> is also a function defined by the NDS specification (that is too long to list here).</li>
</ol></li>
<li>For each 16-byte block of the input <span class="math inline">z = \tup{z^1, \ldots, z^8}</span> (each block contains <span class="math inline">2n</span> bits), run the Feistel cipher ladder with <span class="math inline">f</span> as the component function:
<ol type="1">
<li>Let <span class="math inline">m_0, m_1</span> be the two 8-byte halves of the 16-byte block.</li>
<li>For 16 rounds <span class="math inline">1 \le j \le 16</span>, set <span class="math inline">m_j</span> to <span class="math inline">m_{j - 1}</span> and <span class="math inline">m_{j + 1}</span> to <span class="math inline">m_{j - 1} \oplus f(m_j)</span> (simultaneously).</li>
<li>Output <span class="math inline">m_{16} m_{17}</span>.</li>
</ol></li>
</ol>
<p>Since <span class="math inline">S_k</span> can be considered a random function, there are <span class="math inline">2^8</span> possible values of <span class="math inline">S_k(x)</span> for each of the <span class="math inline">2^8</span> possible values of <span class="math inline">x</span> - we might think of serializing the function as a 256-byte sequence. That means there are <span class="math inline">256^{256}</span> possible values of <span class="math inline">S_k</span>, or <span class="math inline">2^{2048}</span>, which makes trying every possible <span class="math inline">S_k</span> computationally infeasible.</p>
<p>In fact, we can recover the entire secret key from the ciphertext in a few hundred chosen plaintext attacks. The main issue is that there's no subkeys - each round used the same component function and secret key, rather than deriving unique component functions for each round - each <span class="math inline">f_1, \ldots, f_h</span> is the same function! Let <span class="math inline">T</span> denote the function that does one round of encryption, with regard to the secret key <span class="math inline">S_k</span>. Basically, <span class="math inline">T(\tup{m_{i - 1}, m_i}) = \tup{m_i, m_{i - 1} \oplus f(m_i)}</span> for some fixed function <span class="math inline">f</span> and the two halves of the message are <span class="math inline">m_0, m_1</span>.</p>
<p>Clearly, applying <span class="math inline">T(m)</span> to the message 16 times is the whole encryption function. Let's represent this as <span class="math inline">T^{16}(m)</span>. Let <span class="math inline">F = T^{16}</span> - this is the full encryption function, since the cipher is just applying <span class="math inline">T</span> 16 times. Clearly, <span class="math inline">T(F(m)) = T(T^{16}(m)) = T^{17}(m) = T^{16}(T(m)) = F(T(m))</span> for any byte <span class="math inline">m</span>.</p>
<p>Here's the attack. For every possible byte <span class="math inline">r</span>, we want to determine <span class="math inline">S_k(r)</span>. Select <span class="math inline">u = \tup{m_0, m_1}</span> such that the byte formed by taking the first bit of each byte in <span class="math inline">m_1</span> is <span class="math inline">r</span>, and <span class="math inline">p_1^j \ne p_2^j</span> for <span class="math inline">1 \le j \le 8</span> - the scrambled nibbles aren't the same within each of the 8 bytes in <span class="math inline">m_1</span>.</p>
<p>As an aside, <span class="math inline">x^*</span> seems to mean &quot;the first bit of each byte in <span class="math inline">x</span>&quot;.</p>
<p>By the rules of the chosen plaintext attack, we can get Bob to give us <span class="math inline">\tup{a, b} = F(u)</span>, but not what <span class="math inline">F(m)</span> or <span class="math inline">T(m)</span> are (we know the values of the function at this point, but that's it).</p>
<p>However, we know that <span class="math inline">T(F(u)) = F(T(u)) = \tup{b, \cdot}</span>. Since the value of <span class="math inline">S_k(r)</span> is only a byte, we can guess every possible value of <span class="math inline">S_k(r)</span>, and check each guess <span class="math inline">t</span> by computing <span class="math inline">T_t(u)</span> and then get Bob to give us <span class="math inline">F(T_t(u)) = \tup{c, d}</span>. Clearly, <span class="math inline">b \ne c</span> implies that <span class="math inline">S_k(r) \ne t</span>, and <span class="math inline">b = c</span> implies that <span class="math inline">S_k(r) = t</span> is very very likely (there is a tiny chance of accidentally getting it right). This is because we're assuming that <span class="math inline">F</span> works roughly like a random permutation, so since <span class="math inline">b</span> and <span class="math inline">c</span> are 64 bits, <span class="math inline">F(T_t(u))</span> has only a <span class="math inline">\frac 1 {2^{64}}</span> probability of <span class="math inline">b = c</span> without <span class="math inline">S_k(r) = t</span>, because we selected <span class="math inline">u</span> such that <span class="math inline">p_1^j \ne p_2^j</span>. Eventually, we get every possible value of <span class="math inline">S_k</span>.</p>
<p>Also, since we check every possible byte <span class="math inline">r</span> (256 values), and for each value of <span class="math inline">r</span>, we guess-and-check possible bytes <span class="math inline">t</span> (on average, 128 checks, worst case 256), we should expect <span class="math inline">128 \times 256 = 2^{15}</span> chosen plaintexts on average before recovering the entirety of <span class="math inline">S_k</span>.</p>
<p>The entire attack can be summarized as:</p>
<ol type="1">
<li>For each possible byte <span class="math inline">r</span>:
<ol type="1">
<li>Find a byte <span class="math inline">u = \tup{m_0, m_1}</span> such that <span class="math inline">m_1^* = r</span> and <span class="math inline">p_1^j \ne p_2^j, 1 \le j \le 8</span>.</li>
<li>Get Bob to compute <span class="math inline">F(u) = \tup{a, b}</span>.</li>
<li>For each possible byte <span class="math inline">t</span>:
<ol type="1">
<li>Compute <span class="math inline">T_t(u)</span>.</li>
<li>Get Bob to compute <span class="math inline">F(T_t(u)) = \tup{c, d}</span>.</li>
<li>If <span class="math inline">b = c</span>, we know that <span class="math inline">S_k(r) = t</span> with overwhelming probability, and then go to the next <span class="math inline">r</span>.</li>
</ol></li>
</ol></li>
<li>Now we have <span class="math inline">S_k(r)</span>, the secret key. We can use this to decrypt the ciphertext.</li>
</ol>
<p>Basically, we need about <span class="math inline">2^{15}</span> chosen plaintexts to recover the entire secret key, which is quite feasible on today's computers.</p>
<h1 id="section-5">16/1/17</h1>
<p>Chosen plaintext attacks are not always feasible, but when they are they're very powerful. For example, consider a mail forwarder that encrypts incoming emails and forwards them to another destination.</p>
<p>DES is one of NDS' successors. Originally with 64-bit keys, the NSA weakened the final standard to 56 bits. DES is pretty weak compared to modern ciphers, but 3-DES is still commonly in use. The design principles are classified, but it's been heavily analyzed.</p>
<p>DES is a Feistel cipher with 16 rounds, a 64-bit block and a 56-bit key. For each round of the cipher, a 48-bit subkey is derived from the 56-bit secret key, by selecting 48 bits from the secret key. The plaintext halves are also expanded from 32-bits to 48-bits. After each Feistel round completes, we use 8 S-box functions (which are publicly known) to scramble the XORed result and reduce 48-bit results to 32-bits. This is the source of nonlinearity in DES, and is very important for security.</p>
<p>With a key space of size <span class="math inline">2^{56}</span>, it's perfectly practical to just brute force today, even if we can't do it on our laptops just yet. A massively parallel effort broke a DES-encrypted message in just over 22 hours in 1999, in response to a challenge set by RSA security. Also, it's got a block size of 64-bits, which means that you'd expect a collision at around <span class="math inline">2^{32}</span> uniformly distributed blocks, thanks to the birthday paradox. So, on a busy network, if Alice and Bob are sending many blocks back and forth, Eve might observe a duplicated block, which tells us a little bit of information about the plaintext - those two corresponding blocks in the plaintext are identical (this isn't much of a problem in practice, but is still an issue).</p>
<p>Differential cryptoanalysis attacks can be used to recover the key in <span class="math inline">2^{47}</span> chosen plaintexts, though this is usually an infeasibly large number of chosen plaintexts in practice (turns out, DES was specifically designed to guard against this - the NSA was aware of the attack before it was public knowledge). Linear cryptoanalysis attacks can do the same thing in just <span class="math inline">2^{43}</span> chosen plaintexts, which is slightly more practical.</p>
<p>After these weakenings, 3-DES was invented as a more secure SKES in order to take advantage of existing DES hardware (DES was designed for hardware acceleration). Basically, it's just DES applied to DES applies to DES applied to the plaintext. Note that applying ciphers repeatedly doesn't always provide more security (consider a substitution cipher, for example), but in this case it provides reasonably good. Another variant is Double-DES, which is DES applied to DES applied to the plaintext (each DES function has its own key, so we have a 112-bit key overall).</p>
<p>Turns out Double-DES is vulnerable to a meet-in-the-middle attack. Basically, if <span class="math inline">E_{k_1}, E_{k_2}</span> are the two DES encryption functions in Double-DES and <span class="math inline">c = E_{k_2}(E_{k_1}(m))</span>, then <span class="math inline">E^{-1}(c) = E_{k_1}(m)</span>.</p>
<p>Suppose we have 3 known plaintext/ciphertext pairs <span class="math inline">\tup{m_1, c_1}, \ldots, \tup{m_3, c_3}</span>. For each possible 56-bit <span class="math inline">k_2</span>, decrypting <span class="math inline">c_1</span> with our guess for <span class="math inline">k_2</span>, and store that plaintext and the key used in a table, indexed by the plaintext. Then, for each possible 56-bit <span class="math inline">k_1</span>, try encrypting the <span class="math inline">m_1</span> with our guess for <span class="math inline">k_1</span>, and see if it matches any entry in the table. If there's a match, and those guesses for <span class="math inline">k_1, k_2</span> also satisfy <span class="math inline">c_2 = E_{k_2}(E_{k_1}(m_2))</span> and <span class="math inline">c_3 = E_{k_2}(E_{k_1}(m_3))</span>, then our guesses for <span class="math inline">k_1, k_2</span> are the actual secret keys.</p>
<p>Due to this attack, Double-DES is only slightly better than plain DES, with about <span class="math inline">2^{57}</span> bits of security and requiring about <span class="math inline">2^{56}(64 + 56)</span> bits of memory to store the table. The main hard part is getting the ~1 exabyte of storage needed to mount this attack, but it's quite within the realm of possibility. We can even do a time-memory tradeoff and do <span class="math inline">2^{56 + s}</span> DES operations in return for <span class="math inline">2^{56 - s}</span> bits of memory, which can easily bring the memory requirements down to a manageable size while keeping the number of operations feasible.</p>
<h1 id="section-6">18/1/17</h1>
<p>Why do we need 3 pairs of known plaintext/ciphertext pairs rather than just 1 or 2? Well, for each key in the key space, we can think of the encryption function is just a random function (it's not, but this is a good enough assumption for our purposes).</p>
<p>Let <span class="math inline">k</span> be the actual <span class="math inline">l</span>-bit secret key (so <span class="math inline">E_k(m) = c</span>) and <span class="math inline">k&#39;</span> (also <span class="math inline">l</span> bits) be our guess for this key. If <span class="math inline">E_{k&#39;}</span> is a uniformly random function (from our assumption), then it clearly has a <span class="math inline">\frac 1 {2^L}</span> probability of satisfying <span class="math inline">E_{k&#39;}(m) = c</span>, where <span class="math inline">L</span> is the number of bits in the plaintext - it might just randomly happen to output the right ciphertext given our particular plaintext. What's the probability that our guess is correct if it satisfies <span class="math inline">E_{k&#39;}(m) = c</span>?</p>
<p>Suppose <span class="math inline">E_{k&#39;}(m) = c</span> and <span class="math inline">k&#39; \ne k</span> (our guess for the key is incorrect, but gives the right ciphertext for our particular plaintext). Then the number of <span class="math inline">E_{k&#39;}</span> such that <span class="math inline">E_{k&#39;}(m) = c</span> is <span class="math inline">\frac{2^l - 1}{2^{L}}</span> (<span class="math inline">2^l - 1</span> is the number of keys in the keyspace that are not the actual key, and <span class="math inline">2^{L}</span> is the number of possible plaintexts). This is essentially the number of incorrect keys that work for a single plaintext/ciphertext pair without actually being the correct key.</p>
<p>We want to use enough plaintext/ciphertext pairs such that the expected number of these incorrect keys is close to 0. Clearly, with <span class="math inline">t</span> of these plaintext/ciphertext pairs, we have <span class="math inline">\frac{2^l - 1}{2^{Lt}}</span> expected false keys. For Double-DES, one pair gives us <span class="math inline">\frac{2^{112} - 1}{2^{64}}</span> expected incorrect keys, which is far too many. With 3 pairs, we get around <span class="math inline">\frac 1 {2^{16}}</span> expected false keys, much more reasonable.</p>
<p>There's also triple DES, which is just applying DES three times now - <span class="math inline">c = E_{k_1}(E_{k_2}(E_{k_3}(m)))</span>, where <span class="math inline">c</span> is the resulting ciphertext, <span class="math inline">m</span> is the plaintext, and <span class="math inline">k_1, k_2, k_3</span> are the three 56-bit DES keys. We don't have any proof that it's more secure than DES alone, but a meet-in-the-middle attack takes around <span class="math inline">2^{112}</span> steps, around <span class="math inline">2^{64}</span> message/plaintext pairs, and a huge table.</p>
<p>Given our plaintext <span class="math inline">m = m_1 \ldots m_t</span>, how do we encrypt it if the Block cipher modes of operation:</p>
<ul>
<li>Electronic Codebook (ECB) - input is split into blocks, and then each block is encrypted with the key using the block cipher. In other words, for each plaintext block <span class="math inline">i</span>, <span class="math inline">c_i = E_k(m_i)</span>.
<ul>
<li>This is very bad, never use it - identical plaintext blocks result in identical ciphertext blocks under the same key, so chosen-plaintext attacks can tell you whether a given plaintext block is equal to an attacker-chosen plaintext block.</li>
</ul></li>
<li>Cipher Block Chaining (CBC) - select a random <span class="math inline">L</span>-bit initialization vector (where <span class="math inline">L</span> is the block size) as <span class="math inline">c_0</span>. Then, for each plaintext block <span class="math inline">i</span>, let <span class="math inline">c_i = E_k(m_i \oplus c_{i - 1})</span>.
<ul>
<li>This is actually proven to be semantically secure against chosen plaintext attacks, assuming that the block cipher itself is semantically secure.</li>
</ul></li>
</ul>
<p>AES was developed as part of a public, open competition in 1997 to build a successor to DES. It needed to have 3 key sizes (128, 192, and 256 bits), have a 128-bit block size, and be efficiently implementable on hardware/software. While 128 bits is infeasible for the foreseeable future, the larger key sizes are intended to protect against potential future attacks, as well as quantum computers - Grover's algorithm can perform exhaustive key search in only <span class="math inline">2^{\frac 1 2 l}</span> operations, where <span class="math inline">l</span> is the number of bits in the key.</p>
<p>With 15 submissions initially, Rijndael won in the end. Rijndael became AES in late 2001.</p>
<p>Rijndael is an iterated block cipher, not a Feistel cipher. Specifically, a substitution-permutation network. Currently, there's no known attack better than exhaustive key search on AES.</p>
<p>We won't cover the technical details of AES in this course, but some more details are available in the slides</p>
<h1 id="section-7">20/1/17</h1>
<p><span class="math inline">x \in_R S</span> means that <span class="math inline">x</span> is randomly and independently chosen from <span class="math inline">S</span>.</p>
<p>A hash function is a fixed function that transforms an input of arbitrary length and output a fixed-length string, called a <strong>digest</strong>. Some common hash functions are MD5, SHA1, and SHA512.</p>
<p>Formally, an <span class="math inline">n</span>-bit hash function is <span class="math inline">H: \set{0, 1}^* \to \set{0, 1}^n</span>. A good hash function should satisfy three examples:</p>
<ul>
<li><strong>Pre-image resistance</strong> - given a randomly selected hash value <span class="math inline">y \in \set{0, 1}^n</span>, it is computationally infeasible to find <span class="math inline">x</span> such that <span class="math inline">H(x) = y</span>.
<ul>
<li>In other words, it's hard to invert the hash function.</li>
<li>Consider a service that stores accounts as user IDs and hashes of passwords. If the account data was leaked and the hash function didn't have pre-image resistance, attackers could obtain the passwords from the password hashes.</li>
<li>First pre-image resistance generally implies second pre-image resistance in practice, but not in theory. If we can do a first pre-image attack, then we could mount a second pre-image attack by hashing the given document and repeatedly running the first-preimage attack on that hash until we get a document that isn't equal to the original.</li>
</ul></li>
<li><strong>Second pre-image resistance</strong> - given a random value <span class="math inline">x \in \set{0, 1}^*</span>, it is computationally infeasible to find a <span class="math inline">x&#39; \ne x</span> such that <span class="math inline">H(x&#39;) = H(x)</span>.
<ul>
<li>In other words, it's hard to find a collision for a given input in the hash function with non-negligible probability.</li>
<li>Consider a software publisher that has users verify their updates using a certain hash. If the hash function doesn't have second pre-image resistance, attackers can construct a different update that still passes verification.</li>
</ul></li>
<li><strong>Collision resistance</strong> - it is computationally infeasible to find <span class="math inline">x, x&#39; \in \set{0, 1}^*</span> such that <span class="math inline">x \ne x&#39;</span> and <span class="math inline">H(x) = H(x&#39;)</span> (a <strong>collision</strong> is such a pair <span class="math inline">\tup{x, x&#39;}</span>).
<ul>
<li>In other words, it's hard to find any collisions in the hash function.</li>
<li>Note that hash functions must necessarily 0, have collisions, by pidgeonhole principle and the fact that the output space is finite while the input space is infinite. However, it should be computationally feasible to find even a single one.</li>
<li>Consider the common practice of digital signature schemes signing the hash of the message rather than the message itself (due to overhead of signing long messages using things like RSA). If a hash wasn't collision resistant, the signer could find a collision <span class="math inline">x, x&#39;</span>$, sign <span class="math inline">x</span>, and later on claim to have signed <span class="math inline">x&#39;</span> instead. That means Alice might</li>
<li>Collision resistance implies second pre-image resistance (this is easily proved via the contrapositive).</li>
<li>Second pre-image resistance doesn't imply collision resistance, however (this is easily proved by constructing a hash function with only 1 collision that is computationally feasible to find: the probability that we actually get that colliding input is negligible for a second pre-image attack, but the collision is definitely there, so we have a second pre-image resistant hash function that isn't collision-resistant).</li>
<li>Collision resistance also doesn't imply pre-image resistance. Let <span class="math inline">H</span> be an <span class="math inline">n - 1</span>-bit pre-image and collision resistant hash function. Let <span class="math inline">\overline H = \begin{cases} 1 \| x &amp;\text{if } x \text{ has } n - 1 \text{ bits} \\ 0 \| H(x) &amp;\text{otherwise} \end{cases}</span>. Clearly, <span class="math inline">H&#39;</span> is still collision-resistant, but any random hash value that begins with a 1 (so 50% of all hash values) has a trivially computable pre-image (just take off the 1 at the beginning). Therefore, <span class="math inline">H</span> is a function that is collision resistant yet not pre-image resistant.</li>
<li>That said, if the hash function is somewhat uniform (so hash values all tend to have roughly equal numbers of pre-images), collision resistance does guarantee preimage resistance. We'll prove this by contradiction. Suppose we have a hash function <span class="math inline">H</span> that is collision resistant and somewhat uniform, but not pre-image resistant. Let <span class="math inline">x</span> be an arbitrary input, and find a pre-image of <span class="math inline">H(x)</span>. Since <span class="math inline">H(x)</span> has roughly the same number of pre-images as for any other hash value, it should have a lot of pre-images. Plus, since it's not pre-image resistant, we can find another <span class="math inline">x&#39;</span> such that <span class="math inline">H(x&#39;) = H(x)</span>. However, <span class="math inline">x&#39;</span> and <span class="math inline">x</span> are now a collision, which we found efficiently, so <span class="math inline">H</span> isn't collision resistant - contradiction! Therefore <span class="math inline">H</span> must be pre-image resistant as well.</li>
<li>Therefore, collision resistance is the hardest property to achieve in practice, which makes sense, since the hash function must have infinite collisions.</li>
</ul></li>
</ul>
<p>A <strong>Davies-Meyer hash function</strong> is a family of hash functions that uses block ciphers. Basically, given a fixed IV <span class="math inline">H_0</span> and an <span class="math inline">n</span>-bit block encryption function <span class="math inline">E_k</span>, we break up the plaintext into <span class="math inline">n</span>-bit blocks <span class="math inline">x_1, \ldots, x_t</span> (after appending 1 to the end and padding with 0 to the nearest block size), then <span class="math inline">H_i = E_{x_i}(H_{i - 1}) \oplus H_{i - 1}</span> and <span class="math inline">H_t</span> is the hash value.</p>
<p>A <strong>one-way function</strong> is a pre-image resistant hash function. A <strong>cryptographic hash function</strong> is a pre-image resistant and collision-resistant hash function.</p>
<p>A <strong>generic attack</strong> is an attack on a hash function that treats it as a black-box/ideal hash function. As a result, a generic attack must treat the hash function as a random function <span class="math inline">H: \set{0, 1}^* \to \set{0, 1}^n</span> (an ideal hash function is a random function). In practice, actual random functions are too complex to represent in a reasonable way.</p>
<p>A generic attack for finding pre-images: select arbitrary <span class="math inline">x \in \set{0, 1}^*</span> until we find <span class="math inline">H(x) = y</span>. Clearly, we expect about <span class="math inline">2^n</span> time. A generic attack for finding collisions: select arbitrary <span class="math inline">x \in \set{0, 1}^*</span> and store them in a table indexed by <span class="math inline">H(x)</span>, until a hash is found. By birthday paradox, we expect that to take about <span class="math inline">\sqrt{2^n}</span> time and memory.</p>
<h1 id="section-8">23/1/17</h1>
<p>Checking for collisions generically, as mentioned last class, would take around <span class="math inline">\sqrt{2^n}</span> time and memory. However, there's actually a generic attack for finding collisions that has the same order of running time (<span class="math inline">\sqrt{\frac{\pi 2^n}{2}}</span>), but requires far less memory.</p>
<p>Basically, given <span class="math inline">H: \set{0, 1}^* \to \set{0, 1}^n</span>, we want to find <span class="math inline">x_1, x_2 \in \set{0, 1}^*</span>, where <span class="math inline">x_1 \ne x_2</span> yet <span class="math inline">H(x_1) = H(x_2)</span>. How does this algorithm, known as the Van Oorschot/Weiner (VW) algorithm, work?</p>
<p>First, we assume that <span class="math inline">H: \set{0, 1}^n \to \set{0, 1}^n</span> is a random function. Now, we'll search for <span class="math inline">x_1, x_2</span> in <span class="math inline">\set{0, 1}^n</span> rather than <span class="math inline">\set{0, 1}^*</span>, by defining a sequence of hash values <span class="math inline">w_0 \in_R \set{0, 1}^n, w_i = H(w_{i - 1})</span> - a sequence consisting of a hash value, the hash of that hash value, the hash of the hash of that hash value, and so on. This is a random sequence since we're assuming <span class="math inline">H</span> is a random function.</p>
<p>Clearly, there must be a repeat (collision) in the sequence somewhere, because an element of the sequence can only have one of <span class="math inline">2^n</span> possible values. That means the sequence is finite and ends in a cycle. Let <span class="math inline">j</span> be the smallest index such that <span class="math inline">x_j = x_i</span> for some <span class="math inline">i &lt; j</span> - <span class="math inline">j</span> must exist because there must exist a collision. Since , <span class="math inline">x_{j + l} = x_{i + l}</span> for any <span class="math inline">l \ge 0</span>.</p>
<p>Let <span class="math inline">N = 2^n</span>. From the birthday paradox, we expect the value of <span class="math inline">j</span> to be <span class="math inline">E[j] = \sqrt{\frac{\pi N}{2}}</span>, or around <span class="math inline">\sqrt{2^n}</span>.</p>
<p>Also, <span class="math inline">E[i] = E[j - i] = \frac 1 2 \sqrt{\frac{\pi N}{2}}</span>. If we assume, without loss of generality, that <span class="math inline">i \ne 0</span>, then <span class="math inline">H(x_{i - 1}) = H(x_{j - 1})</span> and <span class="math inline">x_{i - 1} \ne x_{j - 1}</span> (with very high probability) - a collision!</p>
<p>How do we actually find <span class="math inline">x_{i - 1}, x_{j - 1}</span>, without using much storage?</p>
<p>If we think of the sequence of hashes is like a linked list, where <span class="math inline">H(x_i)</span> gets the next element <span class="math inline">x_{i + 1}</span>, then this simply becomes a problem analogous to finding cycles in a linked list, which we could solve using something like the tortoise-and-hare algorithm with just constant memory. However, that means a lot of wasted time unnecessarily computing hashes, and it only tells us that there is a collision, not what the collision actually is.</p>
<p>A better way we could do this is to define some easily distinguishable property of the hash values that applies to around fraction <span class="math inline">\theta</span> of all hash values, for a small value of <span class="math inline">\theta</span>. For example, the property might be &quot;the first 20 bits are 0&quot; (<span class="math inline">\frac 1 {2^{20}}</span> of hash values satisfy this property). Then, we only store the hash values in the sequence that satisfy this property (these hash values are called &quot;distinguished points&quot;).</p>
<p>When we encounter a distinguished point for the second time, we've found a collision somewhere behind us in the sequence - assuming there's a distinguished point between <span class="math inline">i</span> and <span class="math inline">j</span>, we must eventually encounter it again when our sequence loops around to <span class="math inline">x_{j}</span>. We can assume there's a distinguished point between <span class="math inline">i</span> and <span class="math inline">j</span> because our <span class="math inline">\theta</span> is chosen to be large enough that there is a very high probability that there is a distinguished point in the cycle - for example, for a 32-bit hash we would expect the cycle to contain about <span class="math inline">\frac 1 2 2^{32} = 2^{31}</span> entries, so we might make our distinguishing property &quot;the first 21 bits are 0&quot;, giving an expected value of <span class="math inline">2^{10}</span> distinguished points in the cycle.</p>
<p>That does mean there's a very tiny chance the algorithm doesn't terminate, however - we can get around this by limiting the length of the path to <span class="math inline">2^n</span> and retrying. There are a lot of other edge cases too (what if the starting value has a collision in the sequence?), but they all happen with negligible probability and can be handled with relatively few operations.</p>
<p>Algorithm:</p>
<ol type="1">
<li>Randomly select a <span class="math inline">x_0</span> from <span class="math inline">\set{0, 1}^n</span>, store <span class="math inline">\tup{x_0, 0, \text{empty}}</span> in the table.</li>
<li>Let <span class="math inline">L = x_0</span> (<span class="math inline">L</span> is the most recently stored distinguished point).</li>
<li>For <span class="math inline">j \ge 1</span> do (phase one starts - finding duplicate points):
<ol type="1">
<li>Let <span class="math inline">x_j = H(x_{j - 1})</span>.</li>
<li>If <span class="math inline">x_j</span> is distinguished:
<ol type="1">
<li>Store <span class="math inline">\tup{x_j, j, L}</span> into the table and let <span class="math inline">L = x_j</span>.</li>
<li>If <span class="math inline">x_j</span> was already in the table, we know <span class="math inline">x_i = x_j</span>, and stop this loop to begin phase 2 - finding the actual collision values.</li>
</ol></li>
</ol></li>
<li>Let <span class="math inline">a</span> be the index of the distinguished point right before <span class="math inline">i</span>, and <span class="math inline">b</span> be the index of the distinguished point right before <span class="math inline">j</span>. This can be looked up as the third element of tuples in the table - the values of <span class="math inline">L</span> associated with <span class="math inline">x_i</span> and <span class="math inline">x_j</span> respectively.</li>
<li>Let <span class="math inline">l_1 = i - a, l_2 = j - b</span>. Assume without loss of generality that <span class="math inline">l_1 \ge l_2</span>. Let <span class="math inline">R = l_1 - l_2</span> - how much longer the earlier chain is until the collision than the later chain.</li>
<li>Compute <span class="math inline">x_{a + 1}, \ldots, x_{a + R}</span>. Let <span class="math inline">p_1 = x_{a + R}</span> and <span class="math inline">q_1 = x_b</span>. Now the collision is an equal number of hashes away from <span class="math inline">p_1</span> and <span class="math inline">q_2</span>.</li>
<li>Let <span class="math inline">p_{k + 1} = H(p_k)</span> and <span class="math inline">q_{k + 1} = H(q_k)</span>. Compute <span class="math inline">p_k</span> and <span class="math inline">q_k</span> until we find a value <span class="math inline">k</span> such that <span class="math inline">p_k = q_k</span>. Then, the previous value <span class="math inline">p_{k - 1}</span> and <span class="math inline">q_{k - 1}</span> form a collision.</li>
</ol>
<p>So overall, step 3 takes around <span class="math inline">\sqrt{\frac{\pi N}{2}} + \frac 1 \theta</span> time, while the steps after it take <span class="math inline">\frac 3 \theta</span>. The memory usage is around <span class="math inline">\theta \sqrt{\frac{\pi N}{2}} \times 3n</span> bits.</p>
<h1 id="section-9">25/1/17</h1>
<p>If we have a 128-bit hash, we might choose a <span class="math inline">\theta = \frac 1 {2^{32}}</span>, for about <span class="math inline">2^{96}</span> distinguished points. With this, we would expect around <span class="math inline">2^{64}</span> operations and <span class="math inline">2^{41}</span> bits, or around 256 gibibytes - totally feasible on modern hardware. This tells us that any 128-bit hashes are simply not collision-resistant today.</p>
<p>How do we parallelize the VW algorithm, given <span class="math inline">m</span> processors? One naive way is to run the algorithm with different starting values on each processor (each with its own memory), and see which one finishes first. Turns out this uses time <span class="math inline">\frac{\sqrt{\frac{\pi N}{2}}}{\sqrt{m}} + \frac 4 \theta</span> - not very good scaling.</p>
<p>Instead, we can store all of the distinguished points from the <span class="math inline">m</span> processors in a central table - at each time unit, each processor adds a new unique hash value, until one of the distinguished points collide. This takes <span class="math inline">\frac{\sqrt{\pi N}{2}}{m} + \frac 4 \theta</span> operations, which is about as well as you can expect this algorithm to be parallelized.</p>
<p>How do we find meaningful collisions in a hash function? Suppose <span class="math inline">m_1</span> is &quot;Alice owes Bob 1000 dollars&quot; and <span class="math inline">m_2</span> is &quot;Alice owes Bob 10 dollars&quot;. Alice wants to find values <span class="math inline">m_1&#39;, m_2&#39;</span> that semantically mean the same thing as <span class="math inline">m_1, m_2</span> (e.g., &quot;Alice should give Bob $1000&quot; and &quot;Alice ought to pay Bob $10&quot;), sign <span class="math inline">m_1</span>, make Bob agree to it, and then later claim to have signed <span class="math inline">m_2</span>. After Bob agrees to it, Alice can claim to have signed <span class="math inline">m_2</span> instead, only owing 10 dollars rather than 1000 dollars. How can Alice find <span class="math inline">m_1&#39;, m_2&#39;</span> using the VW algorithm?</p>
<p>Partition <span class="math inline">m_1</span> into <span class="math inline">n</span> parts, with partitions at indices <span class="math inline">j_1, \ldots, j_n</span>. Let <span class="math inline">g_{m_1}(r): \set{0, 1}^n \to \set{0, 1}^*</span> be <span class="math inline">m_1</span> modified so that a space is inserted into <span class="math inline">m_1</span> at position <span class="math inline">j_i</span> if and only if the <span class="math inline">i</span>th bit of <span class="math inline">r</span> is 1. Note that <span class="math inline">g_{m_1}(r)</span> should have the same meaning as <span class="math inline">m_1</span>, since we just added a space somewhere. Likewise, define <span class="math inline">g_{m_2}(r)</span> for inserting spaces into <span class="math inline">m_2</span>.</p>
<p>Let <span class="math inline">S_0</span> be the elements of <span class="math inline">\set{0, 1}^*</span> that begin with 0, and <span class="math inline">S_1</span> be the elements that begin with 1. Let <span class="math inline">f: \set{0, 1}^n \to \set{0, 1}^n</span> as <span class="math inline">f(r) = \begin{cases} H(g_{m_1}(r)) \text{ if } r \in S_0 \\ H(g_{m_2}(r)) \text{ if } r \in S_1 \end{cases}</span> - the hash of a message semantically equivalent to <span class="math inline">m_1</span> if <span class="math inline">r</span> begins with 0, and the hash of a message semantically equivalent to <span class="math inline">m_2</span> if <span class="math inline">r</span> begins with 1.</p>
<p>If we run the VW algorithm on <span class="math inline">f</span> to find a colliding <span class="math inline">r</span>, we get a collision <span class="math inline">a, b \in \set{0, 1}^n</span> with <span class="math inline">a \ne b</span> and <span class="math inline">f(a) = f(b)</span>. With 50% probability, the first bit of <span class="math inline">a</span> is different from the first bit of <span class="math inline">b</span>, assuming they're drawn from a uniform distribution (we can assume this because we assume the hash function is a random function).</p>
<p>Suppose that the first bits are different - then <span class="math inline">a = g_{m_1}(r)</span> and <span class="math inline">b = g_{m_2}(r)</span>, and <span class="math inline">a, b</span> are two messages, semantically the same thing as <span class="math inline">m_1, m_2</span> respectively, that collide. If the first bits are not different, then we can just run the VW algorithm repeatedly with a different starting point until we get a collision that does have messages with two different first bits. Clearly, the running time is essentially the same for each iteration, and with a 50% probability of success, we can just run it a few times with different variants of <span class="math inline">g_{m_1}, g_{m_2}</span> until we get a meaningful collision.</p>
<h1 id="section-10">27/1/17</h1>
<p>So now we've looked at the naive hash collision finding, the VW algorithm, the parallel VW algorithm, and the meaningful collisions with the parallel VW algorithm.</p>
<p>Now let's look at general methods for constructing hash functions. <strong>Merkle's meta method</strong> is one of these.</p>
<p>Given an input of <span class="math inline">n + r</span> bits, we define a compression function <span class="math inline">f: \set{0, 1}^{n + r} \to \set{0, 1}^n</span>. We also have an IV value in <span class="math inline">\set{0, 1}^n</span>. Using these, we can use Merkle's meta method to define a hash function <span class="math inline">H: \set{0, 1}^* \to \set{0, 1}^n</span>.</p>
<p>Let <span class="math inline">x \in \set{0, 1}^*</span>. Let <span class="math inline">x_1 \ldots x_t</span> be <span class="math inline">x</span> broken into <span class="math inline">r</span>-bit blocks (the last one should be zero-padded if needed). Let <span class="math inline">x_{t + 1}</span> be the binary representation of the length of <span class="math inline">x</span> in bits (technically, this limits the length of <span class="math inline">x</span> to <span class="math inline">2^r</span>, but in practice <span class="math inline">2^r</span> is so large that we can simply pretend <span class="math inline">x</span> is unbounded in length).</p>
<p>Let <span class="math inline">H_0</span> be the IV. For <span class="math inline">1 \le i \le t + 1</span>, we do <span class="math inline">H_i = f(H_{i - 1}, x_i)</span>. Then, <span class="math inline">H(x) = H_{t + 1}</span>.</p>
<p>Merkle's theorem says that if <span class="math inline">f</span>, the compression function, is collision-resistant, then so is <span class="math inline">H</span>. This means we only have to make a fixed input size function collision-resistant, rather than an unbounded-input-size function collision-resistant. That means we should usually try to attack the compression function <span class="math inline">f</span> rather than the hash function itself.</p>
<p>This theorem can be proven using the contrapositive. Assume <span class="math inline">H</span> is not collision resistant, so we can efficiently find a collision <span class="math inline">\tup{x, x&#39;} \in \set{0, 1}^*</span>. Let <span class="math inline">\overline x = x_1 \ldots x_t</span> and <span class="math inline">b</span> be the length of <span class="math inline">x</span> in bits and <span class="math inline">H_1, \ldots, H_{t + 1}</span> be the iteration values for <span class="math inline">x</span>, and let <span class="math inline">\overline x&#39; = x_1&#39; \ldots x_{t&#39;}&#39;</span> and <span class="math inline">b&#39;</span> be the length of <span class="math inline">x&#39;</span> in bits and <span class="math inline">H_1&#39;, \ldots, H_{t&#39; + 1}&#39;</span> be the iteration values for <span class="math inline">x&#39;</span>. Clearly, if <span class="math inline">b \ne b&#39;</span>, then <span class="math inline">x_{t + 1} \ne x_{t&#39; + 1}&#39;</span>, so <span class="math inline">f(H_t, x_{t + 1}) = f(H_t&#39;, x_{t&#39; + 1}&#39;)</span> and <span class="math inline">x_{t + 1} \ne x_{t&#39; + 1}&#39;</span> - a collision! Now suppose <span class="math inline">b = b&#39;</span>, so <span class="math inline">x_{t + 1} = x_{t + 1}&#39;</span> and <span class="math inline">t = t&#39;</span>. If we work backwords from the last iteration toward the first iteration, we must eventually find some <span class="math inline">H_i = f(H_{i - 1}, x_i)</span> and <span class="math inline">H_i&#39; = f(H_{i - 1}&#39;, x_i&#39;)</span> such that <span class="math inline">\tup{H_{i - 1}, x_i} \ne \tup{H_{i - 1}&#39;, x_i&#39;}</span>, since <span class="math inline">x \ne x&#39;</span> - they must differ in at least one message block. This is a collision for <span class="math inline">f</span>. So, a collision is always computationally efficient to find for <span class="math inline">f</span>. So if <span class="math inline">f</span> is collision resistant, <span class="math inline">H</span> must be as well.</p>
<h1 id="section-11">30/1/17</h1>
<p>MDx is a family of iterated hash functions, the most well-known of which are MD4 and MD5. MD4 was a 128-bit hash invented by Ron Rivest in 1990, but it had some serious flaws - in 2004 someone found collisions by hand. Pre-image is still infeasible, however.</p>
<p>MD5 is another 128-bit hash, a strengthened version of MD4. Since its invention in 1991, we've found MD5 collisions in 31 seconds in 2006 on commodity hardware, while preimages can be found in <span class="math inline">2^{123.4}</span> steps. However, MD5 is still in very wide use today.</p>
<p>SHA-1 is a 160-bit hash function invented by the NSA in 1993, and modified by the NSA again in 1994 to fix a classified weakness. In 2005, it was found that that fix increased the cost of finding collisions from <span class="math inline">2^{39}</span> to <span class="math inline">2^{63}</span>. Collisions are feasible to find today, but nobody seems to have done it yet - we still want to move away from SHA-1 though, since it's theoretically feasible to find collisions. We don't know of any pre-image/second pre-image attacks on SHA-1 that are faster than the generic attacks. In about two weeks from now, SSL will no longer consider SHA-1 hashes for server certificates valid. SHA-1 is a Merkle-style hash with <span class="math inline">n = 160, r = 512</span>. Most of the design decisions are classified, because NSA, and the hash operations themselves are rather messy.</p>
<p>In 2001 the NSA proposed SHA-2, a family of hash functions with three variants, each with different output sizes - SHA-256, SHA-384, and SHA-512. Note that SHA-2 has the same security against collisions as AES-128/AES-192/AES-256. As of this writing, no attacks stronger than the generic attacks have been found, but there are concerns remaining because the design is pretty similar to SHA-1.</p>
<p>SHA-3 is a NIST hash function competition, much like the competition that resulted in AES. Starting with 64 candidates, Keecak was selected as the winner in 2012. Keecak isn't an iterated hash function, using a sponge construction method, and it isn't very widely used in practice due to most people being fine with the SHA-2 family.</p>
<p>NIST recommends we stop using SHA-1 if possible, though it's okay for HMACs/KDFs/RNGs. SHA-2 and SHA-3 are recommended.</p>
<p>Suppose we can find a meaningless collision in an iterated hash function <span class="math inline">\tup{x, y}</span>. How can we exploit this in practice? Collision attacks are useful when we can modify <strong>both the original document, and the colliding document</strong>.</p>
<p>Wang proposed an attack on SHA-1 that gives us a two-block collision (two two-block-long messages that hash to the same value) that takes only around <span class="math inline">2^{63}</span> operations. Wang also proposed an attack on MD5 that gives us a one-block collision (two one-block-long messages that hash to the same value) in only <span class="math inline">2^{39}</span> operations. We've actually applied these and found real collisions in SHA-1 and MD5. How do we exploit this concretely?</p>
<p>Suppose <span class="math inline">\tup{c_1, c_2}</span> is a one-block MD5 collision (without loss of generality, assume the blocks don't contain <code>(</code> or <code>)</code>, so it can fit into a Postscript literal). Now consider the following two Postscript files:</p>
<pre><code>%!PS-Adobe-1.0
%%BoundingBox: 0 0 612 792                   
(&lt;c_1&gt;)(&lt;c_1&gt;)eq{
&lt;HARMLESS MESSAGE&gt;
}{
&lt;HARMFUL MESSAGE&gt;
}ifelse
showpage</code></pre>
<pre><code>%!PS-Adobe-1.0
%%BoundingBox: 0 0 612 792                   
(&lt;c_2&gt;)(&lt;c_1&gt;)eq{
&lt;HARMLESS MESSAGE&gt;
}{
&lt;HARMFUL MESSAGE&gt;
}ifelse
showpage</code></pre>
<p>The two files are identical except for line 3, where we have <code>&lt;c_1&gt;</code> in the first and <code>&lt;c_2&gt;</code> in the second.</p>
<p>Clearly, the first one prints out the harmless message, while the second one prints out the harmful message. However, note that the second line has been padded with spaces so that the second block starts exactly at either <code>&lt;c_1&gt;</code> in the first message, or <code>&lt;c_2&gt;</code> in the second message (MD5 uses a block size of 64 bytes, and the file is assumed to be using Windows-style CR-LF line endings).</p>
<p>Suppose <span class="math inline">x</span> and <span class="math inline">y</span> have the same number of blocks in an iterated hash function. Let <span class="math inline">F(I, x) = H_t</span> where <span class="math inline">I = H_0</span> and <span class="math inline">H_i = f(H_{i - 1}, x_i)</span>. Clearly, if <span class="math inline">F(I, x) = F(I, y)</span>, then <span class="math inline">F(I, xz) = F(I, yz)</span> for any string <span class="math inline">z</span>. In other words, given a collision <span class="math inline">\tup{x, y}</span>, <span class="math inline">\tup{xz, yz}</span> is also a collision for any string <span class="math inline">z</span>. This is called a <strong>length extension attack</strong>.</p>
<p>Also, if <span class="math inline">F(I, x) = F(I, y)</span>, then <span class="math inline">F(I, zx) = F(I, zy)</span> for any string <span class="math inline">z</span> that is an integer multiple of the block size long. In other words, given a collision <span class="math inline">\tup{x, y}</span>, <span class="math inline">\tup{zx, zy}</span> is also a collision for any blocks <span class="math inline">z</span>.</p>
<p>Note that each Postscript document can be made by prepending a block to one of <code>&lt;c_1&gt;</code>/<code>&lt;c_2&gt;</code>, and then appending the rest of the document. Since <code>&lt;c_1&gt;</code>/<code>&lt;c_2&gt;</code> collide, the Postscript documents themselves must also collide!</p>
<p>Suppose Alice sends Bob the first Postscript file, and asks Bob for a signature. Bob opens it in a Postscript viewer, sees the harmless message, then signs the MD5 hash of the document (because signing the whole document with public key crypto would be too unwieldy), and sends the signature to Alice. However, Alice now has a signature from Bob for the second Postscript document, which has the same MD5 hash but a different, harmful message - Alice has convinced Bob to sign something Bob didn't want to!</p>
<p>(Menezes now demonstrates an MD5 collision on a real document, where the MD5 hash of a Postscript file that results in a PDF saying &quot;John didn't leave his hat in my office&quot; collides with a Postscript file that results in a PDF saying &quot;John should be given a mark of 100 in this course&quot;.)</p>
<h1 id="section-12">1/2/17</h1>
<p>;wip: slides up to slide 192 ish, MAC schemes (a lot like the cipher block chaining scheme used for symmetric ciphers),</p>
<p>;wip: MACs can't provide non-repudiation, because at least two parties must have the shared key.</p>
<p>;wip: encrypt-and-mac is insecure because the MAC can leak info about the plaintext. suppose we have a secure MAC <span class="math inline">H_k(m)</span>. construct a new MAC <span class="math inline">H_k&#39;(m) = H_k(m) \| m</span>. Clearly, this is still a secure MAC, but it leaks the entire plaintext.</p>
<h1 id="section-13">3/2/17</h1>
<p>Instead, we can append the key to the message before we hash it, rather than prepending it. This ensures that the attacker can't use a length extension attack anymore because they don't know what the secret key <span class="math inline">K</span> is, but it requires the hash function to be collision resistant in order for the resulting MAC to secure. This is called the <strong>secret suffix method</strong>.</p>
<p>If the hash function isn't collision resistant, then we can efficiently find a collision <span class="math inline">\tup{x_1, x_2}</span>. Suppose that <span class="math inline">x_1</span> and <span class="math inline">x_2</span> have lengths that are multiples of the hash block length. Then <span class="math inline">H(x_1 \| K) = H(x_2 \| K)</span>, because the values being passed into the hash function start with two different but colliding values. Therefore, we've found a collision for the MAC - a <strong>collision attack</strong>!</p>
<p>To avoid the requirement that the hash function be collision resistant, we can use the <strong>envelope method</strong>, in which we prepend and append the key to the message before we hash it. This successfully avoids the length extension attack because the attacker doesn't know what <span class="math inline">K</span> to append, and the collision attack because the attacker probably can't find a collision where both values start with <span class="math inline">K</span>, without even knowing what the value of <span class="math inline">K</span> is.</p>
<p>The <strong>Hash MAC</strong> (HMAC) is a commonly used MAC scheme that ;wip. <span class="math inline">H_k(x) = H(K \oplus \mathrm{opad} \| H(K \oplus \mathrm{ipad} \| x))</span>, where <span class="math inline">\mathrm{ipad} = 0x36, \mathrm{opad} = 0x5C</span> repeated for each byte of <span class="math inline">K</span>. The inner hash is just the secret prefix method, but hashing for the second time ensures that the hash can't ;wip. This can be proven to be secure if the compression function used in the iterated hash function <span class="math inline">H</span> is itself a secure MAC scheme with fixed length messages and a secret IV.</p>
<p>HMAC is used a lot in industry, as part of IPSec, SSL, and TLS. In practice, we generally use SHA-1 for the HMAC hash function, and it seems to be unaffected by the known weaknesses in SHA-1.</p>
<p>A secure symmetric encryption scheme ensures confidentiality. A secure MAC scheme like HMAC ensures authentication (data integrity and data origin authentication). If Alice needs to send Bob a message with both confidentiality and authentication, one way to do this is Alice can send Bob <span class="math inline">\tup{c, t} = E_{k_1}(m), H_{k_2}(m)</span>, where <span class="math inline">k_1, k_2</span> are secret keys pre-shared with Bob. This is called <strong>Encrypt-and-MAC</strong>. However, the issue here is that the MAC scheme isn't designed for confidentiality, only authentication, so theoretically, the HMAC might leak information about the plaintext. We still use this a lot in practice though, because most MAC schemes don't seem to leak any information about the plaintext.</p>
<p>A better way to do it would be to send <span class="math inline">\tup{c, t} = \tup{E_{k_1}(m), H_{k_2}(E_{k_1}(m))}</span> (ciphertext, tag) instead - we tag the encrypted message using the HMAC, rather than tagging the plaintext. This can be proven to be secure if the encryption and MAC scheme are secure. This is called the <strong>Encrypt-then-MAC</strong>.</p>
<p><strong>Authenticated encryption</strong> schemes try to provide both confidentiality and encryption in a more integrated package. The most common one in AES-GCM, which is faster than generic encrypt-then-MAC, as well as authentication-but-not-encrypted headers for data. It uses AES in counter mode (CTR mode - encrypting a counter using the key with AES, and then using the output of that as the CSPRNG for a stream cipher) and a custom MAC scheme, and requires the IV never be reused. One advantage of AES-GCM over something like the encrypt-then-MAC is that it's parallelizable and faster. AES-GCM outputs the IV, authenticated header, authenticated and encrypted message, and the 128-bit authentication tag.</p>
<h1 id="section-14">6/2/17</h1>
<p>AES-GCM is widely used in the real world for combined authentication and encryption, because it's fast and is proved correct by Iwata-Ohashi-Minematsu in 2012, under certain assumptions. In fact, Intel and AMD processors have special instructions to make AES-GCM implementations a lot faster.</p>
<h2 id="public-key-cryptography">Public Key Cryptography</h2>
<p>The main drawback of symmetric cryptography is the <strong>key establishment problem</strong> - how do we distribute secret keys to all the parties that need it, but nobody else?</p>
<p>One way to solve this is to distribute keys over a secure channel point-to-point, like a face to face meeting, a trusted courier, or a key embedded in a smart card. However, it's hard to obtain a secure channel, and if we already had a secure channel, we could often just use that channel to communicate instead.</p>
<p>Another way is to give the key to a trusted third party, which distributes the secret key as desired. However, finding a trusted third party is quite difficult, and it's also a single point of failure.</p>
<p>There's also the <strong>key management problem</strong> - in a network of <span class="math inline">n</span> users, each user needs one key for every other user, so there are <span class="math inline">O(n^2)</span> keys needed.</p>
<p>Also, symmetric encryption can't achieve non-repudiation, because there are always multiple parties that could have written a given message (since the key needs to be shared with other parties in order to communicate). Theoretically we could do this by sharing a key only with a trusted third party that ensures non-repudiation, but that kind of defeats the point of using cryptography for non-repudiation in the first place.</p>
<p><strong>Public key cryptography</strong> works somewhat differently. Using a public key cryptosystem, Alice and Bob first exchange authenticated (rather than secret) information, and then can communicate securely. It was first invented by Merkle, Diffie, and Hellman. It's much easier to exchange information in an authenticated way, then in a secret way. Intuitively this seems impossible, and Merkle's professor originally told him it was impossible back in 1975, but it is indeed possible.</p>
<p>The advantages of public key cryptography over symmetric cryptography are: only requiring an authenticated channel rather than a secured one, each user only has one keypair, a signed message can be verified by anyone, and signatures can achieve non-repudiation. The disadvantages are: private/public keys are usually a lot larger than symmetric cipher keys, making them harder to share in some cases, and public key cryptography is usually a lot slower.</p>
<p>Suppose Alice and Bob want to communicate securely. Alice and Bob want to establish a session key by communicating over a public, authenticated channel. We'll now describe Merkle's proof-of-concept public key cryptography idea.</p>
<p>Alice creates puzzles <span class="math inline">P_1, \ldots, P_N</span> for some large <span class="math inline">N</span>, and each puzzle takes time <span class="math inline">t</span> to solve. The solution to a given puzzle <span class="math inline">P_i</span> is the session key <span class="math inline">k_i</span> and the serial number <span class="math inline">n_i</span>, and Alice keeps track of the solutions to each generated puzzle. Alice sends all the puzzles to Bob, over the authenticated channel.</p>
<p>Then, Bob randomly selects a puzzle <span class="math inline">P_j</span>, solves it in time <span class="math inline">t</span> to obtain <span class="math inline">k_j</span> and <span class="math inline">n_j</span>, then sends <span class="math inline">n_j</span> back to Alice. Since Alice knows <span class="math inline">n_j</span>, Alice can easily look up the corresponding <span class="math inline">k_j</span>. Now, both Alice and Bob know <span class="math inline">k_j</span>, which they can now use as the session key. In contrast, Eve doesn't know which puzzle has serial number <span class="math inline">n_j</span>, so Eve must try each puzzle to see if the serial number matches. That means on average Eve must solve <span class="math inline">\frac N 2</span> puzzles, spending <span class="math inline">\frac{Nt}{2}</span> time overall. For large enough values of <span class="math inline">N</span> like <span class="math inline">N = 10^9</span>, it should become infeasible for Eve, while Bob can feasibly still solve the single puzzle.</p>
<p>One example of a Merkle puzzle is AES-encrypting the session key (128-bit) and the serial number (128-bit) (the serial number is appended to the session key twice, so we can check if the puzzle was solved correctly), with a randomly selected 40-bit string as the secret key. Then we can solve the puzzle by exhaustive key search in <span class="math inline">2^{40}</span> operations.</p>
<p>More generally, we want each communicating party <span class="math inline">C</span> to generate a key pair consisting of a public key <span class="math inline">P_C</span> and a private key <span class="math inline">S_C</span>, such that it's hard to recover $S_AC from <span class="math inline">P_C</span>, then publishes <span class="math inline">P_C</span> while keeping <span class="math inline">S_C</span> secret.</p>
<p>If A wants to communicate confidentially with B, then A obtains an authentic copy of <span class="math inline">P_B</span>, then sends B the message encrypted with <span class="math inline">P_B</span>. Bob then decrypts the message using <span class="math inline">S_B</span>, which only B knows. Since public key cryptography is so slow, we usually implement this in practice by using public key cryptography to communicate a session key (which is a fixed-length, relatively short value), and then using symmetric cryptography to encrypt the message with that session key.</p>
<p>If A wants to sign a message, A just signs the message with <span class="math inline">S_A</span>, which only A knows. To verify the signature on the message, we obtain an authentic copy of <span class="math inline">P_A</span> and verify the signature using <span class="math inline">P_A</span>. Since public key cryptography is so slow, we usually implement this in practice by hashing the message, and then using public key cryptography to sign the hash of the message (which is fixed-length, relatively short value).</p>
<h1 id="section-15">8/2/17</h1>
<h2 id="algorithmic-number-theory">Algorithmic Number Theory</h2>
<p>Recall the fundamental theorem of arithmatic - every integer <span class="math inline">n \ge 2</span> has a unique prime factorization.</p>
<p>Just because it exists though, doesn't mean that prime factorization is efficient to find - as of this writing, we are not aware of any way to find the prime factorization in polynomial time - we need to do around <span class="math inline">O(\sqrt{n})</span> operations, while the length of the input is around <span class="math inline">O(\log n)</span> (the number of bits needed to write the input down in a reasonable encoding). This is still an open problem - whether we can factor numbers in polynomial time with respect to the length of the input.</p>
<p>However, it's quite simple to verify whether a given prime factorization of a number is correct - just multiply them together.</p>
<p>In this course, by running time we mean the worst case time complexity. and by efficient we mean polynomial time. We also allow average case time complexity in some cases, especially when we look at probabilistic algorithms.</p>
<p>Suppose we have two integers <span class="math inline">a, b</span>. Clearly, they have lengths <span class="math inline">O(\log a)</span> and <span class="math inline">O(\log b)</span> respectively. Since we usually look at length rather than the numerical value itself, we represent the length as <span class="math inline">k_a, k_b</span> instead - the number of bits in each number.</p>
<p>Clearly, computing <span class="math inline">a + b</span> takes <span class="math inline">O(k_a + k_b)</span> - we always have to at least look at every bit of <span class="math inline">a</span> and <span class="math inline">b</span> to get the correct answer, and there are <span class="math inline">k_a + k_b</span> bits total. Likewise, computing <span class="math inline">a - b</span> is also <span class="math inline">O(k_a + k_b)</span>. We often assume without loss of generality that <span class="math inline">k_a = k_b</span>, and just call it <span class="math inline">k</span></p>
<p>Multiplication can be done naively in <span class="math inline">O(k^2)</span>, and modern algorithms can do it in as little as <span class="math inline">O(k^{\log_2 3})</span> or <span class="math inline">k^{k \log k (\log \log k)}</span> - it's still an open problem what the optimal value is.</p>
<p>For division, we can naively just repeatedly subtract the shifted version of the denominator from the numberator, zeroing one bit of the numerator each time. That means with the naive algorothm, we need to perform <span class="math inline">k</span> subtractions, giving us an total time of <span class="math inline">O(k^2)</span>.</p>
<p>The GCD algorithm <span class="math inline">\gcd(a, 0) = 0, \gcd(a, b) = \gcd(b, a \mod b)</span> continuously finds the remainder of dividing a number by another. Both numbers are positive and the second is strictly decreasing at each step. In fact, we can prove that the values are halved or lower after every two steps.</p>
<p>Since division/remainder takes <span class="math inline">O(k^2)</span>, and we're doing, in the worst case, <span class="math inline">k</span> steps, we might expect that GCD takes <span class="math inline">O(k^3)</span>. However, GCD actually just takes <span class="math inline">O(k^2)</span> time, because the values we're dividing are rapidly decreasing.</p>
<p>Let's say we're now doing modular arithmetic, so with the modulus <span class="math inline">n</span>, all values are within <span class="math inline">[0, \ldots, n - 1]</span>. Now we can compute <span class="math inline">a + b \pmod n</span> by computing <span class="math inline">a + b</span> and subtracting <span class="math inline">n</span> if <span class="math inline">a + b \ge n</span>, and we can compute <span class="math inline">a - b \pmod n</span> by computing <span class="math inline">a - b</span> and adding <span class="math inline">n</span> if <span class="math inline">a - b &lt; 0</span>. That means modular addition and subtraction are both <span class="math inline">O(k)</span>.</p>
<p>Multiplication in modular arithmetic can be done by multiplying normally, and then performing a division by <span class="math inline">n</span> and finding the remainder - it takes <span class="math inline">O(k^2)</span>. Same goes for finding the inverse, <span class="math inline">\frac 1 a \mod n</span></p>
<p>Turns out, exponentiation can be done in <span class="math inline">O(k^3)</span> time - we can compute <span class="math inline">a^b \pmod n</span> in cubic time with respect to the sizes of <span class="math inline">a</span> and <span class="math inline">b</span>!</p>
<h1 id="section-16">10/2/17</h1>
<p>The naive way to compute <span class="math inline">a^m \mod n</span> is to compute <span class="math inline">a^m</span>, then take that number mod <span class="math inline">n</span>. This potentially gets us a really large result, and is somewhat wasteful. Another slightly improved naive method is to let <code>result = 1</code>, then run <code>result = (result * a) % n</code> <span class="math inline">m</span> times. This solves the issue of having really large intermediate results, but isn't polynomial time and is pretty inefficient.</p>
<p>An improved method is to use the squaring method, which decomposes <span class="math inline">m</span> into a sum of powers of 2, then successively squares <span class="math inline">a</span> and multiplies the result for each power:</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">A <span class="op">=</span> a
result <span class="op">=</span> <span class="dv">1</span>
<span class="cf">while</span> m <span class="op">&gt;</span> <span class="dv">0</span>:
    <span class="co"># at zero-index $i$, `A` is $a^{2^i}$ and the lowest bit of `m` is the $i$th bit of the original value of $m$</span>
    <span class="cf">if</span> m <span class="op">&amp;</span> <span class="dv">2</span>: <span class="co"># $m$ written as a sum of powers of 2 has a term with exponent $i$</span>
        result <span class="op">=</span> (result <span class="op">*</span> A) <span class="op">%</span> n
    A <span class="op">=</span> (A <span class="op">**</span> <span class="dv">2</span>) <span class="op">%</span> n
    m <span class="op">&gt;&gt;=</span> <span class="dv">1</span></code></pre></div>
<p>Since there are at most <span class="math inline">k</span> modular squarings and <span class="math inline">k</span> multiplications, each of which is <span class="math inline">O(k^2)</span>, the worst case time is <span class="math inline">O(k^3)</span>.</p>
<p>Recall Fermat's Little Theorem, which says, among other things, that if <span class="math inline">a</span> and <span class="math inline">n</span> are coprime, and <span class="math inline">a^{n - 1} \mod n \ne 1</span>, then we know <span class="math inline">n</span> is composite (if it is 1 though, we don't know if it's prime or not). This is a really useful primality test - we just need to choose the right <span class="math inline">a</span> to prove that <span class="math inline">n</span> is not prime.</p>
<h2 id="rsa">RSA</h2>
<p>RSA was invented by Rivest, Shamir, and Adleman in 1977.</p>
<p>To generate a key for a user U:</p>
<ol type="1">
<li>U randomly selects 2 large distinct primes <span class="math inline">p, q</span> of the same bit length <span class="math inline">k</span>.
<ul>
<li><span class="math inline">p</span> and <span class="math inline">q</span> have to be large so their product <span class="math inline">n</span> is hard to factor. A good value of <span class="math inline">k</span> is 1024, 2048, or 4096.</li>
<li><span class="math inline">p</span> and <span class="math inline">q</span> have to be distinct so <span class="math inline">n</span> can't just be factored by square rooting the product.</li>
<li><span class="math inline">p</span> and <span class="math inline">q</span> have to be of the same bit length because there are algorithms for factoring <span class="math inline">n</span> that take time proportional to the smaller prime factor of <span class="math inline">n</span>, so we do this to maximize the hardness of factoring <span class="math inline">n</span> against these algorithms.</li>
</ul></li>
<li>U computes <span class="math inline">n = pq</span> and <span class="math inline">\phi(n) = (p - 1)(q - 1)</span>.</li>
<li>U selects an arbitrary <span class="math inline">e</span> such that <span class="math inline">1 &lt; e &lt; \phi(n)</span> and <span class="math inline">e</span> is coprime with <span class="math inline">\phi(n)</span>. A common value of <span class="math inline">e</span> is 3 - we can just ensure <span class="math inline">e</span> is coprime by ensuring it's prime.</li>
<li>U computes <span class="math inline">d</span> such that <span class="math inline">ed \equiv 1 \pmod{\phi(n)}</span> and <span class="math inline">1 &lt; d &lt; \phi(n)</span>.
<ul>
<li>We note that if <span class="math inline">ed \equiv 1 \pmod{\phi(n)}</span>, then <span class="math inline">ed + k \phi(n) = 1</span> for some integer <span class="math inline">k</span>.</li>
<li>To solve for <span class="math inline">d</span>: the extended Euclidean algorithm can compute the GCD of <span class="math inline">e</span> and <span class="math inline">\phi(n)</span>, and in the process find values for <span class="math inline">d</span> and <span class="math inline">k</span> that satisfy <span class="math inline">ed + k \phi(n) = 1</span>.</li>
<li>Recall the extended Euclidean algorithm: Initialize <span class="math inline">x_1 = 1, y_1 = 0, r_1 = e</span> and <span class="math inline">x_2 = 0, y_2 = 1, r_2 = \phi(n)</span>. Let <span class="math inline">q_n = \floor{\frac{r_{n - 2}}{r_{n - 1}}}</span> and <span class="math inline">r_n = r_{n - 2} - q_n r_{n - 1}, x_n = x_{n - 2} - q_n x_{n - 1}, y_n = y_{n - 2} - q_n y_{n - 1}</span>. Stop when <span class="math inline">r_{n + 1} = 0</span> and <span class="math inline">gcd(e, \phi(n)) = r_n</span> and <span class="math inline">d = x_n</span> and <span class="math inline">k = y_n</span>.</li>
<li>Since <span class="math inline">e</span> is prime, the GCD must be 1. If the GCD is 1, then <span class="math inline">d</span> is guaranteed to be the multiplicative inverse of <span class="math inline">e</span>, as required.</li>
</ul></li>
<li>U publishes the public key <span class="math inline">\tup{n, e}</span>, and keeps the secret key <span class="math inline">d</span> to themselves.</li>
</ol>
<p>To send an encrypted message from user U to user V (note that real implementations also need to handle things like side channel attacks):</p>
<ol type="1">
<li>U obtains an authentic copy of V's public key <span class="math inline">\tup{n_V, e_V}</span>.</li>
<li>U represents the message as <span class="math inline">0 \le m \le n_V - 1</span> such that <span class="math inline">\gcd(m, n_V) = 1</span> - the message is represented as a single, fixed-size number that is not one of the primes <span class="math inline">p, q</span>.</li>
<li>U computes <span class="math inline">c = m^e_V \mod n_V</span>.</li>
<li>U sends <span class="math inline">c</span> to V.</li>
<li>V computes <span class="math inline">m = c^d_V \mod n_V</span>. Note that no part of this requires <span class="math inline">U</span>'s public or private keys.</li>
</ol>
<p>Decryption is essentially computing <span class="math inline">(m^e)^d = m \mod n</span>. Finding <span class="math inline">e</span>th roots of a number mod <span class="math inline">n</span> requires us to factor <span class="math inline">n</span>, which is hard. The hardness of solving for <span class="math inline">m</span> is the basis for RSA's security.</p>
<p>Suppose <span class="math inline">p \mid m</span>. Then <span class="math inline">m \equiv 0 \pmod p</span>, so <span class="math inline">m^{ed} \equiv 0 \pmod p</span>, so <span class="math inline">(m^e)^d = m \pmod p</span>.</p>
<p>Suppose <span class="math inline">p \nmid m</span>. Since <span class="math inline">ed \equiv 1 \pmod{\phi(n)}</span>, <span class="math inline">ed = 1 + k\phi(n) = 1 + k(p - 1)(q - 1)</span> for some <span class="math inline">k</span>. By Fermat's Little Theorem, <span class="math inline">m^{p - 1} \equiv 1 \pmod p</span>, so taking both sides to the power of <span class="math inline">k(q - 1)</span>, we get <span class="math inline">m^{k(p - 1)(q - 1)} \equiv 1^{k(q - 1)} \pmod p</span>. Multiplying both sides by <span class="math inline">m</span>, we get <span class="math inline">m^{1 + k(p - 1)(q - 1)} \equiv m \pmod p</span>, or <span class="math inline">m^{ed} \equiv m \pmod p</span>. By a similar argument, <span class="math inline">m^{ed} \equiv 1 \pmod q</span>. Since <span class="math inline">p</span> and <span class="math inline">q</span> are coprime (since they're distinct primes), <span class="math inline">m^{ed} \equiv m \pmod{pq}</span>, so <span class="math inline">m^{ed} \equiv m \pmod n</span>.</p>
<p>An implementation of the extended Euclidean algorithm:</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="kw">def</span> EEA(a, b):
    <span class="co"># PRECONDITIONS: `a` and `b` are natural numbers</span>
    <span class="co"># POSTCONDITIONS: `r_second == a * x_second + b * y_second`</span>
    x_first, x_second <span class="op">=</span> <span class="dv">1</span>, <span class="dv">0</span>
    y_first, y_second <span class="op">=</span> <span class="dv">0</span>, <span class="dv">1</span>
    remainder_first, remainder_second <span class="op">=</span> a, b
    <span class="cf">while</span> remainder_first <span class="op">!=</span> <span class="dv">0</span>:
        quotient <span class="op">=</span> remainder_second <span class="op">//</span> remainder_first
        remainder_first, remainder_second <span class="op">=</span> remainder_second <span class="op">-</span> quotient <span class="op">*</span> remainder_first, remainder_first
        x_first, x_second <span class="op">=</span> x_second <span class="op">-</span> quotient <span class="op">*</span> x_first, x_first
        y_first, y_second <span class="op">=</span> y_second <span class="op">-</span> quotient <span class="op">*</span> y_first, y_first
    <span class="cf">return</span> remainder_second, x_second, y_second</code></pre></div>
<h1 id="section-17">13/2/16</h1>
<p>The obvious attack on RSA is to factor <span class="math inline">n</span> to get <span class="math inline">p, q</span>, and then use the extended Euclidean algorithm to get <span class="math inline">d</span> - we just need to compute the private key. After decades of research, however, the consensus is that it's a hard problem.</p>
<p>In other words, we can define the basic RSA attack as &quot;given <span class="math inline">\tup{n, e}</span>, find <span class="math inline">p, q</span> by factoring&quot;. Alternatively, &quot;given <span class="math inline">\tup{n, e}</span>, compute <span class="math inline">d</span>&quot;. Note that the compute-<span class="math inline">d</span> attack is at least as hard as factor-<span class="math inline">n</span> attack, because if we have <span class="math inline">p, q</span>, we can easily compute <span class="math inline">d</span>, but if we have <span class="math inline">d</span>, it's not particularly easy to get <span class="math inline">p, q</span>. More formally, there's a turing reduction from every compute-<span class="math inline">d</span> problem to a corresponding factor-<span class="math inline">n</span> problem - if we can factor <span class="math inline">n</span> quickly, we can compute <span class="math inline">d</span> quickly, by using the oracle to factor <span class="math inline">n</span> to get <span class="math inline">p, q</span>, then compute <span class="math inline">\phi(n) = (p - 1)(q - 1)</span> and <span class="math inline">ed \equiv 1 \pmod{\phi(n)}</span>.</p>
<p>Also, it turns out there's a turing reduction from every factor-<span class="math inline">n</span> attack to a compute-<span class="math inline">d</span> problem. Therefore, since factor-<span class="math inline">n</span> and compute-<span class="math inline">d</span> both turing reduce to each other, they are <strong>polytime equivalent</strong> (denoted <span class="math inline">A_1 = A_2</span>, where <span class="math inline">A_1</span> is factor-<span class="math inline">n</span> and <span class="math inline">A_2</span> is compute-<span class="math inline">d</span>). Knowing this is useful because factoring is a very well studied problem, and we've spent centuries convincing ourselves that it's a difficult problem.</p>
<p>The <strong>RSA problem</strong> is defined as: given <span class="math inline">\tup{n, e}</span> and <span class="math inline">0 \le c \le n - 1</span>, find <span class="math inline">0 \le m \le n - 1</span> such that <span class="math inline">c \equiv m^e \pmod n</span>. In other words, we're trying to find the <span class="math inline">e</span>th root of <span class="math inline">c</span> under mod <span class="math inline">n</span>. It turns out that the RSA problem turing reduces to factor-<span class="math inline">n</span>, so it's no harder than factoring. ;wip: why?</p>
<p>However, we'd really like the factor-<span class="math inline">n</span> problem to turing reduce to the RSA problem, because that would mean the RSA problem is at least as hard as factoring. However, this is an open question, and nobody has managed to prove it yet. That said, we generally just assume the RSA problem is hard, because we want that to be the case and nobody'd solved it yet.</p>
<p>For RSA by itself, a dictionary attack is posible - if the plaintext space is small and predictable enough, the attacker can precompute every possible plaintext/ciphertext pairs. To prevent this, we can append/prepend a random salt (e.g., a random 128-bit string) to the plaintext before encrypting it, then remove it after decrypting the ciphertext (e.g., by trimming it off of the decryption result to get the plaintext). That means the plaintext space is no longer small and predictable.</p>
<p>For RSA by itself, a chosen plaintext attack is also possible - given <span class="math inline">c</span> and a decryption oracle <span class="math inline">f(c&#39;) = m&#39;</span> for any <span class="math inline">c&#39; \ne c</span> (we can decrypt any ciphertext except the one we're given), we can compute <span class="math inline">m</span> from <span class="math inline">c</span>. How does this work?</p>
<p>Suppose without loss of generality that <span class="math inline">\gcd(c, n) = 1</span> (because if <span class="math inline">\gcd(c, n) \ne 1</span>, then it must be a factor of <span class="math inline">n</span>, so we could easily find <span class="math inline">p, q</span> and compute <span class="math inline">d</span>).</p>
<p>Now, select an <span class="math inline">2 \le x \le n - 1</span> such that <span class="math inline">\gcd(x, n) = 1</span> (again, if the GCD wasn't 1 then we've factored <span class="math inline">n</span> and are done). Let <span class="math inline">\hat c = cx^e \pmod n</span>, so we can obtain <span class="math inline">\hat m</span> using the decryption oracle: <span class="math inline">\hat m = f(\hat c)</span>. Since <span class="math inline">\hat m \equiv \hat c^d \pmod n</span> and <span class="math inline">\hat c \equiv cx^e</span>, <span class="math inline">\hat m \equiv (cx^e)^d \equiv c^d x^{ed} \equiv mx \pmod n</span>. Therefore, <span class="math inline">m = \hat m x^{-1} \mod n = f(cx^e) x^{-1} \mod n</span>.</p>
<p>To mitigate this, we can make sure decryption routines only return results if the message is formatted correctly (e.g., a hash of the rest of the message included with the message is correct). That means the decryption oracle would be highly unlikely to give an answer for any ciphertext other than one resulting from a properly encrypted message. We add in the formatting before encrypting, and then strip it out after decrypting.</p>
<h1 id="section-18">15/2/17</h1>
<p>PKCS #1 V1.5 is a set of public key cryptography standards, and has a standard for RSA. It's not really that good in practice, but it's very widely used.</p>
<p>Among other things, it defines the correct way to format the plaintext. Let <span class="math inline">M</span> be the original plaintext, <span class="math inline">m</span> be the formatted plaintext, and <span class="math inline">c = m^e \mod n</span> be the ciphertext. The format is as follows: the bytes 0x00 and 0x02 (the standard's version number), 8 or more random non-zero bytes (the salt, to prevent dictionary attacks), the 0x00 byte (to denote the end of the salt), followed by the message. The total length of the format can be up to the bit size of <span class="math inline">n</span> - 2048-bit RSA means we can have total length of 256 bytes, and a maximum message size of 245 bytes (since there are at least 11 bytes before the message itself in the format).</p>
<p>A public key cryptosystem is <strong>secure</strong> if and only if it is semantically secure against chosen-ciphertext attacks by a computationally bounded adversary.</p>
<p>To simulate a computationally bounded adversary, the adversary gets a target ciphertext and a decryption oracle that can decrypt anything except that particular ciphertext, and is trying to learn anything about the plaintext besides its length. The decryption oracle rejects any decryptions that aren't properly formatted.</p>
<p>The PKCS #1 V1.5 RSA standard's formatting of messages does allow the decryption oracle to reject attacker-generated messages in practice, but there are theoretically weaknesses in this padding scheme.</p>
<p>A polynomial time algorithm has worst case time <span class="math inline">O(n^c)</span> for a constant <span class="math inline">c</span>. An exponential time algorithm has worst case time <span class="math inline">O(2^{cn})</span> for a constant <span class="math inline">c</span>. A subexponential time algorithm has worst case time <span class="math inline">2^{o(n)}</span>. <strong>Polynomial-time (polytime)/exponential-time/subexponential-time efficient</strong> algorithms are similar to polynomial-time/exponential-time/subexponential-time algorithms, but for expected/average time rather than worst case. Polytime is efficient, Subexponential time is inefficient, and exponential time is very inefficient.</p>
<p>Subexponential algorithms can often be written in the form <span class="math inline">L_n[\alpha, c] = O(\exp((c + o(1)) (\log n)^\alpha (\log \log n)^{1 - \alpha}))</span> where <span class="math inline">0 \le \alpha \le 1</span> and <span class="math inline">c &gt; 0</span>. Note that <span class="math inline">L_n[0, c]</span> is polynomial time and <span class="math inline">L_n[1, c]</span> is fully exponential.</p>
<p>The naive way of factoring a number <span class="math inline">n</span> is to try dividing all the integers from 2 to <span class="math inline">\floor{\sqrt{n}}</span>. While this is polynomial with respect to <span class="math inline">n</span>, <span class="math inline">n</span> means a different thing in this context - the value of the number (e.g., &quot;5234&quot; has value 5234), rather than how long the number is (e.g., &quot;5234&quot; needs at least 4 digits to store).</p>
<p>Besides trial divisions, there's also a number of other special-purpose factoring algorithms like the special number field seive, elliptic curve factoring algorithm, and so on, which are only efficient if <span class="math inline">n</span> has a special form, like when <span class="math inline">n - 1</span> has only small factors. To make sure RSA is robust to all known factoring attacks, we choose the RSA primes <span class="math inline">p, q</span> at random and with the same bit length.</p>
<p>;wip: express that RSA bit size grows faster than linear wrt security level, since factoring is subexponential</p>
<p>There are also new general-purpose algorithms that can factor arbitrary numbers in slightly better time. For example, the number field seive factoring algorithm can do it in <span class="math inline">L_n\left[\frac 1 3, 1.923\right]</span>, and before that, the quadratic seive factoring algorithm can do it in <span class="math inline">L_n\left[\frac 1 2, 1\right]</span>.</p>
<p>Because there haven't been improvements on these in several decades, most mathematicians believe that factoring is not solvable in polynomial time. That said, we've used these methods to successfully factor up to RSA-768. Today, RSA-1024 is the minimum length considered secure, while RSA-2048 and RSA-3072 is recommended. Also, Shor's algorithm can factor numbers very easily on quantum computers, though we currently don't know if large-scale quantum computers can be built at all, and depends on the details of quantum mechanics that we don't have yet.</p>
<p>In hand-wavy terms, 3-DES is roughly as secure as SHA-224 and RSA-2048 (112 bits of security), AES-128 is roughly as recure as SHA256 and RSA-3072 (128 bits of security), AES-192 is roughly as secure as SHA-384 and RSA-7680 (192 bits of security), and AES-256 is roughly as secure as SHA-512 and RSA-15360 (256 bits of security). Something having <span class="math inline">k</span> bits of security means the fastest known attack for it requires around <span class="math inline">2^k</span> operations.</p>
<h1 id="section-19">17/2/17</h1>
<p>To send a signed message from user U to user V (note that real implementations also need to handle things like side channel attacks):</p>
<ol type="1">
<li>U computes <span class="math inline">s = m^d \mod n</span>, and publishes this as the signature (the signature is the <span class="math inline">e</span>th root of the message).
<ul>
<li>In practice we would hash the message first to get a fixed-size message in the range <span class="math inline">0 \le m \le n - 1</span>. If the hash function is collision-resistant, then this is secure.</li>
</ul></li>
<li>V obtains an authentic copy of U's public key <span class="math inline">\tup{n, e}</span>.</li>
<li>V checks that <span class="math inline">s^e \equiv m \pmod n</span> to verify that the signature is valid.</li>
</ol>
<p>The RSA problem needs to be hard for this to work, because otherwise we could efficiently solve <span class="math inline">s^e \equiv m \mod n</span>, allowing us to easily compute <span class="math inline">s</span> without <span class="math inline">d</span>, the secret key.</p>
<p>In decreasing order of strength, the adversary is trying to perform a <strong>total break</strong> (recovering the private key or forging signatures at will), <strong>selective forgery</strong> (forging signatures for some messages), and <strong>existential forgery</strong> (forging a signature for any one message). The adversary can perform a key-only attack, known plaintext, and chosen-plaintext attack.</p>
<p>Much like with MAC, a signature scheme is secure if it is existentially unforgeable by a computationally bounded adversary performing chosen-plaintext attack. We simulate computational boundedness by saying the adversary has access to a signing oracle - they can use the oracle to get an arbitrary message signed, as long as it's not one of the messages we actually want to forge a signature for.</p>
<p>Bellare and Rogaway proved proved that if the RSA problem is intractable and <span class="math inline">H</span> is a random function, then RSA signatures with full-domain hashes is a secure signature scheme.</p>
<p>A <strong>full-domain hash function</strong> is one that covers the entire RSA signature domain. SHA-256, for example, is not a full-domain hash function, because there are <span class="math inline">2^{256}</span> possible values, while something like RSA-4096 has <span class="math inline">n</span> possible values, and <span class="math inline">n</span> is much larger than <span class="math inline">2^{256}</span>.</p>
<p>If the hash function isn't full-domain (i.e., the hash size is a lot smaller than the RSA bit size), then there are theoretical attacks on a signature scheme using that hash and that RSA bitsize.</p>
<p>PKCS #1 V1.5 also defines a standard for RSA signatures. Invented in 1993, it's widely used today. The process looks like this:</p>
<ol type="1">
<li>Alice computes <span class="math inline">h = H(m)</span> where <span class="math inline">H</span> is a hash function like SHA-1 or SHA-256.</li>
<li>Alice formats <span class="math inline">h</span> into a message <span class="math inline">M</span> with length <span class="math inline">k</span> (the RSA bit length): the bytes 0x00 and 0x01 (representing the version of the standard), the repeated bytes 0xFF (the padding, computed to be long enough to make the entire message have length exactly <span class="math inline">k</span>), the byte 0x00 (to represent that the padding is over), the name of the hash function (e.g., &quot;SHA-1&quot;, &quot;SHA-256&quot;), and finally the hash value itself.</li>
<li>Alice signs <span class="math inline">M</span> using <span class="math inline">s = M^d \pmod n</span>, then sends Bob <span class="math inline">\tup{m, s}</span>.</li>
<li>Bob verifies the padding is valid, and checks the hash value in <span class="math inline">m</span></li>
</ol>
<h1 id="section-20">27/2/17</h1>
<p>However, this signature scheme is actually really broken if you implement it the obvious way (forgetting to check for additional bytes after the padding), and we can even forge signatures by hand pretty easily. In 2006, Blieichenbacher demonstrated an attack that is capable of breaking the PKCS #1 V1.5 signature scheme by hand.</p>
<p>First, we assume:</p>
<ol type="1">
<li><span class="math inline">e = 3</span>, because 3 is one of the most common values for <span class="math inline">e</span> in practice (we could extend the attack to work for larger <span class="math inline">e</span> values, but it would be pretty messy).</li>
<li><span class="math inline">n = 3072</span> (this works for any <span class="math inline">n</span> though).</li>
<li>The hash function is SHA-1 - a 160-bit hash (so it's a lot smaller than <span class="math inline">n</span>).</li>
<li>The signature verification routine doesn't check whether the padded hash <span class="math inline">M</span> has garbage after the end (this actually occurs surprisingly often in practice, like in OpenSSL, Adobe Acrobat, JRE, etc.).</li>
</ol>
<p>First, we pick an arbitrary message <span class="math inline">m</span>, and compute <span class="math inline">h = \mathrm{SHA1}(m)</span>. Let <span class="math inline">z</span> be the byte 0x00. Let <span class="math inline">C</span> be the 15-byte name of SHA1 in the PKCS #1 V1.5 standard. Let <span class="math inline">D = z \| C \| h</span> be the formatted version of our fake message (we don't use the 0xFF padding here).</p>
<p>Clearly, <span class="math inline">D</span> has 288 bits, since the 0 byte has 8 bits, <span class="math inline">C</span> has 120 bits, and <span class="math inline">h</span> has 160. Let <span class="math inline">N = 2^{288} - D</span>. If <span class="math inline">N</span> isn't divisible by 3, try modifying <span class="math inline">m</span> slightly and redo all the previous computations until we get one that's divisible by 3. On average, this will take 1.5 attempts, so it's very practical to do by hand.</p>
<p>Let <span class="math inline">S = 2^{1019} - \frac{2^{34}N}{3}</span>. At this point, if we try to verify the signed message <span class="math inline">\tup{m, S}</span> with the signature verification routine, it will pass - we just forged the signature! Why does this work?</p>
<p>Verification is done by computing <span class="math inline">S^3 \mod n = \left(2^{1019} - \frac{2^{34}N}{3}\right)^3 \mod n = 2^{3057} - 2^{2072}N + 2^{1087} \frac{N^2}{3} - \left(\frac{2^{34} N}{3}\right)^3 \mod n</span>. Since <span class="math inline">0 \le 2^{3057} - 2^{2072}N + 2^{1087} \frac{N^2}{3} - \left(\frac{2^{34} N}{3}\right)^3 \le n</span>, the modulo is redundant, so <span class="math inline">2^{3057} - 2^{2072}N + 2^{1087} \frac{N^2}{3} - \left(\frac{2^{34} N}{3}\right)^3 \mod n = 2^{3057} - 2^{2072}(2^{288} - D) + 2^{1087} \frac{N^2}{3} - \left(\frac{2^{34} N}{3}\right)^3</span>.</p>
<p>Let <span class="math inline">g = 2^{1087} \frac{N^2}{3} - \left(\frac{2^{34} N}{3}\right)^3</span> (the later parts of the above expression) be our &quot;garbage&quot; that we append to the end. Since <span class="math inline">0 \le g \le 2^{2072}</span>, we can represent <span class="math inline">g</span> with a bitstring of length 2072. So <span class="math inline">S^3 \mod n = 2^{3057} - 2^{2072}(2^{288} - D) + g = 2^{3057} + 2^{2360}(2^{697} - 1) + 2^{2072}D + g</span>. Note that <span class="math inline">2^{697} - 1</span> is 696 bits of all 1's, or 87 bytes of 0xFF.</p>
<p>So overall <span class="math inline">S^3 \mod n</span> is 3072 bits, the last 2072 bits are our garbage <span class="math inline">g</span>, the 288 bits before that is <span class="math inline">D</span>, the 696 bits before that is all 1's due to <span class="math inline">2^{697} - 1</span>, and the bit before that is 1 due to <span class="math inline">2^{3057}</span>, and the 15 bits before that are 0.</p>
<p>(Discussion about SHA-1 collision found recently, and its impact on PGP, SSL, Git, and the switch toward SHA-2 and SHA-3)</p>
<h1 id="section-21">1/3/17</h1>
<p>;wip: slides about properties of ecash and paper cash</p>
<p>David Chaum proposed a cryptographic currency system called e-cash, based on privacy and anonymity (like paper cash, for most purposes), but the system was never widely successful. Bitcoin later attained some success, but provides much weaker guarantees of privacy/anonymity.</p>
<p>The e-cash system has 3 parties - the payer, payee, and the financial network. A coin is a representation of value, and coins are stored on cards. Payments support transferring coins from banks to cards (withdrawal), from cards to cards (payment), and from cards to banks (deposits).</p>
<p>We want payments to <strong>keep the payer anonymous</strong>, and for <strong>payments to be untraceable</strong> (so banks don't know where the coins that are being deposited are from). Additionally, we need it to be <strong>infeasible to forge coins</strong>, and <strong>it should be impossible to double-spend coins</strong>.</p>
<p>Also, it would be nice if it could work offline, be cheap/efficient, and for cash to be easily transfered/divided.</p>
<p>For Alice to withdraw $100 from the bank in a centralized model:</p>
<ol type="1">
<li>Alice signs a withdrawal request for $100.</li>
<li>The bank decreases Alice's account balance by $100.</li>
<li>The bank signs a message containing a randomly selected 256-bit serial number and the value of the coin, and gives that message to Alice as a coin worth $100.</li>
</ol>
<p>For Alice to pay Bob and Bob to deposit to the bank in a central model:</p>
<ol type="1">
<li>Alice sends the coin to Bob, who forwards it to the bank.</li>
<li>The bank verifies that the coin signature is correct, and that it's not already spent.</li>
<li>The bank marks the coin as spent in its internal database.</li>
<li>The bank increases Bob's account balance by $100, and tells Bob that the coin was valid.</li>
<li>Bob completes the transaction with Alice.</li>
</ol>
<p>This gives us unforgeable coins and avoids double-spending, but the bank knows all the payers and payees (because it issues serial numbers), so there's neither payment anonymity or payment untraceability.</p>
<p>This works because Bob checks that the coin is valid before completing the transaction. If we didn't check it before completing the transaction, Alice could spend the coin multiple times and complete multiple transactions, while only the first store to deposit will get the coin. We also can't know who was double spending, because Bob or anyone else with a copy of the coin could do the same thing.</p>
<p>To implement payer anonymity and untraceability in the centralized system, we can use <strong>RSA blind signatures</strong>. A blind signature essentially allows one to sign something without knowing what they're signing.</p>
<p>For Alice to withdraw $100 from the bank in an anonymous centralized model:</p>
<ol type="1">
<li>Alice prepares a message <span class="math inline">M</span> containing a randomly selected 256-bit serial number and the value of the coin they want to withdraw.</li>
<li>Alice selects a blinding factor <span class="math inline">r</span>, an integer coprime to <span class="math inline">n</span> (i.e., <span class="math inline">r \ne p</span> and <span class="math inline">r \ne q</span>).</li>
<li>Alice computes a blind message <span class="math inline">m&#39; = H(m) r^e \mod n</span>. Clearly <span class="math inline">r^e \mod n</span> is a uniformly random number between 0 and <span class="math inline">n</span> (since exponentiating by <span class="math inline">e</span> is just a permutation of <span class="math inline">r</span>). Further, since <span class="math inline">r</span> is independent of <span class="math inline">m</span>, we can't learn anything about <span class="math inline">M</span> from <span class="math inline">m&#39;</span>.</li>
<li>Alice requests $100 from the bank and gives the bank <span class="math inline">m&#39;</span>.</li>
<li>The bank decreases Alice's account balance by $100, signs <span class="math inline">m&#39;</span> with <span class="math inline">s&#39; = (m&#39;)^d \mod n</span>.</li>
<li>Clearly, <span class="math inline">s&#39; = H(m)^d r^{ed} \mod n = H(m)^d r \mod n = sr \mod n</span>, where <span class="math inline">s</span> is a signature for <span class="math inline">H(m)</span>. So Alice can compute <span class="math inline">s = s&#39; r^{-1} \mod n</span> - a signature by the bank for <span class="math inline">H(M)</span>, even though the bank knows nothing about <span class="math inline">M</span>. The signed message <span class="math inline">M</span> is the coin <span class="math inline">\tup{M, s}</span>.</li>
</ol>
<p>Additionally, the bank has a different key pair for each coin value - the bank would sign a $100 coin with one key pair, a $1000 coin with a different key pair, and so on. This ensures that Alice can't prepare an <span class="math inline">M</span> that says they want to withdraw $1000 and actually only withdraw $100, which would make Alice gain $900 if it succeeded, since if Alice tried to, the signature wouldn't validate.</p>
<p>The idea is that because of the blinding factor, a particular coin could have come from any withdrawal. The bank knows it's signing a $100 coin, but not what the corresponding <span class="math inline">M</span> actually is. Therefore, the bank doesn't know who payers and payees are.</p>
<h1 id="section-22">3/3/17</h1>
<p>We still want our electronic cash system to work offline though, without a centralized network. This is difficult because we want coins to be both anonymous, and we don't want to allow double-spending of coins.</p>
<p>We can actually do this using a blinded signature scheme to satisfy both of these properties (see the slides at around page 269 for more details), though it's quite inefficient. There are real-world electronic cash systems that can do the same thing more efficiently, but they're a lot more complex.</p>
<p>This scheme doesn't need coin serial numbers because the masks and salts should be enough randomness to make each coin unique.</p>
<p>Though this sort of scheme isn't in wide use for currency, the ideas it uses, like blind signatures, are used in many real-world systems.</p>
<h1 id="section-23">6/3/17</h1>
<p>For symmetric cryptography, the communicating parties always need to share a secret key over a confidential channel. Public key cryptography reduced this requirement to only requiring an authenticated channel. How do we ensure each party gets an authentic copy of the other party's public key when encrypting a message or verifying a signature?</p>
<p>Where does Alice get Bob's public key from? How do we know it's really Bob's key? How do we revoke/update public keys? <strong>Public key infrastructure</strong> (PKI) needs to address these issues.</p>
<p>Some ways to distribute public keys are: point-to-point over trusted channels (voice, in-person, etc.), centralized trust stores (authentication trees), online/offline certificate authorities, identity-based cryptography (a trusted party holds all the key pairs and gives them out based on identity, like email).</p>
<p>PGP is one of the original public key cryptography software products. It supports algorithms like AES, 3-DES, RSA, DSA, and so on, and doesn't specify public key management itself. What ended up happening is that public keys are distributed based on the <strong>web of trust</strong> - a network of mutually trusting people who have signed each other's keys. This is based on trust being transitive (if A trusts B, and B trusts C, then A trusts C), but it doesn't work out that well in practice since trust isn't really transitive. However, it does have the advantage of not needing to trust a single third party.</p>
<p>Modern cryptography systems often use certificate authorities as a large part of their PKI - trusted third parties that issue signed certificates saying &quot;the public key X belongs to Bob&quot;. Certificates contain the person's identity (e.g., email, domain name), the person's public key, and metadata like the expiry date. The CA doesn't need to be trusted with private keys, we just need to trust that it won't sign fake certificates. Some parts of this type of PKI include the certificate formats, certificate revocation protocols, certificate policy (what each certificate can be used for), and CA protocols (how do they handle the private key)?</p>
<p>SSL and TLS are currently the biggest implementations of this sort of PKI. SSL/TLS consists of a handshake protocol (authenticate and negotiate public keys), and a record protocol (encrypt and authenticate data). The handshake protocol checks that the server's public key has a certificate signed with one of the several hundred CAs whose public keys are preinstalled in the browser, and the server can do likewise for client certificate (although client certificates are rarely used).</p>
<p>(A live demonstration of what makes up a TLS certificate, using Amazon and the Security tab in Chrome's devtools)</p>
<h1 id="section-24">8/3/17</h1>
<p>QA session with the prof. Midterm is tonight at 7-9PM.</p>
<h1 id="section-25">10/3/17</h1>
<p>In SSL, signatures are used to prove that an entity has access to a particular private key. That means that the root CAs sign their own public keys, to verify that they actually have the private keys that correspond to the public keys they use.</p>
<p>When Alice visits Bob's website, amazon.com:</p>
<ol type="1">
<li>Bob sends Alice a certificate, containing public keys for Bob (amazon.com), Symantec (the intermediate CA used by Amazon), and Verisign (the root CA used by Symantec).</li>
<li>Alice checks that Verisign's public key is correctly signed by a root CA hard coded into its trust store.</li>
<li>Alice checks that Symantec's public key is correctly signed by Verisign's public key.</li>
<li>Alice checks that Bob's public key is correctly signed by Symantec's public key. If it is correct, then Bob's public key is authentic.</li>
<li>Alice chooses a session key, encrypts it using Bob's public key, and sends it to Bob.</li>
<li>Bob decrypts the session key using his private key, and Alice and Bob can start communicating using AES-GCM.</li>
</ol>
<p>In this case, Alice and Bob depend on Symantec to correctly vouch for Bob's identity, and to very carefully guard the private keys using strong physical security measures, such as ensuring the key can only be accessed with the cooperation of 3 people and keeping the key in a special, guarded signing room. Even so, there are hundreds of root CAs trusted in the browser, and it only takes one comprimised one to issue a fake certificate. Browser vendors need to carefully choose which root CAs to include in their trust store.</p>
<p>For example, the DigiNotar root RA issued hundreds of fraudulent certificates, which were immediately used to phish around 300000 users in Iran with a fake GMail login page with the real GMail login page URL and a green padlock in the address bar. This appeared to be a state-sponsored attack. Something similar happened in 2016 when a Chinese government-controlled root CA WoSign issued fraudulent certificates for sites like GitHub and Alibaba. Browser vendors immediately removed DigiNotar and WoSign from their trust stores and pushed out updates.</p>
<p>Extended Validation (EV) certificuates are simply those belonging to CAs that meet a set of stronger requirements (e.g., a more thorough identity check process). If CAs apply for EV status, and meet those stronger requirements, they get EV status and can issue EV certificates, which generally show up in browsers as a green box rather than just a padlock.</p>
<h1 id="section-26">13/3/17</h1>
<p>Overview and discussion of the recent Wikileaks CIA documents, such as the cryptographic requirements in use.</p>
<p>Discrete logarithm public key cryptography schemes.</p>
<p>Let <span class="math inline">p</span> be an odd prime. Let <span class="math inline">\mb{Z}_p = \set{0, \ldots, p - 1}</span> with arithmetic performed mod <span class="math inline">p</span> (e.g., in <span class="math inline">\mb{Z}_{17}</span>, <span class="math inline">14 + 11 = 8</span>, and in <span class="math inline">\mb{Z}_{13}</span>, <span class="math inline">5^{-1} = 8</span>).</p>
<p>Also, <span class="math inline">\mb{Z}_p^* = \mb{Z}_p \setminus \set{0}</span>.</p>
<p>We can now write Fermat's little theorem as follows: if <span class="math inline">a \in \mb{Z}_p^*</span> for some prime <span class="math inline">p</span>, then <span class="math inline">a^{p - 1} \equiv 1 \pmod p</span>.</p>
<p>If <span class="math inline">a \in \mb{Z}_p^*</span>, then the <strong>order</strong> <span class="math inline">\ord(a)</span> of a mod <span class="math inline">p</span> is the smallest positive integer <span class="math inline">t \in \mb{Z}_p^*</span> such that <span class="math inline">a^t \equiv 1 \pmod{p}</span>. For example, given <span class="math inline">p = 13</span>, <span class="math inline">\ord(1) = 1</span> since <span class="math inline">1^1 \equiv 1 \pmod{13}</span> and <span class="math inline">\ord{12} = 2</span> since <span class="math inline">12^2 \equiv 1 \pmod{13}</span>. Due to Fermat's little theorem, the order must exist and be between 1 and <span class="math inline">p - 1</span> inclusive. For now, we'll compute these orders by trying all the possible <span class="math inline">t</span> values.</p>
<p>Given a prime <span class="math inline">p</span>, <span class="math inline">a \in \mb{Z}_p^*</span>, and <span class="math inline">t = \ord(a)</span>:</p>
<ol type="1">
<li>Given <span class="math inline">s \in \mb{Z}</span>, <span class="math inline">a^s \equiv 1 \pmod p</span> if and only if <span class="math inline">t \mid s</span>.
<ul>
<li>Proof: <span class="math inline">t \mid s</span> if and only if <span class="math inline">s</span> can be written as <span class="math inline">s = qt + r</span> where <span class="math inline">q , r \in \mb{Z}</span> and <span class="math inline">0 \le r &lt; t</span>. So <span class="math inline">t \mid s</span> if and only if <span class="math inline">a^s \equiv a^{qt + r} \equiv (a^t)^q a^r \equiv 1^q a^r \equiv a^r \pmod p</span>. Since <span class="math inline">r &lt; t</span> and <span class="math inline">t</span> is the minimum value <span class="math inline">1 \le t \le n - 1</span> such that <span class="math inline">a^t \equiv 1 \pmod p</span>, <span class="math inline">a^r \equiv 1 \pmod p</span> implies that <span class="math inline">r = 0</span> (otherwise, <span class="math inline">t</span> wouldn't be the minimum value anymore). Therefore <span class="math inline">t \mid s</span> if and only if <span class="math inline">a^r \equiv 1 \equiv a^0 \equiv a \pmod p</span>.</li>
</ul></li>
<li><span class="math inline">t \mid p - 1</span>, by Fermat's little theorem and point 1.</li>
<li>Given <span class="math inline">x, y \in \mb{Z}</span>, <span class="math inline">a^x \equiv a^y \pmod p</span> if and only if <span class="math inline">x \equiv y \pmod t</span>.
<ul>
<li><span class="math inline">a^x \equiv a^y \pmod p</span> if and only if <span class="math inline">a^{x - y} \equiv 1 \pmod p</span> (divide both sides by <span class="math inline">a^y</span>, or multiply by <span class="math inline">a^{-y}</span>). By point 1, <span class="math inline">a^x \equiv a^y \pmod p</span> if and only if <span class="math inline">t \mid (x - y)</span>, which is equivalent to saying <span class="math inline">x \equiv y \pmod t</span>.</li>
</ul></li>
</ol>
<p>The <strong>discrete logarithm problem</strong> (DLP): let <span class="math inline">p</span> be a large prime, and <span class="math inline">q</span> be a prime divisor of <span class="math inline">p - 1</span>. Let <span class="math inline">g \in \mb{Z}_p^*</span> such that <span class="math inline">\ord(g) = q</span>. Let <span class="math inline">h \in &lt;g&gt;</span>. Determine <span class="math inline">a = \log_g h</span> - in other words, find the integer <span class="math inline">0 \le a \le q - 1</span> such that <span class="math inline">h = g^a \pmod p</span> (<span class="math inline">a</span> is the <strong>discrete logarithm</strong> of <span class="math inline">h</span> in base <span class="math inline">g</span> in <span class="math inline">\mb{Z}_p</span>).</p>
<p>For example, <span class="math inline">\log_{59} 22 = 9 \pmod {67}</span>.</p>
<p>Proof that <span class="math inline">g</span> must exist:</p>
<blockquote>
<p>Clearly, there always exists an <span class="math inline">\alpha \in \mb{Z}_p^*</span> such that <span class="math inline">\ord{\alpha} = p - 1</span>.<br />
This value of <span class="math inline">\alpha</span> is called a <strong>generator</strong> of <span class="math inline">\mb{Z}_p^*</span> because <span class="math inline">\set{\alpha^k \mod p : 0 \le k \le p - 2}</span> (we use <span class="math inline">p - 2</span> because <span class="math inline">\alpha^{p - 1} \equiv 1 \pmod p</span> and <span class="math inline">\alpha^0 \equiv 1 \pmod p</span>, so <span class="math inline">k = p - 1</span> would be redundant with <span class="math inline">k = 0</span>) will give some permutation of the integers 1 to <span class="math inline">p - 1</span>.<br />
Let <span class="math inline">g = \alpha^{\frac{p - 1}{q}}</span>. Clearly, <span class="math inline">g^q \equiv (\alpha^{\frac{p - 1}{q}})^q \equiv alpha^{p - 1} \equiv 1 \pmod p</span>.<br />
So by point 1 above, <span class="math inline">\ord(g) \mid q</span>, so <span class="math inline">\ord(g)</span> is either 1 or <span class="math inline">q</span>, since <span class="math inline">q</span> is prime.<br />
However, if <span class="math inline">q = 1</span>, then <span class="math inline">g \equiv \alpha^{\frac{p - 1}{q}} \equiv 1\pmod p</span> contradicts <span class="math inline">\ord{\alpha} = p - 1</span>, so <span class="math inline">\ord(g) = q</span>.</p>
</blockquote>
<h1 id="section-27">15/3/17</h1>
<p>;wip: copy in DLP discrete logarithm problem from photo</p>
<p>One way of solving the discrete logarithm problem is <strong>exhaustive search</strong>: Compute <span class="math inline">g^a \mod p</span> for every <span class="math inline">0 \le a \le q - 1</span>, until we find the <span class="math inline">a</span> such that <span class="math inline">g^a = h</span>. This takes <span class="math inline">q</span> modular multiplications, so <span class="math inline">O(p)</span> runtime (since <span class="math inline">p</span> and <span class="math inline">q</span> are the same size).</p>
<p>Shanks's Algorithm is a faster way:</p>
<blockquote>
<p>Let <span class="math inline">m = \ceil{\sqrt{q}}</span>. Clearly, <span class="math inline">a = im + j</span> for some <span class="math inline">0 \le i \le m - 1</span> and <span class="math inline">0 \le j \le m - 1</span>, because <span class="math inline">0 \le a \le q - 1</span>. If we find <span class="math inline">i</span> and <span class="math inline">j</span>, we can find <span class="math inline">a</span>.<br />
Clearly, <span class="math inline">j = a - im</span>, so <span class="math inline">g^j \equiv g^{a - im} \pmod p</span>. So <span class="math inline">g^a (g^{-m})^i \equiv g^j \pmod p</span>, which means <span class="math inline">h (g^{-m})^i \equiv g^j \pmod p</span>.<br />
Therefore, we can find <span class="math inline">a</span> simply by trying all the values of Note that this is similar to the meet in the middle attack on DES.</p>
</blockquote>
<p>The algorithm can be written as follows:</p>
<ol type="1">
<li>Construct a table of values <span class="math inline">\tup{j, g^j \mod p}</span> for all <span class="math inline">0 \le j \le m - 1</span>, indexed by the second entry.</li>
<li>Compute <span class="math inline">c = (g^m)^{-1} \mod p</span>.</li>
<li>Compute <span class="math inline">h c^i</span> for all <span class="math inline">0 \le i \le m - 1</span> until we find an <span class="math inline">i</span> such that <span class="math inline">h c^i \equiv g^j \mod p</span> for any <span class="math inline">j</span>.</li>
<li>We get <span class="math inline">a = \log_g h = im + j</span>.</li>
</ol>
<p>Since <span class="math inline">m = \ceil{\sqrt{q}}</span>, we only need <span class="math inline">O(\sqrt q)</span> time to do the above.</p>
<p>There are actually a lot better ways to do this. Pollard's algorithm takes <span class="math inline">O(\sqrt q)</span> as well, but uses negligible space. The Number Field Seive uses ideas similar to integer factorization for RSA, and runs in <span class="math inline">L_p[\frac 1 3, 1.923]</span> - subexponential with respect to <span class="math inline">\log_2 p</span>!</p>
<p>There's also Shor's algorithm, which is a polynomial-time algorithm for quantum computers that can both find <span class="math inline">a</span> and factor <span class="math inline">n</span>. There seems to be an important correspondence between the integer factorization problem and the discrete logarithm problem, though at this time we don't have any proof of that.</p>
<p>Looking at these best known algorithms, how do we get 128 bits of security? Since the number field seive is <span class="math inline">L_p[\frac 1 3, 1.923]</span> with respect to a function of <span class="math inline">p</span>, we need a <span class="math inline">p</span> of about 3072 bits to get 128 bits of security. Since Pollard's algorithm is <span class="math inline">O(\sqrt q)</span>, we need a <span class="math inline">q</span> of about 256 bits to get 128 bits of security (we need <span class="math inline">\sqrt{q}</span> to have around 128 bits).</p>
<p>Diffie-Hellman key agreement was one of the earliest key exchange protocols, and is still widely in use. The goal is to allow Alice and Bob to agree on a shared secret key over an unsecured channel! Given public domain parameters <span class="math inline">p, q, g</span>, defined as they are for the discrete logarithm problem, and a public hash function <span class="math inline">H(x)</span>:</p>
<ol type="1">
<li>Alice chooses a random <span class="math inline">1 \le x \le q - 1</span>.</li>
<li>Alice sends <span class="math inline">X = g^x \mod p</span> to Bob (solving for <span class="math inline">x</span> is the discrete logarithm problem).</li>
<li>Bob does the same, choosing a random <span class="math inline">1 \le y \le q - 1</span> and sending <span class="math inline">Y = g^y \mod p</span> to Alice.</li>
<li>Alice computes <span class="math inline">K = Y^x \mod p = (g^y)^x \mod p = g^{xy} \mod p</span>.</li>
<li>Bob computes <span class="math inline">K = X^y \mod p = (g^x)^y \mod p = g^{xy} \mod p</span>.</li>
<li>Now both Alice and Bob have <span class="math inline">g^{xy} \mod p</span>, but nobody else does, without being able to efficiently solve the discrete logarithm problem. We can now compute <span class="math inline">k = H(K)</span> using some hash function, and use that as the secret key.</li>
</ol>
<p>Diffie-Hellman key exchange provides security against passive adversaries - adversaries that don't actively interfere with the connection. The Diffie-Hellman problem (DHP): given <span class="math inline">p, q, g, X, Y</span>, find <span class="math inline">K</span>. Clearly, DHP is at least as hard as DLP.</p>
<h1 id="section-28">17/3/17</h1>
<p>However, unauthenticated Diffie-Hellman key exchange isn't secure against active attackers, because an active attacker could just act as a man-in-the-middle (an intrude-in-the-middle attack), where the attacker pretends to be Alice to Bob and Bob to Alice, transparently forwarding messages through.</p>
<p>To fix this, we use authenticated Diffie-Hellman. Alice and Bob each generate certificates containing their identity (name, email, etc.), RSA public key, and certificate authority signature (the CA signature confirms that this RSA public key actually belongs to the given identity). Now, we do the same thing as for unauthenticated Diffie-Hellman, except afterwards, they exchange signed messages containing <span class="math inline">X</span>, <span class="math inline">Y</span>, and their identity, to</p>
<p>In real-world implementations of authenticated Diffie-Hellman, like SSL, the client (Alice) doesn't have their own signature, so only Bob sends the signed message with <span class="math inline">X</span>, <span class="math inline">Y</span>, and their identity.</p>
<p>Why do we use DH, even though Alice has Bob's public key and vice versa? In other words, why don't they just communicate with RSA? Well, Diffie-Hellman provides <strong>forward secrecy</strong> - even if an adversary managed to record all their encrypted traffic, and comprimise the long-term RSA keys, the session keys are still safe. To implement this, Alice and Bob both delete their copy of <span class="math inline">K</span>, <span class="math inline">x</span>, and <span class="math inline">y</span> after they're done communicating. Since no other copies of those exist, and it's infeasible to recompute them from the available information <span class="math inline">X</span> and <span class="math inline">Y</span>, this provides forward secrecy.</p>
<h2 id="digital-signing-algorithm-dsa">Digital Signing Algorithm (DSA)</h2>
<p>DSA is a signature scheme that's a common alternative to RSA signatures. It's based on the hardness of the discrete logarithm problem rather than integer factorization. It was designed by David Kravitz from the NSA, and standardized by NIST in 1994.</p>
<p>DSA has the public domain parameters <span class="math inline">p</span> (3072-bit prime), <span class="math inline">q</span> (256-bit prime factor of <span class="math inline">p - 1</span>), <span class="math inline">g</span> (;wip) and uses the SHA256 hash.</p>
<p>For Alice to generate keys:</p>
<ol type="1">
<li>Select a random <span class="math inline">1 \le a \le q - 1</span>.</li>
<li>Compute <span class="math inline">h = g^a \mod p</span> - the public key is <span class="math inline">h</span>, and the private key is <span class="math inline">a</span>.</li>
</ol>
<p>For Alice to generate a signature for a message <span class="math inline">M</span>:</p>
<ol type="1">
<li>Compute <span class="math inline">m = \text{SHA256}(M)</span>.</li>
<li>Select a random per-message secret <span class="math inline">1 \le k \le q - 1</span>. It's very important that <span class="math inline">k</span> not be reused for different messages.</li>
<li>Compute <span class="math inline">r = (g^k \mod p) \mod q</span>. If <span class="math inline">r = 0</span> (the probability of this is tiny however) then go back to step 2.
<ul>
<li>We can't allow <span class="math inline">r = 0</span> because in the next step <span class="math inline">m + ar</span> would just be <span class="math inline">m</span>, which would be bad.</li>
</ul></li>
<li>Compute <span class="math inline">s = k^{-1} (m + ar) \mod q</span>. If <span class="math inline">s = 0</span> (the probability of this is tiny however) then go back to step 2.</li>
<li>The signature is <span class="math inline">\tup{r, s}</span>.</li>
</ol>
<p>For Bob to verify Alice's signature <span class="math inline">\tup{r, s}</span> on a message <span class="math inline">M</span>:</p>
<ol type="1">
<li>Obtain an authentic copy of Alice's public key h$$.</li>
<li>Compute <span class="math inline">m = \text{SHA256}(M)</span>.</li>
<li>Verify that <span class="math inline">1 \le r \le q - 1</span> and <span class="math inline">1 \le s \le q - 1</span>.</li>
<li>Compute <span class="math inline">u_1 = s^{-1} m \mod q</span> and <span class="math inline">u_2 = s^{-1} r \mod q</span>.</li>
<li>Compute <span class="math inline">v = (g^{u_1} h^{u_2} \mod p) \mod q</span>.</li>
<li>Accept the signature if and only if <span class="math inline">v = r</span>.
<ul>
<li>Clearly, <span class="math inline">s = k^{-1} (m + ar) \mod q \iff k = s^{-1} (m + ar) \mod q</span>, so <span class="math inline">g^k = g^{s^{-1} (m + ar)} \mod p = g^{s^{-1} m} g^{s^{-1} ar} \mod p = g^{s^{-1} m} (g^a)^{s^{-1} r} \mod p = g^{s^{-1} m} h^{s^{-1} r} \mod p</span>.</li>
<li>So ;wip</li>
</ul></li>
</ol>
<h1 id="section-29">20/3/17</h1>
<p>Interesting properties of DSA, compared to RSA:</p>
<ul>
<li>Signatures are randomized - the value <span class="math inline">k</span> is chosen randomly, so two signatures of the same message can be different. In contrast, RSA by itself (without any salting) will always have the same signature for the same message.</li>
<li>DSA is believed but not proven to be secure, as long as the discrete logarithm problem is intractable, and the hash function (SHA256) is secure. That is, if we can find a preimage of the hash value 0, then we can forge a signature for it.</li>
<li>There's a long-term secret, Alice's private key <span class="math inline">a</span>, and a short-term, per-message secret <span class="math inline">k</span>. It's important to ensure that <span class="math inline">k</span> isn't reused and is properly disposed of after use, because knowing <span class="math inline">k</span> allows an attacker to find <span class="math inline">a</span>. This is because <span class="math inline">ks \equiv m + ar \pmod q</span>, and the attacker already knows <span class="math inline">s</span>, <span class="math inline">r</span>, and <span class="math inline">q</span> beforehand.</li>
<li>A DSA signature using SHA256 is only 512 bits long, compared to a comparable RSA signature with 3072 bits. The shortness of DSA is a big practical reason people use it over RSA.</li>
<li>For DSA, Alice can precompute <span class="math inline">k, r, k^{-1} \mod q</span>, and <span class="math inline">k^{-1} a r \mod q</span>. That means that after this, Alice can do a signature with just one SHA256 operation and one modular multiplication. Likewise, verification only takes two modular multiplications, which is less fast but still very fast.</li>
<li>DSA does signature generation very fast, while RSA does signature verification very fast.</li>
<li>Shanks' algorithm is exponential-time, compared to RSA's subexponential-time number field seive. That means the key size grows linearly with respect to the required security level.</li>
<li>DSA-512 means that <span class="math inline">q</span> is 512 bits. The corresponding size of <span class="math inline">p</span> is usually much larger. For example, DSA-512 has a <span class="math inline">p</span> of 15360 bits.</li>
</ul>
<h2 id="elliptic-curve-cryptography">Elliptic Curve Cryptography</h2>
<p>Let <span class="math inline">F = \mb{Z}_p</span>, where <span class="math inline">p</span> is a prime of at least 5. An <strong>elliptic curve over <span class="math inline">F</span></strong> is an equation <span class="math inline">E/F : Y^2 = X^3 + aX + b</span>, where <span class="math inline">a, b \in F</span> and <span class="math inline">4a^3 + 27b^2 \ne 0</span> (this is to ensure the right hand side has no cubic roots).</p>
<p>Consider a certain &quot;elliptic curve&quot; over real numbers <span class="math inline">E/\mb{R} : Y^2 = X^3 - X = X(X - 1)(X + 1)</span>. If you graph this on a 2D cartesian plane, it resembles (but is not exactly) an ellipse.</p>
<p>Consider another &quot;elliptic curve&quot; over real numbers <span class="math inline">E/\mb{R} : Y^2 = X^3 + X</span>. If you graph this on a 2D cartesian plane, it resembles a curve that opens toward the right.</p>
<p>One example of an actual elliptic curve <span class="math inline">E/\mb{Z}_{13} : Y^2 = X^3 + 2X + 9</span>. Here, <span class="math inline">F = \mb{Z}_{13}</span>, <span class="math inline">a = 2</span>, <span class="math inline">b = 9</span>, <span class="math inline">4a^3 + 27b^2 \equiv 9 \pmod{13}</span>.</p>
<p>The <strong>set of points</strong> on for an elliptic curve is the set of solutions <span class="math inline">(X, Y)</span> to the equation itself, plus the point at infinity <span class="math inline">\infty</span>. The point at infinity isn't really an actual solution to the elliptic curve - it's just a point we include as an additive/additive-inverse identity.</p>
<p>For this curve the set of points on the curve is <span class="math inline">E(F) = \set{\tup{x, y} : y^2 = x^3 + ax + b} \cup \set{\infty}</span>. Clearly, <span class="math inline">E(F) = \set{\tup{0, 3}, \tup{0, 10}, \tup{1, 5}, \tup{1, 8}, \ldots}</span>.</p>
<p>We find the set of points by trying individual <span class="math inline">X</span> values and then for each of these, trying <span class="math inline">Y</span> values.</p>
<p>Recall that we define security level of a given crypographic operation by powers of 2, where the value is the estimated number of operations we need to do to break that operation. Here are some more common security levels, including the ones we saw last time:</p>
<ul>
<li>80 bits: SKIPJACK (block cipher), SHA-1 (hash function via VW algorithm), RSA-1024 (public key crypto via number field seive), DSA-160 (signatures via Shanks' algorithm).</li>
<li>112 bits: 3DES (block cipher via meet-in-the-middle), SHA-224 (hash function via VW algorithm), RSA-2048 (public key crypto via number field seive), DSA-224 (signatures via Shanks' algorithm).</li>
<li>128 bits: AES-128 (block cipher via exhaustive key search), SHA-256 (hash function via VW algorithm), RSA-3072 (public key crypto via number field seive), DSA-256 (signatures via Shanks' algorithm).</li>
<li>128 bits: AES-128 (block cipher via exhaustive key search), SHA-256 (hash function via VW algorithm), RSA-3072 (public key crypto via number field seive), DSA-256 (signatures via Shanks' algorithm).</li>
<li>192 bits: AES-192 (block cipher via exhaustive key search), SHA-384 (hash function via VW algorithm), RSA-7680 (public key crypto via number field seive), DSA-384 (signatures via Shanks' algorithm).</li>
<li>256 bits: AES-256 (block cipher via exhaustive key search), SHA-512 (hash function via VW algorithm), RSA-15360 (public key crypto via number field seive), DSA-512 (signatures via Shanks' algorithm).</li>
</ul>
<h1 id="section-30">22/3/17</h1>
<p>Elliptic curves have some basic operations defined.</p>
<p>For example, each point on the curve <span class="math inline">P</span> has an additive inverse <span class="math inline">-P = (P_x, -P_y)</span> (basically, we flip the point about the Y axis). The additive inverse of a point is essentially the value that you'd add to that point to get the point at infinity. As a result, the point at infinity is its own additive inverse, so <span class="math inline">\infty = -\infty</span>:</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="kw">def</span> additive_inverse(point, modulus):
    <span class="co"># PRECONDITIONS: `point` is on the curve; `modulus` is prime</span>
    <span class="cf">if</span> point <span class="op">==</span> (): <span class="cf">return</span> ()
    <span class="cf">return</span> (point[<span class="dv">0</span>], <span class="op">-</span>point[<span class="dv">1</span>])</code></pre></div>
<p>The additive inverse operation is closed (<span class="math inline">-P</span> is also a point on the curve if <span class="math inline">P</span> is a point on the curve) because the elliptic curve equation uses <span class="math inline">Y^2</span>, so <span class="math inline">Y^2 = X^3 + aX + b = (-Y)^2</span></p>
<p>We can also add points; geometrically, for two different points <span class="math inline">P, Q</span>, we can draw a line between them, and this line is guaranteed to either intersect a third, different, point <span class="math inline">R</span>, and <span class="math inline">P + Q = -R</span>. For two equal points <span class="math inline">P = Q</span>, we draw a tangent line, and this line is guaranteed to intersect another, different, point <span class="math inline">R</span>, and again <span class="math inline">P + Q = -R</span>:</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="kw">def</span> integer_multiplicative_inverse(a, modulus):
    <span class="co"># PRECONDITIONS: `modulus` is prime</span>
    <span class="co"># by Fermat&#39;s Little Theorem, given a prime $p$,</span>
    <span class="co"># $a^p \equiv a \pmod p$, so $a^{p - 1} \equiv 1 \pmod p$,</span>
    <span class="co"># so $a a^{p - 2} \equiv 1 \pmod p$. Therefore,</span>
    <span class="co"># $a^{p - 2}$ is by definition the modular multiplicative</span>
    <span class="co"># inverse of $a$</span>
    <span class="co"># if $p$ wasn&#39;t prime, we would use the EEA to find an inverse instead</span>
    <span class="cf">return</span> <span class="bu">pow</span>(a, modulus <span class="op">-</span> <span class="dv">2</span>, modulus)

<span class="kw">def</span> elliptic_curve_add(point_1, point_2, curve_a, modulus):
    <span class="co"># PRECONDITIONS: `point_1` and `point_2` are on the curve; `modulus` is prime</span>
    <span class="co"># handle one of the addends being the additive identity - the point at infinity</span>
    <span class="cf">if</span> point_1 <span class="op">==</span> (): <span class="cf">return</span> point_2
    <span class="cf">if</span> point_2 <span class="op">==</span> (): <span class="cf">return</span> point_1

    x_1, y_1 <span class="op">=</span> point_1
    x_2, y_2 <span class="op">=</span> point_2
    <span class="cf">if</span> x_1 <span class="op">==</span> x_2 <span class="op">and</span> point_1 <span class="op">!=</span> point_2: <span class="cf">return</span> () <span class="co"># vertical line, the sum is the point at infinity</span>
    <span class="cf">if</span> point_1 <span class="op">==</span> point_2: <span class="co"># doubled point - use the tangent of the curve as the slope</span>
        slope <span class="op">=</span> ((<span class="dv">3</span> <span class="op">*</span> <span class="bu">pow</span>(x_1, <span class="dv">2</span>, modulus) <span class="op">+</span> curve_a) <span class="op">*</span> integer_multiplicative_inverse(<span class="dv">2</span> <span class="op">*</span> y_1, modulus)) <span class="op">%</span> modulus
    <span class="cf">else</span>: <span class="co"># find the slope of the line joining point_1 and point_2</span>
        slope <span class="op">=</span> (y_2 <span class="op">-</span> y_1) <span class="op">*</span> integer_multiplicative_inverse(x_2 <span class="op">-</span> x_1, modulus)

    <span class="co"># intersect the line with the curve to find the third point, then mirror about X axis to get the sum</span>
    sum_x <span class="op">=</span> (<span class="bu">pow</span>(slope, <span class="dv">2</span>, modulus) <span class="op">-</span> x_1 <span class="op">-</span> x_2) <span class="op">%</span> modulus
    sum_y <span class="op">=</span> (slope <span class="op">*</span> (x_1 <span class="op">-</span> sum_x) <span class="op">-</span> y_1) <span class="op">%</span> modulus
    <span class="cf">return</span> sum_x, sum_y</code></pre></div>
<p>While it's intuitively easy to think of the solutions to the curve over real numbers, with modular arithmetic over integers the geometric intuition quickly fails. There are three main causes: adding the point at infinity to anything just gives that same point again, adding a point to itself gives the additive inverse of the new point intersected by the tangent line, and adding a point to another point gives the additive inverse of the new point intersected by the line passing through the original two points.</p>
<p>It turns out that the elliptic curve and the addition operation we defined forms an Abelian group - essentially, a closed, commutative, associative group containing an identity element and where every point has an inverse:</p>
<ul>
<li>For any <span class="math inline">P, Q \in E(F)</span>, <span class="math inline">P + Q \in E(F)</span>.</li>
<li>For any <span class="math inline">P \in E(F)</span>, <span class="math inline">P + \infty = \infty + P = P</span> for all P (an identity/zero-like point exists).</li>
<li>For any <span class="math inline">P \in E(F)</span>, there exists a <span class="math inline">Q \in E(F)</span> such that <span class="math inline">P + Q = \infty</span> (an inverse exists for each point).</li>
<li>For any <span class="math inline">P, Q \in E(F)</span>, <span class="math inline">P + Q = Q + P</span> (addition is commutative).</li>
<li>For any <span class="math inline">P, Q, R \in E(F)</span>, <span class="math inline">(P + Q) + R = P + (Q + R)</span> (addition is associative).</li>
</ul>
<p>Also, <span class="math inline">\#E(F)</span> is the cardinality of the elliptic curve - the number of solutions. Interesting cryptographic properties show up when there are a prime number of solutions.</p>
<p>Another useful operation is multiplication of a scalar and an elliptic curve point. Essentially, we just add the point to itself repeatedly. We can compute this efficiently using the double-and-multiply algorithm, since addition is associative:</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="kw">def</span> elliptic_curve_multiply(a, point, curve_a, modulus):
    <span class="co"># PRECONDITIONS: `a` is a non-negative integer; `point` is on the curve; `modulus` is prime</span>
    <span class="cf">if</span> a <span class="op">==</span> <span class="dv">0</span>: <span class="cf">return</span> () <span class="co"># if we add the point to itself zero times, we get the point at infinity</span>
    point_times_power_of_2 <span class="op">=</span> point
    result <span class="op">=</span> () <span class="co"># point at infinity is the initial value</span>
    <span class="cf">while</span> a <span class="op">&gt;=</span> <span class="dv">1</span>:
        <span class="cf">if</span> a <span class="op">&amp;</span> <span class="dv">1</span>: <span class="co"># there&#39;s a 1 bit at this position, add the corresponding power of 2 times the point</span>
            result <span class="op">=</span> elliptic_curve_add(result, point_times_power_of_2, curve_a, modulus)
        a <span class="op">&gt;&gt;=</span> <span class="dv">1</span>
        point_times_power_of_2 <span class="op">=</span> elliptic_curve_add(point_times_power_of_2, point_times_power_of_2, curve_a, modulus)
    <span class="cf">return</span> result</code></pre></div>
<h1 id="section-31">25/3/17</h1>
<p>Let <span class="math inline">p</span> be prime, and <span class="math inline">E / \mb{Z}_p</span> with <span class="math inline">q = \#E(\mb{Z}_p)</span> as a prime. Let <span class="math inline">P \in E(\mb{Z}_p)</span> where <span class="math inline">P \ne \infty</span>.</p>
<p>It turns out that <span class="math inline">E(\mb{Z}_p) = \set{\infty, P, 2P, 3P, \ldots, (q - 1)P}</span> - we can repeatedly add <span class="math inline">P</span> to itself to form all of the elements of the elliptic curve (this is because all groups of prime order are cyclic).</p>
<p>We'll call <span class="math inline">P</span> a <strong>generator</strong> of <span class="math inline">E(\mb{Z}_p)</span>. For example, for <span class="math inline">E(\mb{Z}_{13}) : Y^2 = X^3 + 2X + 9</span>, we let <span class="math inline">P = \tup{0, 3}</span>, and get <span class="math inline">E(F) = \set{\tup{0, 3}, \tup{3, 9}, \tup{1, 8}, \tup{11, 7}, \ldots, \tup{11, 6}, \tup{1, 5}, \tup{3, 4}, \tup{0, 10}, \infty}</span>. Because <span class="math inline">p</span> and <span class="math inline">q</span> are prime, and for number-theoretic reasons we won't cover in this course, every point in <span class="math inline">E(\mb{Z}_p)</span> is a generator, excluding the point at infinity. This would not necessarily be true if <span class="math inline">p</span> and <span class="math inline">q</span> weren't prime.</p>
<p>We note that there's a pattern in the X axis coordinates of the points - it starts off as 0, 3, 1, 11, and then mirrors itself at the end with 11, 1, 3, 0. In fact, elliptic curves will always mirror themselves this way, due to the fact that <span class="math inline">Y^2</span> is on the left hand side of the elliptic curve equation. However, there don't seem to be any other obvious patterns.</p>
<p>The <strong>elliptic curve discrete logarithm problem</strong> (ECDLP) is what makes elliptic curve cryptography hard to break. It's defined as follows:</p>
<p>Given <span class="math inline">E p, q, P</span> defined as above and a <span class="math inline">Q \in E(\mb{Z}_p)</span>, find <span class="math inline">0 \le l \le q - 1</span> such that <span class="math inline">Q = lP</span> (we can also denote this as <span class="math inline">l = \log_P Q</span>).</p>
<p>Note that this is quite similar to the discrete logarithm problem we looked at a while ago:</p>
<ul>
<li>The group parameters are <span class="math inline">E, p, q</span> rather than <span class="math inline">p, q</span>.</li>
<li>The group operation is addition of points on the elliptic curve rather than multiplication mod <span class="math inline">p</span> (<span class="math inline">XY \mod p</span> rather than <span class="math inline">R + S</span>).</li>
<li>The generator is <span class="math inline">P \in E(\mb{Z}_p), P \ne \infty</span> rather than <span class="math inline">g \in \mb{Z}_p</span> with order <span class="math inline">q</span>.</li>
<li>The group is <span class="math inline">E(\mb{Z}_p) = \set{\infty, P, 2P, \ldots, (q - 1)P}</span> rather than <span class="math inline">&lt;g&gt; = \set{g^0 \mod p, \ldots, g^{q - 1} \mod p}</span>, both with size <span class="math inline">q</span>.</li>
</ul>
<p>The naive way of solving ECDLP instances is exhaustive search - compute <span class="math inline">P, 2P, 3P, \ldots</span> until we find <span class="math inline">Q</span>. This requires <span class="math inline">O(q)</span> elliptic curve additions, or <span class="math inline">O(p)</span> (by Hasse), which is non-polynomial. One improvement over this is to adapt Shanks' algorithm, with the group operations replaced as necessary. This gets us a running time of <span class="math inline">O(\sqrt q) = O(\sqrt p)</span> additions and <span class="math inline">O(\sqrt q) = O(\sqrt p)</span> space. We could also adapt Pollard's algorithm, which is also <span class="math inline">O(\sqrt q) = O(\sqrt p)</span> time but negligible space.</p>
<p>It turns out that there's no analogue of the number field seive for ECDLP - ECDLP cannot be solved by any known subexponential time algorithm, unlike RSA and DSA! That means using the ECDLP is better for problem hardness than RSAP or DLP.</p>
<p>Just like with DSA, and unlike RSA, ECDLP only requires a 256-bit prime <span class="math inline">p</span> to get a security level of 128 bits.</p>
<p>Elliptic curve cryptography can be used for many different situations that we used the DLP for. For example, in Elliptic Curve Diffie-Hellman Key Agreement (ECDH), we have the domain parameters <span class="math inline">E, p, q, P</span>, Alice sends Bob <span class="math inline">A = aP</span> for a randomly selected <span class="math inline">1 \le a \le q - 1</span> (this can be computed with repeated doubling, analogous to repeated squaring), Bob sends Alice <span class="math inline">B = bP</span> for a randomly selected <span class="math inline">1 \le b \le q - 1</span>, then Alice gets the session key with <span class="math inline">H(aB)</span> and Bob gets the same key with <span class="math inline">H(bA)</span>. This works because <span class="math inline">aB = a(bP) = abP = baP = b(aP) = bA</span>, and is hard to break because finding <span class="math inline">a</span> given <span class="math inline">A = aP</span> would require solving ECDLP.</p>
<h1 id="section-32">29/3/17</h1>
<p>;wip: slides up to the one titled Bitcoins</p>
<p>The big advantage of Bitcoin is that it's decentralized (so it can't be controlled by any single organization, and anyone can use it without a credit history), transactions can't be reversed, and transaction fees are pretty low.</p>
<p>A transaction is a transfer of coins from one user to another. Every transaction is broadcast to all users.</p>
<p>The Bitcoin network is organized as peer-to-peer network. Seed nodes hardcoded into most clients can ensure new clients can find peers, without already knowing any.</p>
<p>Every approximately 10 minutes, a block is created, containing all of the transactions that took place since the last block was created. The block gets added to a block chain - a sort of linked list of blocks where all links are cryptographically verified by hashing. Blocks can only be created by solving a cryptographic challenge that's designed to take 10 minutes on the fastest available computers - a proof of work. The software adjusts automatically to ensure that the proof of work takes around 10 minutes.</p>
<p>New Bitcoins are created by miners, who earn them by putting in computational power to verify transactions and help create blocks.</p>
<p>The main cryptographic primitives in Bitcoin are SHA256 and ECDSA, using the secp256k1 elliptic curve.</p>
<p>Each user picks a random <span class="math inline">1 \le a \le q - 1</span> and then computes the elliptic curve point <span class="math inline">A = aP</span>. The user's ECDSA private key is <span class="math inline">a</span>, while the user's public key is <span class="math inline">A</span>. Users are identified by public key - a given user can choose to generate more key pairs whenever they want.</p>
<p>A coin in Bitcoin is not really a particular thing, like a signed message in Chaum's electronic cash protocol. If Alice wants to give 2.5 coins to Bob, Alice generates the transaction <span class="math inline">T_{A, B} = \mathrm{sign}_A(\tup{\mathrm{hash}(T_{X, A}), A, B, 2.5}</span>, where <span class="math inline">T_{X, A}</span> is the transaction in which Alice obtained the coin she wants to give away, <span class="math inline">A</span> and <span class="math inline">B</span> are Alice and Bob's public keys, and then broadcasts the transaction to the Bitcoin network.</p>
<p>A coin can therefore be thought of as the chain of transactions, composed of all the transactions that involve that coin. Since transactions are public, anyone can go and verify the chain of ownership for the coin.</p>
<p>Satoshi Nakamoto made the first transaction in 2009, generating the first coins. This transaction is hardcoded into the software.</p>
<p>If an attacker wanted to do anything besides follow the exact rules of the currency, they need to modify the blockchain. This requires significant proof-of-work to make sure that ;wip.</p>
<p>For example, suppose Alice tries to double-spend a coin by sending the same coin to both Bob and Chris.</p>
<p>To form a block, some users will collect all transactions created in a certain period of time, and verify that they're correct. They'll then try to find a nonce such that hashing the the previous block, the user's public key, the nonce, and all of those transactions together will give a hash value that starts with <span class="math inline">t</span> zeros, where <span class="math inline">t</span> is decided by the network itself. This is hard because we need to potentially do a lot of hashes before we find a nonce that gets such a hash value.</p>
<p>Users will accept a block if all transactions in it are valid, the coins in those transactions aren't already spent, and the hash value starts with <span class="math inline">t</span> zeroes. Users show that they accept the block by using it as the previous block when generating new ones.</p>
<h1 id="section-33">31/3/17</h1>
<p>It is possible for two different blocks to be mined by different people around the same time. This causes a fork in the blockchain, because both of them are, at the time of the fork, equally valid. The Bitcoin protocol requires that users take the longest current chain, and then continue growing that chain - the users trust the chain that was the hardest to generate, because there was more computational effort expended to create it. The basic idea behind taking the longest known chain, is that in order to control the blockchain, an attacker must continually generate blocks faster than the legitimate miners, or else they would be overtaken by those miners and the legitimate miners' blocks would be accepted over the attacker's. That means to control the blockchain, one must have the majority of all computational power in the network. Most Bitcoin transactions become confirmed (i.e., the other party accepts the transaction as valid) after six new blocks are created (which takes about 60 minutes), since it's widely considered to be infeasible for an attacker to create 6 blocks faster than the legitimate part of the network.</p>
<p>Bitcoin incentivizes miners to be legitimate by rewarding miners for computing blocks. The reward automatically halves every four years, to make sure the currency has a fixed upper bound, and the work factor (number of preceding 0's required in the hash of the block) increases every 2016 blocks, to ensure it always takes about 10 minutes to mine a block.</p>
<p>The current work factor is 70 0's, which means the Bitcoin network as a whole is doing around <span class="math inline">2^{70}</span> hash operations every 10 minutes.</p>
<p>Mining pools are groups of computers that work together to mine blocks, sharing the reward in some fair way when a block is mined. iMiners in mining pools can prove that they actually tried to mine the block by giving the nonce that gave the longest prefix of 0's seen so far. For example, a nonce that gives a prefix of 53 0's in the block hash proves to the mining that the miner did about <span class="math inline">2^{53}</span> hash operations, and should be rewarded accordingly.</p>
<p>Bitcoin doesn't have instant transactions - if the merchant trusts a transaction right after it's made, the customer can quite easily double-spend the coins. Bitcoin is secure as long as the total hashing pwoer of the legitimate users in the network exceeds the malicious users' hashing power. Bitcoin doesn't have payer anonymity or privacy - all transactions are public, though Bitcoin addresses don't need to be associated with real-world identities.</p>
<p>Bitcoin transactions are actually not just from one address to another. A transaction can be from a number of different sources to a number of different recipients - in fact, if you pay 2 BTC to someone out of an address with 6 BTC, you're actually making a transaction that's sending yourself 4 BTC and the other person 2 BTC. Another common use case for multiple recipients is to implement transaction fees - sending some BTC in your transaction to the miner, to incentivise them to include your transaction in their block to mine.</p>
<p>In practice, the tough part of using Bitcoin is securely storing coins - having a copy of a wallet means having access to all of the coins in it.</p>
<p>After Bitcoin came out, blockchain-based technology really took off. Ethereum is a blockchain-based network that supports full smart-contract functionality - Turing-complete programs that run on Ethereum, the advantage being that the code couldn't be modified or stopped unless the attacker has the majority of all computing power. Smart contracts are useful for things like lotteries, crowdfunding, and democratic autonomous organizations (DAOs).</p>
<div class="status-banner" style="display: none; position: fixed; bottom: 0; left: 0; right: 0; text-align: center;">
    <div style="display: inline-block; padding: 0.8em 2em 0.5em 2em; background: black; color: white; font-size: 2em;">
        Rendering <svg xmlns="http://www.w3.org/2000/svg" height="1.4em" viewbox="0 0 1200 500" style="vertical-align: text-bottom"><title>LaTeX logo</title><g transform="matrix(45 0 0 45 40 40)" fill="white"><path d="M5.5 4.4C5.5 4.4 5.2 4.4 5.2 4.4 5.1 5.4 5 6.7 3.2 6.7 3.2 6.7 2.4 6.7 2.4 6.7 1.9 6.7 1.9 6.6 1.9 6.3 1.9 6.3 1.9 1 1.9 1 1.9 0.6 1.9 0.5 2.9 0.5 2.9 0.5 3.2 0.5 3.2 0.5 3.2 0.5 3.2 0.2 3.2 0.2 2.8 0.2 1.9 0.2 1.5 0.2 1.1 0.2 0.3 0.2 0 0.2 0 0.2 0 0.5 0 0.5 0 0.5 0.2 0.5 0.2 0.5 1 0.5 1 0.6 1 0.9 1 0.9 1 6.2 1 6.2 1 6.6 1 6.7 0.2 6.7 0.2 6.7 0 6.7 0 6.7 0 6.7 0 7 0 7 0 7 5.2 7 5.2 7 5.2 7 5.5 4.4 5.5 4.4z"/><path d="M5.3 0.2C5.3 0 5.2 0 5.1 0 5 0 4.9 0 4.9 0.2 4.9 0.2 3.3 4.2 3.3 4.2 3.2 4.4 3.1 4.7 2.5 4.7 2.5 4.7 2.5 5 2.5 5 2.5 5 4 5 4 5 4 5 4 4.7 4 4.7 3.7 4.7 3.5 4.6 3.5 4.4 3.5 4.3 3.5 4.3 3.6 4.2 3.6 4.2 3.9 3.4 3.9 3.4 3.9 3.4 5.9 3.4 5.9 3.4 5.9 3.4 6.3 4.4 6.3 4.4 6.3 4.4 6.3 4.5 6.3 4.5 6.3 4.7 5.9 4.7 5.8 4.7 5.8 4.7 5.8 5 5.8 5 5.8 5 7.7 5 7.7 5 7.7 5 7.7 4.7 7.7 4.7 7.7 4.7 7.6 4.7 7.6 4.7 7.1 4.7 7.1 4.7 7 4.5 7 4.5 5.3 0.2 5.3 0.2zM4.9 0.9C4.9 0.9 5.8 3.1 5.8 3.1 5.8 3.1 4 3.1 4 3.1 4 3.1 4.9 0.9 4.9 0.9z"/><path d="M13.3 0.2C13.3 0.2 7.2 0.2 7.2 0.2 7.2 0.2 7 2.5 7 2.5 7 2.5 7.3 2.5 7.3 2.5 7.4 0.9 7.6 0.5 9.1 0.5 9.3 0.5 9.5 0.5 9.6 0.6 9.8 0.6 9.8 0.7 9.8 0.9 9.8 0.9 9.8 6.2 9.8 6.2 9.8 6.5 9.8 6.7 8.8 6.7 8.8 6.7 8.4 6.7 8.4 6.7 8.4 6.7 8.4 7 8.4 7 8.8 6.9 9.8 6.9 10.3 6.9 10.7 6.9 11.7 6.9 12.2 7 12.2 7 12.2 6.7 12.2 6.7 12.2 6.7 11.8 6.7 11.8 6.7 10.7 6.7 10.7 6.5 10.7 6.2 10.7 6.2 10.7 0.9 10.7 0.9 10.7 0.7 10.7 0.6 10.9 0.6 11 0.5 11.3 0.5 11.5 0.5 13 0.5 13.1 0.9 13.2 2.5 13.2 2.5 13.5 2.5 13.5 2.5 13.5 2.5 13.3 0.2 13.3 0.2z"/><path d="M18.7 6.7C18.7 6.7 18.4 6.7 18.4 6.7 18.2 8.2 17.9 8.9 16.2 8.9 16.2 8.9 14.9 8.9 14.9 8.9 14.4 8.9 14.4 8.8 14.4 8.5 14.4 8.5 14.4 5.9 14.4 5.9 14.4 5.9 15.3 5.9 15.3 5.9 16.3 5.9 16.4 6.2 16.4 7 16.4 7 16.6 7 16.6 7 16.6 7 16.6 4.4 16.6 4.4 16.6 4.4 16.4 4.4 16.4 4.4 16.4 5.2 16.3 5.5 15.3 5.5 15.3 5.5 14.4 5.5 14.4 5.5 14.4 5.5 14.4 3.2 14.4 3.2 14.4 2.8 14.4 2.8 14.9 2.8 14.9 2.8 16.2 2.8 16.2 2.8 17.7 2.8 18 3.3 18.1 4.7 18.1 4.7 18.4 4.7 18.4 4.7 18.4 4.7 18.1 2.5 18.1 2.5 18.1 2.5 12.5 2.5 12.5 2.5 12.5 2.5 12.5 2.8 12.5 2.8 12.5 2.8 12.7 2.8 12.7 2.8 13.5 2.8 13.5 2.9 13.5 3.2 13.5 3.2 13.5 8.4 13.5 8.4 13.5 8.8 13.5 8.9 12.7 8.9 12.7 8.9 12.5 8.9 12.5 8.9 12.5 8.9 12.5 9.2 12.5 9.2 12.5 9.2 18.2 9.2 18.2 9.2 18.2 9.2 18.7 6.7 18.7 6.7z"/><path d="M21.7 3.1C21.7 3.1 23 1.1 23 1.1 23.3 0.8 23.6 0.5 24.5 0.5 24.5 0.5 24.5 0.2 24.5 0.2 24.5 0.2 22.1 0.2 22.1 0.2 22.1 0.2 22.1 0.5 22.1 0.5 22.5 0.5 22.7 0.7 22.7 0.9 22.7 1 22.7 1.1 22.6 1.2 22.6 1.2 21.5 2.8 21.5 2.8 21.5 2.8 20.2 0.9 20.2 0.9 20.2 0.9 20.1 0.8 20.1 0.8 20.1 0.7 20.4 0.5 20.8 0.5 20.8 0.5 20.8 0.2 20.8 0.2 20.4 0.2 19.7 0.2 19.3 0.2 19 0.2 18.4 0.2 18 0.2 18 0.2 18 0.5 18 0.5 18 0.5 18.2 0.5 18.2 0.5 18.8 0.5 19 0.5 19.2 0.8 19.2 0.8 21 3.6 21 3.6 21 3.6 19.4 6 19.4 6 19.2 6.2 18.9 6.7 17.9 6.7 17.9 6.7 17.9 7 17.9 7 17.9 7 20.3 7 20.3 7 20.3 7 20.3 6.7 20.3 6.7 19.8 6.7 19.7 6.4 19.7 6.2 19.7 6.1 19.7 6.1 19.8 6 19.8 6 21.2 3.9 21.2 3.9 21.2 3.9 22.8 6.3 22.8 6.3 22.8 6.3 22.8 6.3 22.8 6.4 22.8 6.5 22.6 6.7 22.2 6.7 22.2 6.7 22.2 7 22.2 7 22.5 6.9 23.2 6.9 23.6 6.9 24 6.9 24.5 7 24.9 7 24.9 7 24.9 6.7 24.9 6.7 24.9 6.7 24.7 6.7 24.7 6.7 24.2 6.7 24 6.6 23.8 6.3 23.8 6.3 21.7 3.1 21.7 3.1z"/></g></svg> math...
    </div>
</div>
<div class="license">
  <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-nc-sa/4.0/80x15.png" /></a> This work by <a xmlns:cc="http://creativecommons.org/ns#" href="https://uberi.github.io/" property="cc:attributionName" rel="cc:attributionURL">Anthony Zhang</a> is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>.
  Copyright 2013-2017 Anthony Zhang.
</div>
</body>
</html>
