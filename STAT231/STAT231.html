<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <title>STAT231 | Anthony Zhang</title>
  <link rel="stylesheet" href="../css/base.css" type="text/css">
  <link rel="stylesheet" href="../css/note.css" type="text/css">
  <link rel="stylesheet" href="../highlight/styles/default.css">
  <link rel="stylesheet" href="../highlight/styles/paraiso.light.css">
  <script src="../highlight/highlight.pack.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js" type="text/javascript"></script>
</head>
<body onload="highlight()">
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-68271407-1', 'auto');
    ga('send', 'pageview');

  </script>
  <h1>Lecture Notes by <a href="/">Anthony Zhang</a>.</h1>
  <ul class="site_links">
    <li><a href="/blog/" class="page">blog</a></li>
    <span class="divider"></span>
    <li><a href="http://uberi.github.io/University-Notes" class="page">notes</a></li>
    <span class="divider"></span>
    <li><a href="/resume.pdf" class="page">résumé</a></li>
    <span class="divider"></span>
    <li><a href="https://github.com/Uberi" class="contact">github</a></li>
    <span class="divider"></span>
    <li><a href="https://www.linkedin.com/in/uberi/" class="contact">linkedin</a></li>
    <span class="divider"></span>
    <li><a href="mailto:me@anthonyz.ca" class="contact">email</a></li>
    <span class="divider"></span>
    <li><a href="https://www.facebook.com/anthony.zhang.user" class="contact">facebook</a></li>
    <span class="divider"></span>
    <li><a href="https://twitter.com/anthony926535" class="contact">twitter</a></li>
    <span class="divider"></span>
    <li><a href="https://keybase.io/uberi" class="info">public key</a></li>
  </ul>
<p style="display:none"><span class="math">\[
\newcommand{\set}[1]{\left\{ #1 \right\}}
\newcommand{\tup}[1]{\left\langle #1 \right\rangle}
\newcommand{\abs}[1]{\left\lvert #1 \right\rvert}
\newcommand{\floor}[1]{\left\lfloor #1 \right\rfloor}
\newcommand{\ceil}[1]{\left\lceil#1 \right\rceil}
\newcommand{\mb}[1]{\mathbb{#1}}
\newcommand{\rem}{\operatorname{rem}}
\newcommand{\sign}{\operatorname{sign}}
\newcommand{\imag}{\boldsymbol{i}}
\newcommand{\dee}{\mathop{}\!\mathrm{d}}
\newcommand{\lH}{\overset{\text{l'H}}{=}}
\newcommand{\evalat}[1]{\left.\left(#1\right)\right|}
\newcommand{\sech}{\operatorname{sech}}
\newcommand{\spn}{\operatorname{Span}}
\newcommand{\proj}{\operatorname{proj}}
\newcommand{\prp}{\operatorname{perp}}
\newcommand{\refl}{\operatorname{refl}}
\newcommand{\magn}[1]{\left\lVert #1 \right\rVert}
\newcommand{\rank}{\operatorname{rank}}
\newcommand{\sys}[2]{\left[ #1 \mid #2\hskip2pt \right]}
\newcommand{\range}{\operatorname{Range}}
\newcommand{\adj}{\operatorname{adj}}
\newcommand{\cof}{\operatorname{cof}}
\newcommand{\diag}{\operatorname{diag}}
\newcommand{\formlp}{\operatorname{Form}(\mathcal{L}^P)}
\]</span></p>
<h1 id="stat231">STAT231</h1>
<p>Statistics.</p>
<pre><code>Cyntha Struthers
Section 001
Email: castruth@gmail.com
Mondays/Wednesdays/Fridays 1:30pm-2:20pm in STP 105</code></pre>
<p>This is the second time I'm taking this course. The first time around, I received an injury about a month in that made going to this class no longer feasible. This set of notes therefore contains the Spring 2016 version of the course, combined with the Winter 2017 version of the course. Over time, these will be merged together.</p>
<h1 id="section">4/1/17</h1>
<p>Course notes are available on LEARN - don't buy the paper copy unless you really want to.</p>
<p>There are i-clicker questions, worth 5% overall. Lowest 25% of clicker answers are not included in grade calculation. There are also 5 assignments, each worth 1%, that teach the use of R. Some tutorials include tests, and there are two midterms (each worth 15%). Final exam worth 50%.</p>
<p>In STAT230, we were given information about the population, and wanted to figure things out about individual samples. In STAT231, we will be learning about the opposite - given information about a sample of the population, what can we figure out about the population?</p>
<p>An <strong>empirical study</strong> is a study in which we learn by observation or experimentation. Empirical studies involve uncertainty by nature - if we run the same experiment multiple times, we are very likely to get results that aren't totally identical. Using the probability tools we learned about in STAT230, we can model this uncertainty and use it to learn more from the study results.</p>
<p>A <strong>population</strong> is a collection of units, like all students taking STAT231, or all the cars on a particular road. A <strong>process</strong> is a system by which units are produced, like a car insurance claim system (it produces car insurance claims as units).</p>
<p>Both of these are collections of units, but processes generally emit units over time, while populations are generally all already there at a certain point in time. For example, a population might be &quot;current STAT231 students&quot;, while a process might be &quot;past, present, and future STAT231 students&quot;.</p>
<h1 id="section-1">6/1/17</h1>
<p>Read chapter 1 and do the end of chapter problems to review STAT230. Assignment 1 due next Monday, concerns the setup and use of R.</p>
<p>A <strong>variate</strong> is a characteristic of units in a population/process, usually represented as <span class="math inline">\(x\)</span>, <span class="math inline">\(y\)</span>, or <span class="math inline">\(z\)</span>. They fall under a couple types (the type of the variate determines what kind of operations we can perform on them):</p>
<ul>
<li>Continuous - real numbers or intervals of real numbers. For example, height, weight, or component lifetime.</li>
<li>Discrete - integers or intervals of integers. For example, number of vehicular accidents on a highway, number of i-clicker questions in class.</li>
<li>Categorical - elements of a set. For example, hair color or dog breed.</li>
<li>Ordinal - elements of an ordered sequence. For example, a person's health (poor, satisfactory, good, excellent), or survey responses (strongly agree, agree, neutral, disagree, strongly disagree).</li>
<li>Complex - other types of values. For example, images or written responses.</li>
</ul>
<p>Knowing the type of a variate helps us choose a good probability model for the data.</p>
<p>An <strong>attribute</strong> of a population/process is a function of a variate that is defined for every unit in the population (not every unit in a study or survey). For example, if we have all students in a class (the population), and each individual student (the unit in the population) has a grade (variate on every unit), then an attribute might be the class average or proportion of students who passed the class (a function of the grade variate). Common attributes are mean/median/mode/variance/proportion.</p>
<p>Empirical studies are either <strong>observational</strong> or <strong>experimental</strong>. Observational empirical studies are those in which information is collected about the population/process without attempting to change any variates of units in the population. When we do change variates, it is an experimental empirical study. Generally, in an experimental empirical study, researchers will do something to one subset of the population, and leave the rest as a control group. Note that even in an observational empirical study, researchers can change variates outside of the context of the study - for example, researchers might ask the entire population to complete a survey, which changes variates like &quot;how fed up is this participant with surveys?&quot;, yet this is still an observational study, since the survey is external to the study itself.</p>
<p><strong>Sample surveys</strong> are observational empirical studies in which we select a representative sample of units from population and determine relevant variates for those units.</p>
<p>Numerical and graphical summaries are used to make sense of large datasets, where we would otherwise not be able to by looking at the numbers directly.</p>
<p>Some numerical summaries include measures of location (sample mean, median, mode), variability/dispersion (sample variance, sample standard deviation, range, interquartile range), and shape (sample skewness, sample kurtosis).</p>
<p>A dataset is a set <span class="math inline">\(y = \set{y_1, \ldots, y_n}\)</span>. If <span class="math inline">\(y_i \in \mb{R}\)</span>, sample mean is <span class="math inline">\(\overline y = \frac 1 n \sum_{y_i \in y} y_i\)</span>, sample median is <span class="math inline">\(\hat{y} = \frac{y_{\left(\floor{\frac n 2}\right)}}{y_{\left(\ceil{\frac n 2}\right)}}\)</span>, and sample mode doesn't usually have a widely accepted notation.</p>
<p>The median of a dataset is not unique - a dataset with an even number of elements can have two medians, though usually we just average them to get a single summary value.</p>
<p>The mode is sometimes considered to be multiple values as well when there are several values that are most common, though in this course we consider the mode to not exist when there's no single most common value. Also, the most commonly occurring group/class in grouped data is known as the <strong>sample modal group/class</strong>.</p>
<p>An <strong>order statistic</strong> is just a sorted version of a dataset. It's denoted <span class="math inline">\(\set{y_{(1)}, \ldots, y_{(n)}}\)</span>.</p>
<h1 id="section-2">9/1/17</h1>
<p>There's actually a shortcut to get to STP from MC - go left instead of straight and cut through St. Jerome.</p>
<p>Tutorial test next wednesday. One question might be, &quot;a study was conducted in October 2016 on people recovering from heart disease at a particular clinic, where they were asked to complete a questionnaire - what is the process or population?&quot;. Any reasonable answer will work: &quot;population of people who attended the clinic in October 2016&quot;. Also look into questions like &quot;how does the mean/median/variance change if we do something to the dataset, like add 5 to eah sample?&quot;.</p>
<p>A bimodal distribution is one that has what appears to have multiple &quot;humps&quot; on its histogram. A unimodal distribution is one that looks like a single &quot;hump&quot; - a bell-curve-like shape.</p>
<p>Common numerical summaries we care about are:</p>
<ul>
<li>The centre of the data, or <strong>central tendency</strong>:
<ul>
<li>The <strong>sample mean/arithmetic mean</strong>: for a variable <span class="math inline">\(y\)</span>, the sample mean is denoted <span class="math inline">\(\overline y = \frac 1 n \sum_{i = 0}^n y_i\)</span>.
<ul>
<li>The nice thing about this is that the sum of the deviations from the mean is always 0, so <span class="math inline">\(\sum (y_i - \overline y) = 0\)</span>.</li>
<li>Under an affine transformation <span class="math inline">\(y_i = ax_i + b\)</span>, <span class="math inline">\(\overline y = a \overline x + b\)</span> - the mean can simply be transformed as well.</li>
</ul></li>
<li>The <strong>geometric mean</strong> is better for logarithmically distributed data: for a variable <span class="math inline">\(y\)</span>, the geometric mean is denoted <span class="math inline">\(\overline y = \left(\prod_{i = 0}^n y_i\right)^{\frac 1 n}\)</span>.</li>
<li>The <strong>harmonic mean</strong> is rarely useful, and is the reciprocal of the arithmetic mean of the reciprocals of a variable.</li>
<li>The average is usually the one value that, if applied in the problem's situation, is equivalent to applying all the original values.</li>
<li>The <strong>median</strong> is the middle-most observation, or the sample mean of the middlemost observations if there are multiple. Essentially, we arrange the dataset in ascending order, and then pick the middle one.
<ul>
<li>The advantage of medians is that they are less sensitive to outliers than summaries like the sample mean.</li>
<li>The concept of the median can be extended to <strong>quartiles</strong> and <strong>percentiles</strong>, and generalized into <strong>quartiles</strong>. While the median is the value such that 50% of the dataset is at or below it, the quartiles and percentiles are a different fraction. Percentiles are represented with <span class="math inline">\(q(p)\)</span> where <span class="math inline">\(0 \le p \le 1\)</span>.</li>
<li>The first quartile is a value for which 25% of the data is equal or below that value (<span class="math inline">\(q(0.25)\)</span>), while the second quartile is 50% (<span class="math inline">\(q(0.5)\)</span>), the third 75% (<span class="math inline">\(q(0.75)), and the fourth 100% (\)</span>q(1)$).</li>
<li>The first percentile is a value for which 1% of the data is equal or below that value, while the second percentile is 2%, the third 3%, and so on.</li>
<li>To find the <span class="math inline">\(0 \le p \le 1\)</span> percentile, we take the <span class="math inline">\(p(n + 1)\)</span>th value, or average the nearest 2 values if <span class="math inline">\(p(n + 1)\)</span> is not an integer.</li>
</ul></li>
<li>The <strong>mode</strong> is the observation or observations that occur most often (a dataset can have more than 1 mode). This is often more useful for categorical data, or discrete numerical data with only a few possibilities.</li>
<li>The mean, median, and mode aren't always a good measure of centrality. We can combine this with other types of summaries to get a more accurate picture. For example, histograms are a great way to see the entire distribution of the data.</li>
</ul></li>
<li>The volatility, or <strong>dispersion</strong> - how far the values are spread out:
<ul>
<li>The <strong>range of a dataset</strong> is two numbers - the minimum value of the dataset, and the maximum value. This can also be thought of as the zeroth and fourth quartile values. We also often subtract the smaller from the larger to get a single-number range.</li>
<li>The <strong>interquartile range</strong> (IQR) is the range of the middle 50% of the dataset - the first quartile value and third quartile value (<span class="math inline">\(q(0.75) - q(0.25)\)</span>). It's more robust than the range because it excludes extreme values and outliers.</li>
<li>The <strong>sample variance</strong> is defined as <span class="math inline">\(s^2 = \frac{1}{n - 1} \sum_{i = 0}^n \left(y_i - \overline y\right)^2\)</span>. Note that the variance is <span class="math inline">\(s^2\)</span>, not <span class="math inline">\(s\)</span>. Also, the variance of a whole dataset/population <span class="math inline">\(x = \set{x_1, \ldots, x_n}\)</span>.
<ul>
<li>This is almost, but not quite, the average of the squared deviation from the mean.</li>
<li>Basically, this measures how much the data is spread out from the mean.</li>
<li>There's a good reason to divide by <span class="math inline">\(n - 1\)</span> rather than <span class="math inline">\(n\)</span>, but we'll cover that later on.</li>
<li>Under an affine transformation <span class="math inline">\(y_i = ax_i + b\)</span>, <span class="math inline">\(s_y^2 = a^2 s_x^2\)</span> - the squared factor applies to the variance, but not the variance.</li>
<li>Another useful formula for the sample variance is <span class="math inline">\(s^2 = \frac 1 {n - 1} \left(\sum y_i^2 - n(\overline y)^2\right)\)</span>.</li>
<li>Note that the units for the variance is always the square of the units for the samples themselves.</li>
<li>Variance is also known as <span class="math inline">\(\mu_2\)</span>, the second central moment, <span class="math inline">\(E[(Y - \mu)^2]\)</span>.</li>
</ul></li>
<li>The <strong>standard deviation</strong> is the positive square root of the sample variance, denoted <span class="math inline">\(s\)</span>.
<ul>
<li>Why do we square the deviations rather than just adding them up like <span class="math inline">\(\sum_{i = 0}^n (y_i - \overline y)\)</span>? The negative deviations and positive deviations would cancel each other out; squaring the values ensures that the standard deviation accumulates to a non-negative value. This also ensures that variance is always symmetric - points above and below the mean both contribute the same amount to the variance.</li>
<li>Why do we square the deviations rather than adding their absolute values like <span class="math inline">\(\sum_{i = 0}^n \abs{y_i - \overline y}\)</span>? While the absolute value function would still represent variance (the given formula is called the <strong>mean absolute deviation</strong>), it's harder to work with since it's not differentiable. Also, the squared deviation is affected by outliers quadratically while the absolute deviation is only affected linearly, so taking its square root later would give unintuitive results.</li>
<li>Under an affine transformation <span class="math inline">\(y_i = ax_i + b\)</span>, <span class="math inline">\(s_y = a s_x\)</span> - the factor applies to the standard deviation, but not the intercept (this trivially follows from the formula for the sample variance).</li>
<li>If the dataset is unimodal and roughly symmetric, then about 68% of the data will be within 1 standard deviation of the mean, and around 95% of the data will be within 2 standard deviations. This is because the dataset would look somewhat like the normal distribution, which has probability values that give us those numbers. This doesn't work so well for things like bimodal distributions, but is a very common summary in the real world.</li>
</ul></li>
<li>The <strong>mean absolute deviation</strong> is defined as <span class="math inline">\(\frac{1}{n - 1} \sum_{i = 0}^n \abs{y_i - \overline y}\)</span>. This is what we get if we use the absolute value function rather than squaring in the formula for variance. It's relatively rarely used.</li>
</ul></li>
<li>How fat the tails are, or <strong>kurtosis</strong> - the frequency of extreme obserations.</li>
<li>How symmetric the data is about some point or axis, or <strong>symmetry</strong>.</li>
</ul>
<h1 id="section-3">11/1/17</h1>
<h3 id="symmetry">Symmetry</h3>
<p>We also often care about <strong>sample skewness</strong> - how non-symmetric the data is. This measures how different the dataset looks when comparing the side of the mean, mirrored over to the right side. Left-skewing/negative-skewing datasets have distributions that seem to &quot;lean to the right&quot; (they have a long left tail), while right-skewing/positive-skewing distributions seem to &quot;lean to the left&quot; (they have a long right tail).</p>
<p>One way to estimate skewness is to compare the mean and median, measuring <span class="math inline">\(\text{mean} - \text{median}\)</span>. If the mean is less than the median, then we'd say the dataset might be left skewed (more of the weight is on the left), if they were equal, we say the dataset is probably symmetric (weight is about the same above and below the mean), and if the mean was greater than the median, then we say the dataset might be right-skewed (more of the weight is on the right side).</p>
<p>The real measure of skewness is <span class="math inline">\(\frac{\frac 1 n \sum \left(y_i - \overline y\right)^3}{\left(\frac 1 n \sum \left(y_i - \overline y\right)^2\right)^{\frac 3 2}}\)</span>. We won't be expected to memorize this for the course. The numerator is the the mean cubed error, and the denominator somewhat resembles the population variance. The use of cubing in the numerator preserves the sign of the error, while the denominator has the <span class="math inline">\(\frac 3 2\)</span> exponent in order to scale the numerator to a reasonable range.</p>
<p>A long right tail in a dataset distribution means a positive skewness. A long left tail means a negative skewness. A symmetric distribution has a skewness close to 0. A skewness between -1 and 1 is generally considered &quot;close to 0&quot; for Guassian data.</p>
<p>For example, if a person is diagnosed with a disease that has a mean time of death about 8 months after diagnoses, and survives for several months after diagnosis, then they should hope that the distribution of deaths is more right-skewed, so that most of the probability of death is already behind them.</p>
<p>Skewness is defined as <span class="math inline">\(\frac{\mu_3}{\sigma^3}\)</span>, where <span class="math inline">\(\mu_3\)</span> is the third central moment, <span class="math inline">\(E[(Y - \mu)^3]\)</span>, and <span class="math inline">\(\sigma\)</span> is the standard deviation.</p>
<h3 id="spikiness">Spikiness</h3>
<p><strong>Sample kurtosis</strong> is the measure of how frequent extreme observations are with respect to the normal distribution - how &quot;s&quot;. Basically, it checks if the tails of the dataset's distribution are fatter (more extreme observations) or thinner (less extreme observations) than a normal distribution. It can also be said to measure how peaky/pointy the distribution's curve is - too much stuff in the middle means big values greater than 3, too little stuff in the middle means small values less than 3.</p>
<p>Kurtosis is defined as <span class="math inline">\(K = \frac{\frac 1 n \sum \left(y_i - \overline y\right)^4}{\left(\frac 1 n \sum \left(y_i - \overline y\right)^2 \right)^2}\)</span>. Note that <span class="math inline">\(K\)</span> must be non-negative. A normal distribution has <span class="math inline">\(K = 3\)</span>. If <span class="math inline">\(K &gt; 3\)</span>, then the tails of the dataset's distribution are fatter than a normal distribution (there are more extreme observations), and if <span class="math inline">\(K &lt; 3\)</span>, then the tails of the dataset's distribution are thinner than a normal distribution (there are fewer extreme observations).</p>
<p>Kurtosis of 0 means the data is all the same - there's no error. The uniform distribution has a kurtosis close to 1.8, because it has no outliers - all of the values are within a couple standard deviations. Values between 2 and 4 are generally considered &quot;close to 3&quot; for Guassian data.</p>
<p>Kurtosis is especially useful in finance, where accurate assessment of risks and expected returns are important. It basically tells us how normal-like our data is - if the kurtosis is very close to 3, we can often assume that the dataset is normally distributed, which is extremely useful for doing statistics with.</p>
<p>Kurtosis is defined as <span class="math inline">\(\frac{\mu_4}{\sigma^4}\)</span>, where <span class="math inline">\(\mu_4\)</span> is the fourth central moment, <span class="math inline">\(E[(Y - \mu)^4]\)</span>, and <span class="math inline">\(\sigma\)</span> is the standard deviation.</p>
<p>So, to tell if a Guassian model is suitable for a given dataset, we can check if the mean/median are roughly the same, the skewness is close to 0, the kurtosis is close to 3, and about 95% of samples are within 2 standard deviations.</p>
<p>The five-number summary of any dataset is a tuple containing the min, first quartile, median, third quartile, and the max. They're useful for summarizing location and spread, but they can't represent things like bimodal distributions.</p>
<h2 id="graphical-summaries">Graphical summaries</h2>
<p>A frequency histogram has rectangles where the height is the number of observations in that bucket. Note that the bucket sizes might not be the same. A <strong>relative frequency</strong> histogram is a frequency histogram where the rectangle heights are scaled based on the bucket width and the total number of observations - <span class="math inline">\(\text{rectangle height} = \frac{\text{number of observations in bucket}}{\text{total number of observations} \times (\text{upper bound of bucket} - \text{lower bound of bucket})}\)</span>.</p>
<p>Relative frequency histograms are useful because we can superimpose a probability density function on top of them, since the Y axes match. The sum of the rectangle areas also sum up to 1 (the heights of the rectangles don't necessarily sum up to 1, however).</p>
<p>Potential test question: which bucket of a relative frequency histogram does the median fall into? Each percentile of a histogram lies within the first bucket whose value, when added to all previous bucket values, is greater than that percentile. For example, to find the median's bucket, we add bucket heights from left to right until we reach or exceed 0.5.</p>
<p>Empirical CDFs are to CDFs as relative frequency histograms are to PDFs - they're a way to let us graphically compare CDFs to the actual shape of the data. An empirical CDF is a step function that jumps up by <span class="math inline">\(\frac 1 n\)</span> (where <span class="math inline">\(n\)</span> is the number of observations) for each observation, where the X axis is observation values and the Y axis is the cumulative probability. Each percentile can be found in the empirical CDF by taking the X axis value correponding to that percentage of the Y axis. The CDF technically allows us to reproduce the entire dataset, given that it's drawn with enough resolution. The distribution and skewness of the data can be found by looking at the spacing between steps on the X axis - closer spacing on the left side compared to the right side implies the data tends to be positively skewed.</p>
<p>Empirical CDFs are often used in research papers to summarize a dataset, since most datasets are too large to include in the paper.</p>
<p>A <strong>box-and-whiskers plot</strong> is a useful way to show the five number summary that also highlights outliers. It's a 1D plot that looks something like this:</p>
<pre><code>  o   &lt;- each outlier is plotted as a point, while non-outliers are only summarized by the box and whiskers

----- &lt;- top whisker represents maximum value that isn&#39;t an outlier
  |
----- &lt;- top of box represents third quartile
|   |
|---| &lt;- line in the box represents median
|   |
----- &lt;- bottom of box represents first quartile
  |
----- &lt;- bottom whisker represents minimum value that isn&#39;t an outlier

      &lt;- there might not be any outliers in the dataset</code></pre>
<p>An <strong>outlier</strong> is an observation <span class="math inline">\(x_i\)</span> such that <span class="math inline">\(x_i &gt; Q_3 + 1.5 IQR\)</span> or <span class="math inline">\(x_i &lt; Q_1 - 1.5 IQR\)</span>, where <span class="math inline">\(Q_1, Q_3\)</span> are the first and third quartiles, and <span class="math inline">\(IQR\)</span> is the interquartile range <span class="math inline">\(Q_3 - Q_1\)</span>. The reason we use 1.5 is because it is convention, and fits the normal distribution relatively well. In other words, an outlier is any point that doesn't fall into the range <span class="math inline">\([2.5Q_1 - 1.5Q_3, 2.5Q_3 - 1.5Q_1)]\)</span>. When we're drawing a box plot, we figure out the quartiles, figure out the ranges in which observations are outliers, and then find the max/min observations that aren't outliers.</p>
<p>Box plots visually show the shape of the data. A box with whiskers that are different lengths is skewed to one side, while the height of the box shows the dispersion.</p>
<p>Box plots tell us the approximate variance, the quartiles, the outliers, and the skewness.</p>
<h1 id="section-4">13/1/17</h1>
<p><strong>Order statistics</strong> are the sorted observation values.</p>
<p>Consider a dataset containing mortality percentages from cancer types for female humans in Ontario, including lung cancer, stomach cancer, and other. Clearly, the population is human females in Ontario, and the dataset is categorical since the types of cancer are discrete and not ordered.</p>
<p>What kind of graphical summary should we use? News sites might use a pie chart, while a better option might be a bar graph (a bar graph looks a lot like a histogram, but we space out the bars to show that each bar is a different thing).</p>
<p>Graphs should always have appropriate axis bounds, have clearly labelled axes and titles with units, and should only be used when really appropriate.</p>
<p>All of the above summaries are for univariate data. However, bivariate datasets (datasets of the form <span class="math inline">\(\set{\tup{x_1, y_1}, \ldots, \tup{x_n, y_n}}\)</span>) are also common. We often graphically summarize these datasets with a scatter plot, which is very useful for showing relationships between the two variates.</p>
<p>The <strong>sample correlation</strong> is a measure of association between two numerical variables. It's defined as <span class="math inline">\(r_{xy} = \frac{\sum \left(x_i - \overline x\right)\left(y_i - \overline y\right)}{\sqrt{\sum \left(x_i - \overline x\right)^2} \sqrt{\sum \left(y_i - \overline y\right)^2}}\)</span>. Basically, it's a simplified version of <span class="math inline">\(r_{xy} = \frac{S_{x, y}}{\sqrt{S_x S_y}}\)</span>, where <span class="math inline">\(S_{x_1 \ldots, x_n} = \frac 1 n \sum (x_i - \overline x)(y_i - \overline y)\)</span> is the sample covariance and <span class="math inline">\(S_x\)</span> is the sample variance. Note that <span class="math inline">\(-1 \le r_{xy} \le 1\)</span> - values close to 1 means that there's a linear positive correlation, and values close to 0 means that there's almost no correlation. Specifically, sample correlation measures <strong>linear association</strong>.</p>
<p>The sample correlation value is generally not very intuitive to guess - it's easy to find datasets that intuitively have no correlation, that have a large sample correlation. We usually wantto see a scatter plot or something to help verify our results.</p>
<p>An explanatory variate is a variate that's used to explain or determine the distribution in a study. A response variate is a variate that changes in response to the explanatory variate. In other words, the explanatory variate is sort of like the cause, and the response variate is sort of like the effect. For example, if we have a study that asks if clicker grades determine the final STAT231 grade, an explanatory variate might be clicker grades, while the response variate might be the final STAT231 grade. Likewise, if we have a study that asks if final grades explain clicker marks, the final grade would be the explainatory variate, and the clicker grade would be the response variate.</p>
<h1 id="section-5">16/1/17</h1>
<p>Start reading chapter 2. Look up information about tutorial test 1 on Odyssey and on LEARN. Seats are assigned.</p>
<p>Scatter plots work for numeric datasets, like discrete or continuous ones. However, we can't use them for categorical datasets, like the dataset consisting of pairs containing whether a particular student is from Canada, and which university program they're in. For these bivariate categorical datasets, we have different summaries. One of them is a table.</p>
<p>Another way is the relative risk. Suppose we have a dataset <span class="math inline">\(D = \set{\tup{x_1, y_1}, \ldots, \tup{x_n, y_n}}\)</span> with two variables. <strong>Relative risk</strong> a way of summarizing bivariate categorical data - a measurement of how two variables affect each other. Basically, relative risk is <span class="math inline">\(\frac{P(A \mid B)}{P(A \mid \neg B)}\)</span> - the probability of <span class="math inline">\(A\)</span> occurring given that <span class="math inline">\(B\)</span> occurrs over the probability of <span class="math inline">\(A\)</span> occurring given that <span class="math inline">\(B\)</span> does not occur. For independent variables, relative risk is 1. The farther the relative risk is from 1, the more strongly it implies that the variables are associated.</p>
<p>For example, suppose we have categorical variables, <span class="math inline">\(x = \set{\text{smoker}, \text{non-smoker}}\)</span> and <span class="math inline">\(y = \set{\text{lung cancer}, \text{no lung cancer}}\)</span>. How do we determine the relationship between these variables, given a dataset of samples?</p>
<h2 id="statistical-problems">Statistical Problems</h2>
<p>We have been doing descriptive statistics so far - describing statistical phenomena that we can see with numerical and graphical techniques to show features of interest (e.g., data mining, data analysis).</p>
<p>Now we will focus on statistical inference - predicting statistical phenomena that we can't see, and generalizing conclusions from studies to the whole process/population (i.e., machine learning). It's a form of inductive reasoning.</p>
<p>The types of problems we'll look at in this course are:</p>
<ul>
<li>Estimation problems (what proportion of Ontario residents aged 14-20 smoke? what's the distribution of survival times for HIV patients?)</li>
<li>Hypothesis testing problems (is it true that a particular cancer treatment works? does so-and-so reduce criminal activity?)</li>
<li>Prediction problems (what will the value of this stock be at a particular time? how much will a patient's blood pressure drop if they take this drug?)</li>
</ul>
<p>We use our numerical and graphical summaries to choose a statistical models (mathematical models incorporating probability) to fit the data. For example, we can use statistical models to fit the exchange rate between CAD and Euro, or when earthquakes/natural disasters are likely to happen. Statistical models can even be used to simulate experiments using computers.</p>
<p>A <strong>random variable</strong> is a variable that represents a variate of a randomly selected unit of a population/process.</p>
<p>Statistical models are chosen based on background knowledge about populations (for example, we might have traffic accident data, which can be a Poisson process due to the way the process works), past experience (human height data is usually normally distributed), mathematical convenience (using easy-to-understand and easy-to-use models where possible, like Guassian), and datasets that the models will be tested against (for example, we might be testing against normally distributed data).</p>
<p>In STAT230 we covered a lot of different families of statistical models, like:</p>
<ul>
<li><span class="math inline">\(Y \sim \mathrm{Binomial}(n, \theta)\)</span> for sequences of <span class="math inline">\(n\)</span> independent success/fail trials each with <span class="math inline">\(\theta\)</span> probability of success, where <span class="math inline">\(Y\)</span> is the number of successes.</li>
<li><span class="math inline">\(Y \sim \mathrm{Poisson}(\theta)\)</span> for random occurences of an event over time averaging <span class="math inline">\(\theta\)</span> per unit time, where <span class="math inline">\(Y\)</span> is the number of event occurences.</li>
<li><span class="math inline">\(Y \sim \mathrm{Exponential}(\theta)\)</span> for random occurences of an event over time averaging <span class="math inline">\(\theta\)</span> per unit time, where <span class="math inline">\(Y\)</span> is the time between event occurences.</li>
<li><span class="math inline">\(Y \sim \mathrm{Guassian}(\theta), \theta = \tup{\mu, \sigma}\)</span> for a wide variety of measures occurring in nature or when many statistical models are summed together, where <span class="math inline">\(Y\)</span> is the value of the quantity.</li>
</ul>
<p>The PDF of a random variable is <span class="math inline">\(f(y; \theta) = \frac{\dee}{\dee y} P(Y \le y) = \frac{\dee}{\dee y} F(y)\)</span> where <span class="math inline">\(y \in \mathrm(range)(Y)\)</span> - we want to show that the model is dependent on the value of the statistical model family's parameters.</p>
<h1 id="section-6">18/1/17</h1>
<p>Tutorial test 1 was today. Start working on chapter 2 end of chapter problems.</p>
<p>The tutorial test had a few questions about parameter estimation - estimating the parameters for a statistical model based on a sample of a population/process, and quantify how good those estimates are.</p>
<p>For example, we can estimate <span class="math inline">\(\mu\)</span> in a normal distribution as the sample mean, or <span class="math inline">\(\sigma^2\)</span> as the sample variance. We could also use the sample proportion to estimate <span class="math inline">\(p\)</span> in a Binomial distribution. We figured this out intuitively, but there are formal ways to estimate parameters too. In this course, we'll look at the Method of Maximum Likelihood (MLE), the most often used method. Another common one is the Bayesian Estimation Method.</p>
<p>A <strong>point estimate</strong> of a parameter <span class="math inline">\(\theta\)</span> is an estimate <span class="math inline">\(\hat \theta\)</span> that is the result of a function (called a point estimator) of observed data <span class="math inline">\(y_1, \ldots, y_n\)</span>. For example, the sample mean <span class="math inline">\(\overline y\)</span> is a point estimate for the parameter <span class="math inline">\(\mu\)</span> in a Guassian distribution.</p>
<p>Suppose we have a coin that has probability <span class="math inline">\(\theta\)</span> of being heads. How do we estimate <span class="math inline">\(\theta\)</span>? Intiutively, to get the <strong>best guess/estimate</strong> we can from the available data, we could use a Binomial model, flip the coin a lot of times, and take the sample proportion to estimate the model's <span class="math inline">\(p\)</span> parameter. We could also keep flipping the coin until we got some number of failures, and use a negative binomial model.</p>
<p>The best guess is not the same thing as a good/reasonable guess - a good/reasonable guess is just one that makes the result we got relatively probable. A bad guess would make the result we got unlikely, like guessing that our coin was fair if we only got 1/100 heads (it's highly improbably that the coin has 50% heads if we observe that).</p>
<p>The best guess of a parameter is the one that is most likely to give us the results we did - the <strong>maximum likelihood estimate</strong> <span class="math inline">\(\hat \theta\)</span> (note that this is the same as the point estimate). For example, if we flip a coin 100 times and get 45 heads, assuming a binomial model, the value of parameter <span class="math inline">\(p\)</span> that is most likely to give us this result is 0.45.</p>
<p>In other words, if we graph <span class="math inline">\(P(Y = 45)\)</span> where <span class="math inline">\(Y ~ \mathrm{Binomial}(100, p)\)</span> over all possible <span class="math inline">\(p\)</span>, the graph has a peak at <span class="math inline">\(p = 0.45\)</span>. Any procedure that finds the parameter that gives us the maximum likelihood estimate is called a <strong>Method of Maximum Likelihood</strong>. Basically, we decide how plausible each value of <span class="math inline">\(\theta\)</span> is by looking at how probably it makes our observations.</p>
<p>Note that <span class="math inline">\(Y\)</span> is a random variable representing the potential data used to estimate <span class="math inline">\(\theta\)</span>, while <span class="math inline">\(y\)</span> is the actual, observed data. Also, the set of possible values of <span class="math inline">\(\theta\)</span> is often called <span class="math inline">\(\Omega\)</span>, the parameter space.</p>
<h2 id="maximum-likelihood-estimates">Maximum Likelihood Estimates</h2>
<p>The <strong>likelihood function</strong> for <span class="math inline">\(\theta\)</span> in a statistical model is defined as <span class="math inline">\(L(\theta; y) = P(Y = y; \theta) = P(Y = y; \theta)\)</span> (the probability of observing data <span class="math inline">\(y\)</span> as a function of <span class="math inline">\(\theta\)</span>), sometimes shortened as <span class="math inline">\(L(\theta)\)</span>. The <strong>maximum likelihood estimate</strong> is the value of <span class="math inline">\(\theta\)</span> that maximizes the likelihood function. Usually, we do this by differentiating the likelihood function with respect to <span class="math inline">\(\theta\)</span>, then setting that derivative to 0 and solving for <span class="math inline">\(\theta\)</span> to find the critical points, then checking to see if they're the global maximum.</p>
<h3 id="discrete-distributions">Discrete distributions</h3>
<p>For example, the likelihood function for <span class="math inline">\(\theta\)</span> in a binomial model is <span class="math inline">\(L(\theta; y) = P(Y = y; \theta) = {n \choose y} \theta^y (1 - \theta)^{n - y}\)</span> - we just substituted <span class="math inline">\(y\)</span> in place of <span class="math inline">\(k\)</span>.</p>
<p>The value of the likelihood function doesn't actually matter, only how large it is relative to other values of the likelihood function. That means we can drop constant factors and terms in the likelihood function, since the maximum would still occur at the same <span class="math inline">\(\theta\)</span> value. For example, we can just as well use <span class="math inline">\(L(\theta; y) = \theta^y (1 - \theta)^{n - y}\)</span>.</p>
<h1 id="section-7">20/1/17</h1>
<p>The likelihood function is the probability of observing the data <span class="math inline">\(y\)</span> as a function of the unknown constant <span class="math inline">\(\theta\)</span>, scaled by an arbitrary positive constant <span class="math inline">\(k\)</span>.</p>
<p>The <strong>relative likelihood function</strong> is defined as <span class="math inline">\(R(\theta; y) = \frac{L(\theta; y)}{L(\hat \theta)}\)</span> - the likelihood function rescaled to be between 0 and 1. This makes comparing likelohood functions easier.</p>
<p>The binomial relative likelihood function is <span class="math inline">\(\frac{\theta^y (1 - \theta)^{n - y}}{\hat \theta^y (1 - \hat \theta)^{n - y}}\)</span> where <span class="math inline">\(\hat \theta = \frac y n\)</span>.</p>
<p>What is the Y axis of the relative likelihood function? It's the <strong>likelihood</strong> of the X axis, not the probability of anything. Likewise, <span class="math inline">\(\theta\)</span> isn't a probability either, just an unknown constant. For example, a value of <span class="math inline">\(\theta\)</span> that makes <span class="math inline">\(R(\theta) = 0.6\)</span> means that <span class="math inline">\(\theta\)</span> is 0.6 times as likely as the maximum likelihood estimate.</p>
<p>A likelihood is different from a probability - <strong>likelihoods are the frequency of certain hypotheses out of all hypotheses, while probabilities are the frequency of certain results out of all results</strong>. Hypotheses are statements like &quot;the unknown parameter <span class="math inline">\(\theta\)</span> is 0.3&quot;, and results are something like &quot;we observed 300 of 1000 trials were successful&quot;.</p>
<p>Likewise, a probability function is what we use when <strong>we have a parameter value and want to describe what samples will look like</strong>, while the likelihood function is what we use when <strong>we have a sample and want to describe what the parameter value looked like</strong>.</p>
<p>The <strong>log likelihood function</strong> is just the log of the likelihood function, <span class="math inline">\(l(\theta) = \ln L(\theta)\)</span>. Clearly, it has its maximum at the same point, even though the shape is different.</p>
<p>We like the log likelihood function because it's often easier to differentiate, which makes it easier to find the maximum likelihood estimate. Since likelihood functions are often products of lots of factors, using the product rule over and over again quickly gets messy. The log likelihood function can often be written as multiple terms using the <span class="math inline">\(\ln a + \ln b = \ln(ab)\)</span> identity, after which we can differentiate individual terms really easily using the sum rule.</p>
<p>For example, the log likelihood function is <span class="math inline">\(l(\theta; y) = \ln \left(\theta^y (1 - \theta)^{n - y}\right) = y \ln \theta + (n - y) \ln (1 - \theta)\)</span>. Differentiating this is a piece of cake, and we get <span class="math inline">\(\frac{\dee}{\dee \theta} l(\theta) = \frac y \theta - \frac{n - y}{1 - \theta} = \frac{y - \theta n}{\theta(1 - \theta)}\)</span>. Solving <span class="math inline">\(\frac{y - \theta n}{\theta(1 - \theta)} = 0\)</span>, we get <span class="math inline">\(\theta = \frac y n\)</span>, as expected.</p>
<p>Suppose we have two independent data sets <span class="math inline">\(y, z\)</span> for two independent random variables <span class="math inline">\(Y, Z\)</span> that share a parameter <span class="math inline">\(\theta\)</span>. Clearly, <span class="math inline">\(P(Y = y, Z = z; \theta) = P(Y = y; Z = z; \theta)\)</span>, so the combined likelihood is <span class="math inline">\(L(\theta; y) L(\theta; z)\)</span>.</p>
<h1 id="section-8">23/1/17</h1>
<p>Suppose <span class="math inline">\(y_1, \ldots, y_n\)</span> is the number of events that occurred at day 1 to <span class="math inline">\(n\)</span>, respectively, and a Poisson distribution is appropriate for this dataset. What's the likelihood function for the Poisson distribution <span class="math inline">\(Y \sim \mathrm{Poisson}(\theta)\)</span>, where <span class="math inline">\(\theta\)</span> is the average rate of events?</p>
<p>Intuitively, we know that the average rate of accidents is <span class="math inline">\(\overline y\)</span>. Why is this?</p>
<p>Here, the likelihood function is <span class="math inline">\(L(\theta; y_1, \ldots, y_n)P(\text{observing the data } y_1, \ldots, y_n; \theta)\)</span> - the probability of observing the events we did for any given average event rate.</p>
<p>Let <span class="math inline">\(Y_i\)</span> be the number of events recorded on day <span class="math inline">\(i\)</span>. Then <span class="math inline">\(L(\theta; y_1, \ldots, y_n) = P(Y_1 = y_1, \ldots, Y_n = y_n; \theta)\)</span>. Since each event occurring is independent (one of the assumptions necessary to use the Poisson distribution), <span class="math inline">\(P(Y_1 = y_1, \ldots, Y_n = y_n; \theta) = P(Y_1 = y_1; \theta) \ldots P(Y_n = y_n; \theta) = \prod_{i = 1}^n \frac{\theta^{y_i} e^{-\theta}}{y_i!} = \frac{1}{\prod_{i = 1}^n y_i!} \theta^{\sum_{i = 1}^n y_i} e^{-n\theta}\)</span>.</p>
<p>To simplify the equation, we can use the fact that <span class="math inline">\(n\overline y = \sum_{i = 1}^n y_i\)</span>. Then, dropping the constant factors and substituting <span class="math inline">\(n \overline y\)</span> in, we get <span class="math inline">\(L(\theta; y_1, \ldots, y_n) = \theta^{n \overline y} e^{-n\theta}\)</span>. So, the log likelihood function is <span class="math inline">\(l(\theta; y_1, \ldots, y_n) = n \overline y \log \theta - n \theta\)</span>.</p>
<p>Differentiating, <span class="math inline">\(l&#39;(\theta) = \frac{n \overline y}{\theta} - n\)</span>, so if <span class="math inline">\(l&#39;(\theta) = 0\)</span>, <span class="math inline">\(\theta = \overline y\)</span>.</p>
<p>So <span class="math inline">\(\hat \theta = \overline y\)</span> - the sample mean. Also, the relative likelihood function is <span class="math inline">\(\frac{\theta^{n \overline y} e^{-n \theta}}{{\hat \theta}^{n \overline y} e^{-n \hat \theta}}\)</span>.</p>
<p>Recommended problem: course notes, example 2.2.4 (page 57-58)</p>
<p>Suppose we have a randomly selected sample <span class="math inline">\(y = \tup{y_1, \ldots, y_n}\)</span> from a random variable <span class="math inline">\(Y = \tup{Y_1, \ldots, Y_n}\)</span>. Since each <span class="math inline">\(Y_i\)</span> is independent of the others, <span class="math inline">\(L(\theta; y) = P(Y_1 = y_1, \ldots, Y_n = y_n) = P(Y_1 = y_1) \cdots P(Y_n = y_n)\)</span>.</p>
<h3 id="continuous-distributions">Continuous distributions</h3>
<p>For discrete distributions so far we had <span class="math inline">\(L(\theta; y) = P(Y = y; \theta)\)</span>. However, this isn't suitable for continuous random variables, since <span class="math inline">\(P(Y = y; \theta)\)</span> is always 0 (what's the probability that a dart hits a particular exact point?).</p>
<p>How do we construct a likelihood function? The basic idea is that whenever we're taking an observation in real life, we're actually rounding it to some number of decimal places. As a result, we can treat those observations as if they fall into lots of tiny intervals.</p>
<p>Basically, we can replace <span class="math inline">\(P(Y = y)\)</span> with the probability density function (PDF) <span class="math inline">\(f(y; \theta)\)</span>. So, <span class="math inline">\(L(\theta; y) = \prod_{i = 1}^n f(y_i; \theta)\)</span>.</p>
<p>What's the likelihood function for an exponential distribution <span class="math inline">\(Y \sim \mathrm{Exponential}(\theta)\)</span> for a dataset <span class="math inline">\(y_1, \ldots, y_n\)</span>, where <span class="math inline">\(\theta\)</span> is the mean rate of events? Note that the PDF is <span class="math inline">\(f(y; \theta) = \theta e^{-\frac y \theta}\)</span>.</p>
<p>Clearly, <span class="math inline">\(L(\theta; y) = \frac{e^{-\frac 1 \theta \sum_{i = 0}^n y_i}}{\theta} = \frac{e^{-\frac{n \overline y}{\theta}}}{\theta}\)</span>. Dropping the constant factors, we get <span class="math inline">\(e^{-n} e^{-\frac{n\overline y}{\theta}}\)</span>. So <span class="math inline">\(\hat \theta = \overline y\)</span>, the sample mean again.</p>
<p>What's the likelihood function for a Gaussian distribution <span class="math inline">\(Y \sim \mathrm{Guassian}(\mu, \sigma^2)\)</span> for a dataset <span class="math inline">\(y_1, \ldots, y_n\)</span>, where <span class="math inline">\(\mu, \sigma^2\)</span> are the mean and variance? Here, we have two parameters for our likelihood function, so we'll need to use multivariate calculus to maximize it.</p>
<p>Details are in the course notes, but <span class="math inline">\(\hat\mu = \overline y\)</span> (the sample mean), and <span class="math inline">\(\sigma^2 = \frac 1 n \sum_{i = 1}^n (y_i - \overline y)^2\)</span> (sort of like the sample variance, but with <span class="math inline">\(n\)</span> rather than <span class="math inline">\(n - 1\)</span> for the denominator).</p>
<p>Also, if we know one of the parameters, we can simply use a partial differential for the likelihood function to maximize the likelihood of the unknown parameter.</p>
<h1 id="section-9">25/1/17</h1>
<p>Midterm next tuesday. Midterm covers everything up to but not including Monday's lecture. Do the practice problems at the end of chapter 2.</p>
<p>The <strong>invariance property</strong> says that if <span class="math inline">\(\hat \theta\)</span> is the MLE of <span class="math inline">\(\theta\)</span>, then <span class="math inline">\(g(\hat \theta)\)</span> is the MLE of <span class="math inline">\(g(\theta)\)</span>. In other words, to find the maximum likelihood of a function, we just need to find the maximum likelihood of its parameter.</p>
<p>For example, the variance of the binomial distribution is <span class="math inline">\(\sigma^2 = n\theta(1 - \theta)\)</span>. Therefore, the MLE of the variance is <span class="math inline">\(n \hat \theta(1 - \hat \theta)\)</span></p>
<p>Aside, if <span class="math inline">\(Y_1, \ldots, Y_n\)</span> are independent random variables with <span class="math inline">\(E[Y_i] = \mu\)</span> and <span class="math inline">\(\mathrm{Var}(Y_i) = \sigma^2\)</span>. Then <span class="math inline">\(E[\overline Y] = \mu\)</span> and <span class="math inline">\(\mathrm{Var}(\overline Y) = \frac{\sigma^2}{n}\)</span>.</p>
<p>When we solve statistical problems, we usually start by fitting a model to the data. After we fit a model, however, we should always check that the model actually represents the distribution well. Some ways we can do this are: superimposing a PDF onto the relative frequency histogram and visually checking similarity, superimposing an empirical CDF on the theoretical CDF and visually checking similarity, checking the QQ-plot for straight lines, and comparing observed frequencies with expected frequencies predicted by the model.</p>
<p>The last one might warrant some explanation. One way to check model fit is to compare how often we see some samples to how often the model says we should see them. For example, for a Poisson model we would first fit it to the dataset using the sample mean as <span class="math inline">\(\theta\)</span>. Then, we can calculate the theoretical frequency for an interval <span class="math inline">\(nP(y_1 \le Y \le y_2)\)</span> predicted by the Poisson model, and then compare them with the actual number of times we observe values between <span class="math inline">\(y_1\)</span> and <span class="math inline">\(y_2\)</span>. We usually choose around 10-15 intervals for <span class="math inline">\(y_1\)</span> and <span class="math inline">\(y_2\)</span> such that each one contains at least one sample.</p>
<h1 id="section-10">27/1/17</h1>
<p>The <strong>PPDAC</strong> is an algorithm for designing statistical studies. It consists of the following steps:</p>
<ul>
<li>Problem - study objectives.
<ul>
<li>What's the target population?</li>
<li>The target population is the group of things we want the conclusions to apply to - this can be different from the study population (generally a superset), and it's what we think the researchers consider the conclusions apply to. The target population is generally somewhat up for interpretation. For example, if we wanted to see if R assignments in STAT231 help students understand the material, the units would be students and the target process would be students in STAT231 now and in the future.</li>
<li>What are the variates of interest? What are the units? These are often ill-defined, but we can figure out the</li>
<li>What are the questions we are trying to answer, in terms of attributes of the target population? We want to ask questions about the target population, and answer them using data from the study population</li>
<li>This step deals only with target populations - don't talk about samples here!</li>
<li>Problems fall into three types: descriptive (what is the value of some attribute?), causative (does A cause B?), and predictive (what would be the effect of X?).</li>
</ul></li>
<li>Plan - procedures for collecting data, whether the data will answer the problem, etc.
<ul>
<li>What's the study population?</li>
<li>Which units are available to study? How do we choose the units to study? With or without replacement? The <strong>sampling protocol</strong> is the procedure used to select a sample of units from the study population. The number of units selected is the <strong>sample size</strong>. The sampling protocol is often difficult to choose because getting a truly random sample is expensive and involved. Most studies just use things like online surveys, which means the sampling protocol is &quot;people who voluntarily take online surveys&quot;.</li>
<li>What variates will we collect for each unit?</li>
<li>What are possible study error? <strong>Study error</strong> is a systematic difference between a target population and a study population. The hard part of getting rid of study error is that we don't know if it's present or not. Statisticians use peer-review to avoid this. For example, if the study population is mice and the target population is humans, there are significant differences between mice and humans that we would have to account for.</li>
<li>An example of study error is a real study on male physicians where aspirin reduced heart attacks by 44%. Applying this to female patients who are potentially not physicians is potentially a study bias, if women or non-doctors react systematically differently to aspirin.</li>
<li>What are possible sources of sample error? <strong>Sample/sampling error</strong> is a systematic difference between the sample and the study population. The sampling protocol needs to be chosen carefully to avoid this.</li>
<li>An example of sample error is when a study uses an online survey - only people who are inclined to take online surveys will be part of the sample, and if those people are systematically different in relevant ways, we have a sample error.</li>
<li>Measurement often introduces <strong>measurement error</strong> - the difference between the true value of the variate and the observed value.</li>
<li><strong>Response bias</strong> is when study respondents systematically tend to give incorrect answers. For example, people tend to exaggerate their income on financial surveys, or lie about experiencing police mistreatment on studies about police behaviour.</li>
</ul></li>
<li>Data - actual collection of data.
<ul>
<li>Note that the sample size is defined before the study starts - if we originally start our study with 500 units, and 100 drop out by the end of the study, the sample size is still 500, even though we only end up with 400 values. Our analysis step should use a sample size of 500 and handle the missing values properly.</li>
<li>It's important to record the time and location of each measurement, and detail the measurement procedures unambiguously.</li>
<li>If any deviations from the measurement plan occur, it's important to make note of this, because they could affect the analyses.</li>
</ul></li>
<li>Analysis - extracting information out of the data.
<ul>
<li>This is what the entire rest of the course is about - estimation, hypothesis testing, etc.</li>
<li>This should include numerical/graphical summaries of the data.</li>
<li>An important step to selecting an appropriate statistical model, and checking the fit of the selected statistical model using things like QQ plots, comparing frequencies, and so on.</li>
<li>We actually need to consider two models: one model for variation of the attributes in the study population/process (Guassian/Binomial/etc.), and another which accounts for how data is collected (with/without replacement, how units are selected). In other words, one is targeting the target population, and the other one is targeting the study population.</li>
<li>For example, the first one might be &quot;let <span class="math inline">\(Y\)</span> be the BMI of a random male human chosen from the target population, <span class="math inline">\(Y\)</span> follows a normal distribution with unknown parameters <span class="math inline">\(\mu_T, \sigma_T\)</span> (the average and standard deviation in BMI in the target population)&quot;. For the second one we need to account for the fact that the target population is different from the study population, so it might be &quot;let <span class="math inline">\(Y\)</span> be the BMI of a random male human chosen from the study population, <span class="math inline">\(Y\)</span> follows a normal distribution with unknown parameters <span class="math inline">\(\mu, \sigma\)</span> (the average and standard deviation in BMI in the study population)&quot;. The first one is for the target population, while the second is for the study population.</li>
</ul></li>
<li>Conclusion - answering the problem, as well as the limitations of our answers.
<ul>
<li>Make sure to address any departures from the plan in the Data step that affect the Analysis step.</li>
<li>It's important to outline the limitations of the study - if the study didn't definitively answer the questions posed, why and how?</li>
</ul></li>
</ul>
<h1 id="section-11">30/1/17</h1>
<p>For example, a real Vitamin D study has the following PPDAC-like structure:</p>
<ul>
<li>Problem (objective) - would taking Vitamin D reduce the proportion of school aged children in Japan who contract influenza A?
<ul>
<li>The target population would be children of school age in Japan. We don't have to worry about things like children who can't take supplements, have allergies, or so on, because at this point it's too early to think about details that come up during the planning stage.</li>
<li>The variates we care about are probably whether the children get influenza (categorical, response variate), whether they get vitamin D (categorical, explanatory variate), plus a few secondary (less important) variates, like weight, height, etc.</li>
<li>The questions of interest might be &quot;what proportion of the target population would get influenza if they took vitamin D?&quot;, and &quot;what proportion of the target population would get influenza if they took the placebo?&quot;.</li>
<li>This is a causative problem, because we're trying to determine whether vitamin D has a causal relationship to influenza A.</li>
</ul></li>
<li>Plan (study design) - a randomized double-blind trial on schoolchildren comparing Vitamin D to a placebo. Schoolchildren in 12 hospitals in Tokyo (note that we are not doing this in all of Japan, just one city) in 2009 were asked to participate and followed for a few months. Double-blinding was performed by a computer system and placebos/supplements looked identical.
<ul>
<li>The study population is the children who participated in the study at pediatricians in the 12 hospitals in Tokyo in 2009.</li>
<li>Researchers can probably easily obtain access to the given study population since they work in the hospital, which is probably why they chose this study population.</li>
<li>The variates we collect are whether the children contacted influenza, whether they were taking Vitamin D or the placebo, age, weight, and other factors.</li>
<li>One possible source of study error is if only children who are already sick go to the hospital, while the target population isn't already sick.</li>
<li>The study recruited 430 children, but only 334 were followed to the end of the study. The sample size is 430, because that's the size of the sample at the beginning of the study. Using 334 as the sample size is wrong - consider what would happen if some children dropped out of the study because they had bad reactions to the vitamin D.</li>
<li>One possible source of sample error is that children who couldn't swallow pills or had allergies or didn't have consent from their parents were excluded from the study. If there was a systematic difference in those children, that would give us sample error.</li>
<li>One possible source of measurement error is false positives/negatives for the influenza test. The children were given a nose swab, which might give incorrect results in a small number of cases. ;wip: rest of the case study</li>
</ul></li>
<li>Conclusion
<ul>
<li>One possible limitation is that the study population is relatively small - it's hard to be really confident about the answers with this sample size for cases involving humans and drugs.</li>
</ul></li>
</ul>
<h1 id="tutorial">1/2/17 - Tutorial</h1>
<p>;wip: learn about chi squared and t squared distributions</p>
<h1 id="section-12">1/2/17</h1>
<p>Do chapter 3 end of chapter problems. Start reading chapter 4. Start doing assignment 2, and make sure to read the instructions about the ID carefully in the assignment template.</p>
<p>On assignments and exams, we'll often be given a plan, and asked to critique strengths and weaknesses, sources of error, identify target population, study population, variates of interest, units, etc.</p>
<p>For example, if we were conducting a study about financial habits of university students in Ontario, it would be a study error to only study UW students (because of the co-op program would affect student finances), and it would be a sample error to only sample UW students who were attending to a particular football game (because people who go to football games might systematically have different financial situations).</p>
<p>When running the study, we want to take as random a sample as possible. We might try sending out an email to everyone with a UW email, but then the students themselves choose whether they're in the sample, a possibly large source of sample error. We could try posting in the UW Facebook group, but then only people who use Facebook would see it. We might ask professors to put them on final exams, but some courses don't have final exams, for example. Instead, we might go to the university registrar, randomly select students by student ID, and then ask them to participate in a study with enough monetary incentive that they are mostly convinced to participate.</p>
<p>See section 3.3 of the course notes for a case study.</p>
<h1 id="section-13">3/2/17</h1>
<p>When we have a dataset and a Gaussian model is a good fit for the data, we usually estimate the parameter <span class="math inline">\(\mu\)</span> using the sample mean, and the parameter <span class="math inline">\(\sigma\)</span> with the sample standard deviation (even though it might be slightly off for a small population, since we divide by <span class="math inline">\(n - 1\)</span> rather than <span class="math inline">\(n\)</span>).</p>
<p>Note that <span class="math inline">\(\hat \theta\)</span> is really only an estimate - we would expect that it approaches the true value of <span class="math inline">\(\theta\)</span> for large sample sizes. How close would <span class="math inline">\(\hat \theta\)</span> be to <span class="math inline">\(\theta\)</span>? How certain is our estimate? To quantify this, we can use sampling distributions and point estimators.</p>
<p>Suppose for our study population we have observed data <span class="math inline">\(y_1, \ldots, y_n\)</span>, and the attribute of interest is fully represented by the parameter <span class="math inline">\(\theta\)</span>.</p>
<p>A <strong>point estimate</strong> for that parameter <span class="math inline">\(\theta\)</span> is a function <span class="math inline">\(\hat \theta = g(y_1, \ldots, y_n)\)</span>, used to estimate the unknown parameter <span class="math inline">\(\theta\)</span>. For example, a point estimate for <span class="math inline">\(\theta\)</span> in a Poisson distribution is <span class="math inline">\(\hat \theta = \overline y\)</span>, and a point estimate for <span class="math inline">\(\theta\)</span> in a Binomial distribution is <span class="math inline">\(\hat \theta = \frac {\sum y_i} n\)</span>.</p>
<p>Since the value of <span class="math inline">\(\hat \theta\)</span> is potentially going to be different for each possible sample we take, <span class="math inline">\(\hat \theta\)</span> has a distribution. In fact, we can represent the potential samples <span class="math inline">\(y_1, \ldots, y_n\)</span> we can draw with <span class="math inline">\(Y_1, \ldots, Y_n\)</span> - each unit in our sample can be thought of as being drawn from a given distribution.</p>
<p>We therefore associate the point estimate function with the <strong>point estimator</strong> function, <span class="math inline">\(\widetilde \theta = g(Y_1, \ldots, Y_n)\)</span>. Note that <span class="math inline">\(\widetilde \theta\)</span> is now a random variable, rather than a simple value (this is also not standard notation, by the way). For example, a point estimator for <span class="math inline">\(\theta\)</span> in a Poisson distribution is <span class="math inline">\(\widetilde \theta = \overline Y\)</span>, and a point estimator for <span class="math inline">\(\theta\)</span> in a Binomial distribution is <span class="math inline">\(\hat \theta = \frac {\sum Y_i} n\)</span>.</p>
<p>Recall that a random variable is simply a function. <span class="math inline">\(\widetilde \theta\)</span> is therefore a rule that tells us how to obtain an estimate of <span class="math inline">\(\theta\)</span> for an abstract sample <span class="math inline">\(Y_1, \ldots, Y_n\)</span>, while <span class="math inline">\(\hat \theta\)</span> is the value obtained using this rule for a concrete, observed sample <span class="math inline">\(y_1, \ldots, y_n\)</span>.</p>
<p>The method of maximum likelihood is one general method for getting point estimates/point estimators. Others include the method of moments and bayesian estimation.</p>
<p>The <strong>sampling distribution</strong> is the distribution of a point estimator <span class="math inline">\(\widetilde \theta\)</span> - it must have a probability function if it's discrete, and a probability density function if it's continuous.</p>
<p>How do we figure out what the sampling distribution is? Technically, we could figure out the sampling distribution with brute force, by enumerating every possible sample and getting the point estimate for each of them. However, this is generally not feasible. Instead, we can use the central limit theorem.</p>
<p>Recall the central limit theorem - if we average together enough iid (independent and identically distributed) random variables <span class="math inline">\(Y_1, \ldots, Y_n\)</span>, they start to resemble a Gaussian distribution where the mean is about <span class="math inline">\(E(Y_1) = \ldots = E(Y_n)\)</span> and the variance is <span class="math inline">\(\frac{\mathrm{Var}(Y_1)}{\sqrt{n}} = \ldots = \frac{\mathrm{Var}(Y_n)}{\sqrt{n}}\)</span>. &quot;Large enough&quot; is somewhat ill defined, and depends on the shape of the distributions, but symmetric distributions are faster.</p>
<p>If <span class="math inline">\(X \sim \mathrm{Poisson}(\mu_1), Y \sim \mathrm{Poisson}(\mu_2)\)</span>, then <span class="math inline">\(X + Y \sim \mathrm{Poisson}(\mu_1 + \mu_2)\)</span>. When <span class="math inline">\(\mu \ge 5\)</span>, a Poisson random variable will start to resemble a <span class="math inline">\(\mathrm{Guassian}(\mu, \mu)\)</span> distribution.</p>
<p>If <span class="math inline">\(X \sim \mathrm{Binomial}(n_1, \theta), Y \sim \mathrm{Binomial}(n_2, \theta)\)</span>, then <span class="math inline">\(X + Y \sim \mathrm{Binomial}(n_1 + n_2, \theta)\)</span>.</p>
<p>If <span class="math inline">\(Y_i \sim \mathrm{Gaussian}(\mu_i, \sigma_i^2), 1 \le i \le n\)</span>, then <span class="math inline">\(\sum a_i Y_i \sim \mathrm{Gaussian}(\sum a_i \mu_i, \sum a_i^2 \sigma_i^2), a_i \in \mb{R}\)</span>. In other words, any linear combination of Gaussian random variables also forms a Gaussian distribution. From this, we know that if <span class="math inline">\(\mu = \mu_1 = \ldots = \mu_n\)</span> and <span class="math inline">\(\sigma^2 = \sigma_1^2 = \ldots = \sigma_n^2\)</span> (all of the normal random variables are iid), then <span class="math inline">\(\overline Y \sim \mathrm{Gaussian}(\mu, \frac{\sigma^2}{n})\)</span>.</p>
<p>When we take a sample <span class="math inline">\(Y_1, \ldots, Y_n\)</span> from our population, each entry in the sample can be thought of as iid. Using this observation, we can approximate the mean of different distributions with:</p>
<ul>
<li>If <span class="math inline">\(Y \sim \mathrm{Binomial}(n, \theta)\)</span> (<span class="math inline">\(Y\)</span> of <span class="math inline">\(n\)</span> samples are successes) and <span class="math inline">\(n\)</span> is large, then <span class="math inline">\(Y \sim \mathrm{Gaussian}(n \theta, n \theta (1 - \theta))\)</span>.</li>
<li>If <span class="math inline">\(Y_i \sim \mathrm{Poisson}(\theta), 1 \le i\le n\)</span> (<span class="math inline">\(n\)</span> samples, yielding <span class="math inline">\(Y_i\)</span> events for each) and <span class="math inline">\(n\)</span> is large, then <span class="math inline">\(\overline Y \sim \mathrm{Gaussian}(\theta, \frac{\theta}{n})\)</span></li>
<li>If <span class="math inline">\(Y_i \sim \mathrm{Exponential}(\theta), 1 \le i\le n\)</span> (<span class="math inline">\(n\)</span> samples, taking <span class="math inline">\(Y_i\)</span> time before an event for each) and <span class="math inline">\(n\)</span> is large, then <span class="math inline">\(\overline Y \sim \mathrm{Gaussian}(\theta, \frac{\theta^2}{n})\)</span></li>
</ul>
<p>For more complicated cases, we can simulate experiments using software like R.</p>
<h1 id="section-14">6/2/17</h1>
<p>Tutorial test 2 is coming up, will include PPDAC question and estimators.</p>
<p>A few classes ago we conducted the diamond experiment to estimate an unknown attribute of the study population, the mean of the diamond values, Suppose we observe data <span class="math inline">\(y_1, \ldots, y_n\)</span>, and that we have a probability model with unknown parameter <span class="math inline">\(\theta\)</span>. Let <span class="math inline">\(\hat \theta = g(y_1, \ldots, y_n) = \overline y\)</span> be a point estimate of <span class="math inline">\(\theta\)</span>. How good is <span class="math inline">\(\hat \theta\)</span> as an estimate of <span class="math inline">\(\theta\)</span>? What is the uncertainty in the estimate? To help us answer these questions, we can use <strong>interval estimation</strong> rather than point estimation.</p>
<p>Recall that the sampling distribution is the distribution of a point estimator <span class="math inline">\(\widetilde \theta\)</span>. This is essentially the distribution of estimates of our unknown parameter <span class="math inline">\(\theta\)</span>, over all possible ways we could sample from the study population. We can approximate this by taking lots and lots of samples, estimating <span class="math inline">\(\theta\)</span> based on each sample, and form the distribution out of all the estimates we get.</p>
<p>The sampling distribution of a point estimator when we're trying to estimate the mean has <span class="math inline">\(E(\overline Y) = \mu\)</span> and <span class="math inline">\(\mathrm{Var}(\overline Y) = \frac{\sigma^2}{n}\)</span>.</p>
<p>Given a sampling distribution, one common question we use to quantify the uncertainty is &quot;what proportion of our point estimates within <span class="math inline">\(d\)</span> units of the true value of the unknown parameter?&quot;. We can easily answer this using histograms of sampling distributions. Factors that affect this uncertainty are the sample size, variance, and the shape of the sampling distribution.</p>
<p>In a real empirical study, we only get one sample - we can't just repeatedly resample. How do we quantify uncertainty only from one sample?</p>
<p>Suppose we have a dataset <span class="math inline">\(y_1, \ldots, y_n\)</span> sampled from our study population with an assumed model <span class="math inline">\(Y_i \sim \mathrm{Gaussian}(\mu, 0.5), 1 \le i \le n\)</span> (each number in our sample is in a Gaussian distribution where <span class="math inline">\(\sigma = 0.5\)</span>). Suppose we're using <span class="math inline">\(\widetilde \mu = \overline Y = \frac{\sum Y_i}{n}\)</span> to estimate <span class="math inline">\(\mu\)</span> using our dataset. Clearly, we don't need to simulate the sampling distribution, because we know it's <span class="math inline">\(\widetilde \mu = \mathrm{Gaussian}(\mu, \frac{\sigma}{\sqrt{n}})\)</span>. Now, how often is this estimate <span class="math inline">\(\hat \mu\)</span> within 0.1 of the true value of <span class="math inline">\(\mu\)</span>?</p>
<p>In other words, we want <span class="math inline">\(P(\abs{\overline Y - \mu} \le 0.1)\)</span> given our distribution <span class="math inline">\(\mathrm{Gaussian}(\mu, \frac{0.5}{\sqrt{n}})\)</span>, but we don't know <span class="math inline">\(\mu\)</span>. However, if we divide both sides by <span class="math inline">\(\sigma\)</span>, we get <span class="math inline">\(P(\abs{\overline Y - \mu} \le 0.1) = P(\frac{\abs{\overline Y - \mu}}{\frac{0.5}{\sqrt{n}}} \le \frac{0.1}{\frac{0.5}{\sqrt{n}}})\)</span>. Note that this is now a normalized Guassian distribution, so we have <span class="math inline">\(P(\abs{Z} \le \frac{0.1}{\frac{0.5}{\sqrt{n}}}) = P(\frac{\sqrt{n}}{5} \le Z \le \frac{\sqrt{n}}{5})\)</span>, which we can then compute using the Gaussian CDF table.</p>
<h1 id="tutorial-1">8/2/17 - Tutorial</h1>
<p>Practice problem about identifying the PPDAC steps in a news article about a study. The example is about an observational study (researchers didn't attempt to control any variates), about the relationship about women who attend science fairs/competitions at a young age and whether they go into STEM careers later on, trying to establish whether there's a causative relationship. Data was collected by an in-class survey, though we don't know if students were required to complete it.</p>
<p>The explanatory variate here would be whether each student is participating in science fairs/competitions, a categorical variate. The response variate is whether each student ended up going into a STEM career, and is also a categorical variate. The analysis concluded that women who participated in those things were 2.7 times more likely to go into STEM careers than women who did not - this is a relative risk numerical summary.</p>
<p>The sample is a subset of the study population, but the study population is not necessarily a subset of the target population.</p>
<h1 id="section-15">8/2/17</h1>
<p>To indicate uncertainty in an estimate, we define an <strong>interval estimate</strong> of <span class="math inline">\(\theta\)</span> given observed data <span class="math inline">\(y\)</span> to be <span class="math inline">\([L(y), U(y)]\)</span>, where <span class="math inline">\(L\)</span> and <span class="math inline">\(U\)</span> are generic interval bounding functions.</p>
<p>For example, the interval estimate <span class="math inline">\([\overline y - 2 \frac{\sigma}{\sqrt n}, \overline y + 2 \frac{\sigma}{\sqrt n}]\)</span> summarizes the uncertainty in the point estimate <span class="math inline">\(\hat \mu\)</span>. As the sample size increases, the confidence interval shrinks, converging on <span class="math inline">\(\overline y\)</span>.</p>
<p>The <span class="math inline">\(100p\)</span> percent <strong>likelihood interval</strong> is defined as <span class="math inline">\(\set{\theta : R(\theta; y) \ge p}\)</span>, where <span class="math inline">\(\theta\)</span> is the unknown parameter, <span class="math inline">\(R(\theta)\)</span> is the relative likelihood function, and <span class="math inline">\(0 \le p \le 1\)</span>. Note that this isn't necessarily a single interval unless <span class="math inline">\(R(\theta; y)\)</span> is unimodal, but all of the likelihood functions we'll look at in this course will be unimodal.</p>
<p>The log relative likelihood function is defined as <span class="math inline">\(r(\theta; y) = \ln R(\theta; y)\)</span>, and we use it because it's often easier to compute <span class="math inline">\(r(\theta; y)\)</span> than <span class="math inline">\(R(\theta; y)\)</span>. The log relative likelihood function often looks pretty quadratic, while the relative likelihood function often looks like a bell curve. We sometimes write the <span class="math inline">\(100p\)</span> percent likelihood interval as <span class="math inline">\(\set{\theta : r(\theta; y) \ge \ln p}\)</span>.</p>
<p>It's important to note that the likelihood interval is always a function of the observed data. The likelihood interval is an example of an interval estimate.</p>
<p>The wider the likelihood interval, the more uncertainty about our estimate, and the less we know about the value of the unknown parameter. Values in the 10% likelihood interval are generally considered <strong>plausible</strong>. Values inside the 50% likelihood interval are generally considered <strong>very plausible</strong>. Values outside the 1% likelihood interval are generally considered <strong>very implausible</strong>.</p>
<p>We usually find the likelihood interval either by solving the inequality, or by drawing a horizontal line at <span class="math inline">\(y = p\)</span> on the graph of the relative likelihood function and reading off the X-axis values that are above the line.</p>
<p>How often does the likelihood interval contain the true parameter value? An <strong>interval estimator</strong> can answer this question, much like point estimators did for point estimates.</p>
<p>We again represent our potential dataset as random variables <span class="math inline">\(Y = Y_1, \ldots, Y_n\)</span> (as opposed to the dataset <span class="math inline">\(y = y_1, \ldots, y_n\)</span>). Then, we have the <strong>interval estimator</strong> <span class="math inline">\([L(Y), U(Y)]\)</span> where ;wip: likelihood estimator slides</p>
<p>To determine how good the interval estimator is we want to compute <span class="math inline">\(P(L(Y) \le \theta \le U(Y))\)</span>. This gives us the concept of the <strong>confidence interval</strong> - what's the <strong>narrowest</strong> possible interval <span class="math inline">\([L(Y), U(Y)]\)</span> that has a <span class="math inline">\(100p\)</span> percent chance of containing the true value of <span class="math inline">\(\theta\)</span>?</p>
<p>The <span class="math inline">\(100p\)</span> percent <strong>confidence interval</strong> is the interval estimate <span class="math inline">\([L(y), U(y)]\)</span> such that <span class="math inline">\(P(L(y) \le \theta \le U(y)) = p\)</span>, where <span class="math inline">\(\theta\)</span> is the unknown parameter and <span class="math inline">\(0 \le p \le 1\)</span>. The center of the confidence interval is usually the point estimate for the mean (i.e., the sample mean), so the confidence interval is often just the point estimate plus or minus the <strong>margin of error</strong>. Also, <span class="math inline">\(p\)</span> is called the <strong>confidence coefficient</strong>.</p>
<p>It's not valid to say that a 99% confidence interval means that we'd expect the true value to have a 99% probability of being in the interval, because the true value of <span class="math inline">\(\theta\)</span> is just a constant, so doesn't have a distribution or probability of being in there - it's either in the interval or not in the interval, and we're only doing the study once. However, we can say that a 99% confidence interval means that we would be 99% <strong>confident</strong> that the true value is in the interval - if we did the experiment 100 times, we'd expect the interval we got to contain the true value 99 times.</p>
<p>Turns out the confidence interval can be pretty easily calculated. We calculate the sample mean <span class="math inline">\(\overline y\)</span> and sample standard deviation <span class="math inline">\(s\)</span>, and then choose a value. Then, the confidence interval is <span class="math inline">\([\overline y - z^* \frac{s}{\sqrt n}, \overline y + z^* \frac{s}{\sqrt n}]\)</span></p>
<h1 id="section-16">10/2/17</h1>
<p>Suppose <span class="math inline">\(Y_1, \ldots, Y_n\)</span> is a random sample of a <span class="math inline">\(\mathrm{Gaussian}(\mu, 1)\)</span> distribution. Then <span class="math inline">\(E(Y_i) = \mu\)</span> and <span class="math inline">\(\mathrm{Var}(Y_i) = 1\)</span>.</p>
<p>Consider the interval <span class="math inline">\([\overline Y - \frac{1.96}{\sqrt n}, \overline Y + \frac{1.96}{\sqrt n}]\)</span>. What's the probability that <span class="math inline">\(\mu\)</span> is in this interval, or <span class="math inline">\(P(\overline Y - \frac{1.96}{\sqrt n} \le \mu \le \overline Y + \frac{1.96}{\sqrt n})\)</span>?</p>
<p>This is a somewhat unusual form for a probability calculation, because the random variable appears on the range bounds and the inner value <span class="math inline">\(\mu\)</span> is a constant. To solve this using the techniques we learned in STAT230, we want to rewrite this so the random variable is in the middle.</p>
<p>Subtracting <span class="math inline">\(\overline Y\)</span> and multiplying by <span class="math inline">\(\sqrt{n}\)</span>, we get <span class="math inline">\(P(-1.96 \le \sqrt{n} (\mu - \overline Y) \le 1.96)\)</span>. We can rewrite this as <span class="math inline">\(P(-1.96 \le \frac{\overline Y - \mu}{\frac{1}{\sqrt{n}}} \le 1.96)\)</span>.</p>
<p>Clearly, <span class="math inline">\(\overline Y \sim \mathrm{Gaussian}(\mu, \sigma = \frac{1}{\sqrt n})\)</span>, because it's the average of <span class="math inline">\(n\)</span> samples. Therefore, <span class="math inline">\(\frac{\overline Y - \mu}{\frac{1}{\sqrt{n}}} \sim \mathrm{Gaussian}(0, 1)\)</span>, so we can find the value of <span class="math inline">\(P(-1.96 \le \frac{\overline Y - \mu}{\frac{1}{\sqrt{n}}} \le 1.96)\)</span> using the Gaussian CDF table.</p>
<p>Clearly, the width of the table is <span class="math inline">\(2 \frac{1.96}{\sqrt n}\)</span>, which decreases as <span class="math inline">\(n\)</span> increases.</p>
<p>If we had a sample of size 16 with mean 10.4, we would have a 95% confidence interval <span class="math inline">\([10.4 - \frac{1.96}{\sqrt{16}}, 10.4 + \frac{1.96}{\sqrt{16}}]\)</span>. This tells us that we are 95% confident that <span class="math inline">\(10.4 - \frac{1.96}{\sqrt{16}} \le \mu \le 10.4 + \frac{1.96}{\sqrt{16}}\)</span>. Note that this is not the same thing as <span class="math inline">\(P(10.4 - \frac{1.96}{\sqrt{16}} \le \mu \le 10.4 + \frac{1.96}{\sqrt{16}}) = 0.95\)</span> - all of the values in that statement are constants, so the probability is either 0 or 1, we just don't know which it is. If we repeated the experiment many times, the range would include the true value <span class="math inline">\(100p\)</span> percent of the time (though we very rarely get the chance to repeat our experiments).</p>
<p>A <strong>pivotal quantity</strong> <span class="math inline">\(Q(Y; \theta)\)</span> is a function of a random sample <span class="math inline">\(Y\)</span> and unknown parameter <span class="math inline">\(\theta\)</span> such that the distribution of <span class="math inline">\(Q(Y; \theta)\)</span> is completely known. That makes <span class="math inline">\(P(a \le Q(Y; \theta) \le b)\)</span> depends only on <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>. The pivotal quantity must be a function of the data and unknown parameter, so <span class="math inline">\(\overline Y\)</span> is not a pivotal quantity while <span class="math inline">\(\overline Y - \mu\)</span> is.</p>
<p>In this course we won't need to find pivotal quantities on our own, but we should be able to show why a given function is a pivotal quantity. We also need to be able to construct a confidence interval using a pivotal quantity.</p>
<p>To use a pivotal quantity to find a <span class="math inline">\(100p\)</span> percent confidence interval, we determine values <span class="math inline">\(a, b\)</span> such that <span class="math inline">\(P(a \le Q(Y; \theta) \le b) = p\)</span>. Then, we rewrite <span class="math inline">\(a \le Q(Y; \theta) \le b\)</span> as <span class="math inline">\(L(Y) \le \theta \le U(Y)\)</span>. Then, we use the fact that <span class="math inline">\(p = P(a \le Q(Y; \theta) \le b) = P(L(Y) \le \theta \le U(Y))\)</span> to solve for <span class="math inline">\(L(Y), U(Y)\)</span>. The interval <span class="math inline">\([L(Y), U(Y)]\)</span> is then the <span class="math inline">\(100p\)</span> percent confidence interval.</p>
<p>For <span class="math inline">\(Y \sim \mathrm{Gaussian}(\mu, \sigma)\)</span> where <span class="math inline">\(\sigma\)</span> is known, we use the normal table to find a value such that <span class="math inline">\(P(-a \le Z \le a) = p\)</span> (equivalently, <span class="math inline">\(P(Z \le a) = \frac{1 + p}{2}\)</span>, which should be easier to look up in the normal table), and then the <span class="math inline">\(100p\)</span> percent confidence interval for samples is is <span class="math inline">\([\overline y - a \frac{\sigma}{\sqrt n}, \overline y + a \frac{\sigma}{\sqrt n}]\)</span>. This is also the case for other distributions if the sample is large enough, because of the central limit theorem.</p>
<p>Common confidence intervals we care about are 90%, 95%, and 99%, with values of <span class="math inline">\(a\)</span> equal to 1.645, 1.960, and 2.576 respectively.</p>
<h1 id="section-17">13/2/17</h1>
<p>Suppose we have an empirical study of hand/leg dimensions of students in class. The target population might be the set of students taking STAT231, while the sample population might be the set of students that showed up to the class that the study was taking place in. This is an observational study, since even though the experimenter asked us to do something, it was not something that would affect our variate of interest.</p>
<p>Tutorial test 2 on wednesday. Check information on LEARN.</p>
<p>Recall the definition of a pivotal quantity: a function of the data and the unknown parameter such that the distribution is fully known. Recall that to turn a pivotal quantity into a confidence intervals, we find the bounds of the pivotal quantity, and then transform the expression until we get the form <span class="math inline">\(P(L(Y) \le \theta \le U(Y)) = p\)</span>, or the confidence interval <span class="math inline">\([L(Y), U(Y)]\)</span>.</p>
<p>Pivotal quantities are often hard to find analytically. However, we often use <strong>approximate pivotal quantities</strong> that apply for values as the sample size gets large, due to the central limit theorem.</p>
<p>Recall that if <span class="math inline">\(Y \sim \mathrm{Binomial}(n; \theta)\)</span>, then <span class="math inline">\(E(Y) = \theta\)</span> and <span class="math inline">\(\mathrm{Var}(Y) = \frac 1 n \theta(1 - \theta)\)</span>. By the central limit theorem, the approximate sampling distribution of <span class="math inline">\(\widetilde \theta = \frac Y n\)</span> is <span class="math inline">\(\frac{\widetilde \theta - \theta}{\sqrt{\frac 1 n \theta(1 - \theta)}}\)</span> for a large enough value of <span class="math inline">\(n\)</span>.</p>
<p>However, this formula has too many <span class="math inline">\(\theta\)</span> - the function is too complicated to use as a pivotal quantity. Instead, we can replace some of our <span class="math inline">\(\theta\)</span> isntances with <span class="math inline">\(\widetilde \theta\)</span>, to get <span class="math inline">\(Q_n = \frac{\widetilde \theta - \theta}{\sqrt{\frac 1 n \widetilde \theta(1 - \widetilde \theta)}}\)</span>. This is an approximate pivotal quantity for the binomial distribution.</p>
<p>We can use this to construct <strong>approximate confidence intervals</strong>, using the usual way of converting a pivotal quantity into a confidence interval. Given a large <span class="math inline">\(n\)</span>, a <span class="math inline">\(100p\)</span> percent approximate confidence interval for <span class="math inline">\(\theta\)</span> is <span class="math inline">\(\theta \pm a \sqrt{\frac 1 n \hat \theta (1 - \hat \theta)}\)</span> where <span class="math inline">\(a\)</span> is defined the same way as for the Gaussian confidence interval above.</p>
<p>;wip: confidence interval formula for Poisson</p>
<p>We now have two types of interval estimates - <span class="math inline">\(100p\)</span> percent likelihood intervals, and <span class="math inline">\(100p\)</span> percent confidence intervals. There's actually an inportant connection between the two types of interval estimates, which we'll look at later.</p>
<p>Researchers use confidence intervals to choose how large to make their samples. They compute the sample size their studies need based on a worst-case confidence interval width (and available resources like budget and manpower).</p>
<h1 id="section-18">15/2/17</h1>
<p>Most interval estimates have the form <span class="math inline">\(\text{point estimate} \pm \text{uncertainty}\)</span>.</p>
<p>Likelihood intervals are just a different way to write approximate confidence intervals. To show this, we use the <span class="math inline">\(\chi^2(k = 1)\)</span> distribution, which looks like a downward up-concave slope.</p>
<p>The <strong>maximum likelihood estimator</strong> of <span class="math inline">\(\theta\)</span> is <span class="math inline">\(A = -2 \ln \frac{L(\theta; Y)}{L(\widetilde \theta; Y)}\)</span>. <span class="math inline">\(A\)</span> is a random variable that depends on the random data <span class="math inline">\(Y = Y_1, \ldots, Y_n\)</span>, and is also called the likelihood ratio statistic (likelihood ratio random variable).</p>
<p>We won't show why this works in this class, but it turns out that with a large enough <span class="math inline">\(n\)</span>, <span class="math inline">\(A\)</span> roughly has a <span class="math inline">\(\chi^2(1)\)</span>. In other words, with a large enough sample, we can use the Chi-squared distribution to approximate the likelihood ratio statistic. That means <span class="math inline">\(A\)</span> is approximately a pivotal quantity for <span class="math inline">\(\theta\)</span>.</p>
<p>The &quot;large enough&quot; value of <span class="math inline">\(n\)</span> depends on the distributions of the individual samples, just like the central limit theorem - 30 is usually a good value for this course. The above statement is actually quite similar to the central limit theorem, and the likelihood ratio statistic is somewhat similar to the relative likelihood function.</p>
<p>We can then use the likelihood ratio statistic as an approximate pivotal quantity to create <span class="math inline">\(100p\)</span> percent confidence intervals. First, we find a <span class="math inline">\(c\)</span> such that <span class="math inline">\(p = P(W \le c)\)</span> where <span class="math inline">\(W \sim \chi^2(1)\)</span>. Now, we note that <span class="math inline">\(P(W \le c) \approxeq P(-2 \ln \frac{L(\theta; Y)}{L(\widetilde \theta; Y)} \le c)\)</span> for large enough <span class="math inline">\(n\)</span>.</p>
<p>Therefore, the approximate <span class="math inline">\(100p\)</span> percent confidence interval is <span class="math inline">\(\set{\theta: -2 \ln \frac{L(\theta; y)}{L(\hat \theta; y)} \le c}\)</span>, or <span class="math inline">\(\set{\theta: -2 \ln R(\theta; y) \le c}\)</span>, or <span class="math inline">\(\set{\theta: R(\theta; y) \ge e^{-\frac c 2}}\)</span>. But, note that this is just a likelihood interval!</p>
<p>Note that if <span class="math inline">\(W \sim \chi^2(1)\)</span> and <span class="math inline">\(Z \sim \mathrm{Gaussian}(0, 1)\)</span>, then <span class="math inline">\(P(W \le c) = 2 P(Z \le \sqrt c) - 1\)</span> - we can convert between Chi-squared and Guassian CDF values exactly. This is useful for when our Chi-squared tables don't go up to our desired values.</p>
<p>For example, since <span class="math inline">\(0.95 = 2P(Z \le 1.96) - 1\)</span>, <span class="math inline">\(0.95 = P(W \le 1.96^2)\)</span> and an approximate confidence interval for <span class="math inline">\(\theta\)</span> is <span class="math inline">\(\set{\theta: R(\theta) \ge e^{-\frac{1.96^2}{2}}} = \set{\theta: R(\theta) \ge 0.147}\)</span>.</p>
<p>Therefore, the 14.7% likelihood interval is the same thing as an approximate 95% confidence interval (for large enough sample sizes)! In the same way, the 4% likelihood interval is a 99% confidence interval, a 10% likelihood interval is a 97% confidence interval, and a 26% likelihood interval is a 90% confidence interval.</p>
<p>Note that as <span class="math inline">\(p\)</span> increases a likelihood interval gets narrower, while a confidence interval gets wider.</p>
<p>So to convert a <span class="math inline">\(100p\)</span> percent likelihood interval into a confidence interval (given a large enough sample size), we let <span class="math inline">\(c = -2 \ln p\)</span>, and then look up the value <span class="math inline">\(k\)</span> such that <span class="math inline">\(k = P(W \le c)\)</span> in the Chi-squared CDF table, or <span class="math inline">\(k = 2P(Z \le \sqrt c) - 1\)</span> in the Gaussian CDF table. ;wip</p>
<p>Consider the binomial distribution. We can now find the 95% approximate confidence interval with the 14.7% likelihood interval, or using the other approach <span class="math inline">\(\overline y \pm \hat \theta \sqrt{\frac{\hat \theta (1 - \hat \theta)}{n}}\)</span>. The former is based on the Chi-squared approximation, while the latter is based on the central limit theorem approximation.</p>
<p>The former tends to be a better approximation for smaller sample sizes, but is often harder to calculate as well. In fact, likelihood interval estimates are at least as good as pivotal-quantity-based estimates, in general.</p>
<h1 id="section-19">17/2/17</h1>
<p>We already looked at the confidence interval for the mean of a Gaussian distribution when <span class="math inline">\(\sigma\)</span> is known, but what about when we don't know <span class="math inline">\(\sigma\)</span>?</p>
<p>Clearly, <span class="math inline">\(\overline Y\)</span> is a point estimator for <span class="math inline">\(\mu\)</span>, and <span class="math inline">\(\mathrm{Var}(Y)\)</span> is a point estimator for <span class="math inline">\(\sigma^2\)</span> (the sample variance). Note that the point estimator for <span class="math inline">\(\sigma\)</span> is NOT the MLE, <span class="math inline">\(\frac{}{}\)</span>. We use the sample variance instead of the MLE because <span class="math inline">\(E(\mathrm{Var}(Y)) = \sigma^2\)</span> - it's an <strong>unbiased estimator</strong>. ;wip: it's not Var(Y), copy the real one and compare to MLE, write in terms of estimator <span class="math inline">\(S\)</span></p>
<p>The confidence interval for <span class="math inline">\(\mu\)</span> is still <span class="math inline">\(\overline y \pm a \frac{\sigma}{\sqrt n}\)</span>, but we have two unknowns now, which makes it hard to make a pivotal quantity. Instead, we replace <span class="math inline">\(\sigma\)</span> with the estimator <span class="math inline">\(S\)</span> to get the pivotal quantity <span class="math inline">\(\frac{\overline Y - \mu}{\frac{S}{\sqrt n}}\)</span>. It turns out that this pivotal quantity exactly has the <span class="math inline">\(\mathrm{student-t}(n - 1)\)</span> distribution. The student-t distribution has <span class="math inline">\(n - 1\)</span> degrees of freedom rather than <span class="math inline">\(n\)</span> because <span class="math inline">\(\sum (Y - \overline Y) = 0\)</span> - when we estimated the mean, we essentially locked away one degree of freedom.</p>
<p>This is a pivotal quantity for <span class="math inline">\(\mu\)</span> because <span class="math inline">\(S\)</span> is a random variable that's a function of the data, so the whole thing is a function of the data and the unknown parameter <span class="math inline">\(\mu\)</span> whose distribution is completely known.</p>
<p>Therefore, we have the <span class="math inline">\(100p\)</span> confidence interval for <span class="math inline">\(\mu\)</span> <span class="math inline">\(p = P(-a \le \frac{\overline Y - \mu}{\frac{S}{\sqrt n}} \le a)\)</span>, where <span class="math inline">\(a\)</span> is a value such that <span class="math inline">\(P(T \le a) = \frac{1 + p}{2}\)</span> where <span class="math inline">\(T \sim \mathrm{student-t}(n - 1)\)</span>. Rearranging the pivotal quantity, we get <span class="math inline">\(p = P(\overline Y - a\frac{S}{\sqrt n} \le \mu \le \overline Y + a \frac{S}{\sqrt n})\)</span>, or a confidence interval of <span class="math inline">\([\overline y - a\frac{s}{\sqrt n}, \overline y + a\frac{s}{\sqrt n}]\)</span>. This is actually an exact confidence interval - we didn't make any distribution approximations!</p>
<p>To summarize, the <span class="math inline">\(100p\)</span> percent confidence interval of a Guassian distribution where both <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span> are unknown is <span class="math inline">\(\overline y \pm a\frac{s}{\sqrt n}\)</span> where <span class="math inline">\(a\)</span> is a value such that <span class="math inline">\(P(T \le a) = \frac{1 + p}{2}\)</span> and <span class="math inline">\(T \sim \mathrm{student-t}(n - 1)\)</span> and $s = $ ;wip</p>
<h1 id="section-20">27/2/17</h1>
<p>Midterm next week, covers everything up to and including section 5.2. Review of content before reading week.</p>
<p>How do we find a <span class="math inline">\(100p\)</span> percent confidence interval for <span class="math inline">\(\sigma^2\)</span> in a Gaussian distribution where <span class="math inline">\(\mu\)</span> is unknown?</p>
<p>For a sample of size 25 drawn from a Gaussian distribution with unknown <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma^2\)</span>, then <span class="math inline">\(\overline y \pm a \frac{s}{5}\)</span> is a 95% confidence interval for <span class="math inline">\(\mu\)</span> if <span class="math inline">\(P(T \le a) = 0.975\)</span> and <span class="math inline">\(T \sim t(24)\)</span>.</p>
<p>If <span class="math inline">\(Y_1, \ldots, Y_N\)</span> is a random normal sample and we don't know <span class="math inline">\(\mu\)</span>, then <span class="math inline">\(\frac{(n - 1)S^2}{\sigma^2} = \frac{\sum (Y_i - \overline Y)}{\sigma^2} \sim \chi^2(n - 1)\)</span>, and we can use this as a pivotal quantity for <span class="math inline">\(\sigma\)</span>. If we do know <span class="math inline">\(\mu\)</span>, then <span class="math inline">\(\frac{\sum (Y_i - \mu)}{\sigma^2} \sim \chi^2(n)\)</span> instead (there's another degree of freedom that wasn't removed by the subtracting of <span class="math inline">\(\overline Y\)</span>). We prefer <span class="math inline">\(S^2\)</span> over the MLE for the sample variance because <span class="math inline">\(E(S^2) = \sigma^2\)</span>, while <span class="math inline">\(E(\mathrm{Var}(Y)) \ne \sigma^2\)</span>.</p>
<p>Note that <span class="math inline">\(\frac{\sum (Y_i - \mu)^2}{\sigma^2} \sim \chi^2(n)\)</span>. This is <span class="math inline">\(n\)</span> rather than <span class="math inline">\(n - 1\)</span> because in the previous formula, we removed one degree of freedom by replacing <span class="math inline">\(\mu\)</span> by <span class="math inline">\(\overline Y\)</span>.</p>
<p>Clearly, <span class="math inline">\(W = \frac{(n - 1)S^2}{\sigma^2}\)</span> is a pivotal quantity, so to construct a <span class="math inline">\(100p\)</span> percent confidence interval, we want the values of <span class="math inline">\(a, b\)</span> such that <span class="math inline">\(p = P(a \le W \le b)\)</span>. In other words, we want <span class="math inline">\(P(W \le a) = \frac{1 - p}{2}\)</span> and <span class="math inline">\(P(W \le b) = \frac{1 - p}{2} + p = \frac{1 + p}{2}\)</span>. We can then look up the corresponding values of <span class="math inline">\(a, b\)</span> in the chi-squared distribution table.</p>
<p>Rearranging the formula, we get <span class="math inline">\(p = P(\frac{(n - 1)S^2}{b} \le \sigma^2 \le \frac{(n - 1)S^2}{a})\)</span>, the <span class="math inline">\(100p\)</span> percent confidence interval for <span class="math inline">\(\sigma^2\)</span> when <span class="math inline">\(\mu\)</span> is unknown is <span class="math inline">\([\frac{(n - 1)S^2}{b}, \frac{(n - 1)S^2}{a}]\)</span>.</p>
<h1 id="section-21">1/3/17</h1>
<p>In north america, the criminal court results in either &quot;the defendant is guilty&quot; or &quot;the defendant is not guilty&quot;. We start by assuming the defendant is innocent, and then the prosecuter tries to find enough evidence to show that the hypothesis, that the defendant is innocent, is not plausible. Meanwhile. the defence tries to show that the hypothesis is plausible. The defendant is guilty only if we can show that the hypothesis isn't plausible - we don't have to prove the hypothesis. There are two types of possible errors here: convicting an innocent person, and failing to convict a guilty person.</p>
<p>Statistical tests work in a way similar to these criminal trials - we start from the <strong>null hypothesis</strong>, and then design an experiment to show that our <strong>alternative hypothesis</strong> is more plausible than the null hypothesis.</p>
<p>The null hypothesis is the &quot;default option&quot;, and is usually denoted <span class="math inline">\(H_0\)</span>. For example, in a drug trial, it would be &quot;this new drug does not work&quot;, or &quot;this previous drug is the best option for treating this disease&quot;. Likewise, if we were trying to prove someone had ESP, the null hypothesis would be &quot;the person does not have ESP&quot;. Our experiment should be designed to falsify the null hypothesis and support our own hypothesis. The null hypothesis.</p>
<p>Suppose we want to tell if someone has ESP. To do this, we might design an experiment to see if someone can guess a coin flip value better than by chance. If the person guesses right 50% of the time, they probably don't have ESP. If they guess right 100%, there's probably some evidence that they do. How do we measure the strength of this evidence?</p>
<p>For this purpose, we need a <strong>test statistic/discrepancy measure</strong> - a function of the data <span class="math inline">\(D = g(Y)\)</span> that measures the degree that the data <span class="math inline">\(Y\)</span> agrees with the null hypothesis <span class="math inline">\(H_0\)</span>, where smaller values indicate more agreement between the data and the null hypothesis. In this course, we're generally given the test statistic, but we will need to be able to explain why a particular test statistic makes sense.</p>
<p>For example, a test statistic for the ESP experiment with 25 trials might be <span class="math inline">\(D = \abs{Y - E(Y)} = \abs{Y - 12.5}\)</span>, because small values indicate that it's more likely the person is randomly guessing, while larger values indicate that it's less likely. Suppose we get a value of <span class="math inline">\(y = 15\)</span>, so <span class="math inline">\(d = 2.5\)</span>. Assuming the null hypothesis is true, what's probability of getting <span class="math inline">\(d = 2.5\)</span>?</p>
<p>Clearly, <span class="math inline">\(P(D \ge 2.5 \mid H_0) = P(\abs{Y - 12.5} \ge 2.5 \mid H_0)\)</span> where <span class="math inline">\(Y \sim \mathrm{Binomial}(25, 0.5)\)</span>. So <span class="math inline">\(P(D \ge 2.5 \mid H_0) = P(Y \le 10) + P(Y \ge 15) = P(Y \le 10) + (1 - P(Y \le 15)) = 0.4244\)</span>. So if the null hypothesis is true, we would expect <span class="math inline">\(D \ge 2.5\)</span> to happen in 42% of our experiments.</p>
<h1 id="section-22">3/3/17</h1>
<p>Read sections 5.1 to 5.2 in the course notes.</p>
<p>To statistically test a hypothesis <span class="math inline">\(H\)</span>, we assume that we're testing a null hypothesis <span class="math inline">\(H_0\)</span> using data <span class="math inline">\(Y\)</span>. Then, we set a discrepency measure <span class="math inline">\(D(Y)\)</span> for which large values of <span class="math inline">\(D\)</span> are less consistent with <span class="math inline">\(H_0\)</span>, where <span class="math inline">\(d = D(Y)\)</span> is the observed value of <span class="math inline">\(D\)</span>. We then compute the p-value <span class="math inline">\(P(D \ge d; H_0)\)</span>.</p>
<p>A p-value's meaning depends on the test statistic and the data. It's defined as <span class="math inline">\(P(D \ge d; H_0)\)</span> - the probability of observing the discrepency measure that we did, assuming the null hypothesis is true. Essentially, for a p-value, if we ran the experiment many times, we would expect that <strong>that percentage of the time, the experiment would be at least as unusual as the result we got</strong>, by the definition of unusual set by the discrepency statistic.</p>
<p>For example, the ESP experiment had the null hypothesis <span class="math inline">\(\theta = 0.5\)</span> where <span class="math inline">\(Y \sim \mathrm{Binomial}(25, \theta)\)</span>, and the discrepency measure <span class="math inline">\(d = D(Y) = \abs{Y - n \theta} = \abs{Y - 12.5}\)</span>. Suppose <span class="math inline">\(y = 15\)</span>, so <span class="math inline">\(P(D \ge d) = P(\abs{Y - 12.5} \ge 15; \theta = 0.5) \approxeq\)</span>.</p>
<p>If the p-value is small, then either the null hypothesis is true and we only got what we did by chance, or the null hypothesis is false. A small p-value gives strong evidence against the null hypothesis, while a large p-value says that there's little evidence that the null hypothesis is false.</p>
<p>p-values less than or equal to 0.001 is very strong evidence of null hypothesis being false, 0.001 to 0.01 is strong evidence, 0.01 to 0.05 is evidence, 0.05 to 0.1 is some evidence, and greater values are no evidence.</p>
<p>For example, if we have a p-value 0.001, then the event &quot;observing a value as more extreme than the value observed given that the null hypothesis is true&quot; happens only about one time out of 1000</p>
<p>For binomial models, <span class="math inline">\(\abs{Y - E(Y)}\)</span> is a reasonable discrepency measure, because it's 0 when the results are the same as the null hypothesis predicts, and increases as the results differ more.</p>
<p>Suppose we have hypothesis <span class="math inline">\(\mu = 5\)</span> for a Gaussian model, where . How do we calculate p-values for that?</p>
<p>Recall that the pivotal quantity is <span class="math inline">\(\frac{\sqrt n (\overline Y - \mu)}{S} \sim \mathrm{student-t}(n - 1)\)</span>. A sensible discrepency measure for <span class="math inline">\(\mu\)</span> would be <span class="math inline">\(\frac{\abs{\sqrt n (\overline Y - \mu)}}{S}\)</span>, because . ;wip: why is the absolute value of the pivotal quantity a good test statistic</p>
<p>p-values are often accompanied by confidence intervals - when p-values are small for a null hypothesis, the confidence interval shows where the actual value falls.</p>
<p>Basically, if we use the same pivotal quantity to construct a confidence interval and a p-value <span class="math inline">\(p\)</span>, then the hypothesis <span class="math inline">\(\theta = k\)</span> is inside a <span class="math inline">\(100q\)</span> percent confidence interval if and only if <span class="math inline">\(p \ge 1 - q\)</span>. For example, if <span class="math inline">\(p \ge 0.05\)</span>, then <span class="math inline">\(\theta\)</span> is inside the 95% confidence interval.</p>
<h1 id="section-23">6/3/17</h1>
<p>The p-value for testing <span class="math inline">\(H_0\)</span>, a hypothesis <span class="math inline">\(\theta = k\)</span>, is greater or equal to a p-value <span class="math inline">\(q\)</span> if and only if <span class="math inline">\(\theta = k\)</span> is in the <span class="math inline">\(100(1 - q)\)</span> percent confidence interval for <span class="math inline">\(\theta\)</span>, if we use the same pivotal quantity. In other words, the confidence interval is roughly the opposite of the p-value.</p>
<p>There's a difference between statistical and practical significance. If you toss a fair coin millions of times, you can almost certainly find evidence against the hypothesis <span class="math inline">\(\theta = 0.5\)</span>. If we calibrated a scale and noticed that it was consistently biased 1% below the true weight, then it would have strong evidence that the scale wasn't perfectly fair, but in practice a 1% interval doesn't have any practical significance for most purposes.</p>
<p>Suppose we have a random Gaussian sample <span class="math inline">\(Y_1, \ldots, Y_n \sim \mathrm{Gaussian}(\mu, \sigma)\)</span>, and we're trying to test the hypothesis <span class="math inline">\(H_0\)</span>, which is <span class="math inline">\(\sigma^2 = k^2\)</span>. Recall that we have the pivotal quantity <span class="math inline">\((n - 1)\frac{S^2}{\sigma^2} \sim \chi^2(n - 1)\)</span>. If we assume the hypothesis, <span class="math inline">\((n - 1)\frac{S^2}{\sigma^2} = (n - 1)\frac{S^2}{k^2} \sim \chi^2(n - 1)\)</span>.</p>
<p><span class="math inline">\(S^2\)</span> is the point estimator for <span class="math inline">\(\sigma^2\)</span>.</p>
<p>For normal data, we'll use the test statistic <span class="math inline">\(U = (n - 1)\frac{S^2}{k^2}\)</span> (the absolute value of the pivotal quantity). This is a reasonable test statistic because the expected value is the expected value of a <span class="math inline">\(\chi^2(n - 1)\)</span> random variable, so <span class="math inline">\(n - 1\)</span>. That means <span class="math inline">\(E(\frac{S^2}{k^2}) = 1\)</span>, so the expected value of <span class="math inline">\(S^2\)</span> is <span class="math inline">\(k^2\)</span>. In other words, if the hypothesis is true, then the value of the test statistic is close to the mean <span class="math inline">\(n - 1\)</span>. If the hypothesis is false and <span class="math inline">\(\sigma^2\)</span> is far away from <span class="math inline">\(k^2\)</span>, then the test statistic increases in value.</p>
<p>If <span class="math inline">\(P(U \le u) &lt; 0.5\)</span>, then the p-value is <span class="math inline">\(2P(U \le u)\)</span>. If <span class="math inline">\(P(U \ge u) &lt; 0.5\)</span>, then the p-value is <span class="math inline">\(2P(U \ge u)\)</span>, where <span class="math inline">\(U \sim \chi^2(n - 1)\)</span> (the value that is between 0 and 1 is the actual p-value, the other value is discarded).</p>
<h1 id="section-24">8/3/17</h1>
<p><strong>Likelihood ratio test statistic</strong> for a single parameter: <span class="math inline">\(R(\theta)\)</span> is close to 1 if and only if <span class="math inline">\(-2 \ln \frac{L(\theta)}{L(\hat \theta)}\)</span> is close to 0. Likewise, <span class="math inline">\(R(\theta)\)</span> is close to 0 if and only if <span class="math inline">\(-2 \ln \frac{L(\theta)}{L(\hat \theta)}\)</span> is large. We can therefore use the likelihood ratio as a test statistic, because it gets smaller as the value of <span class="math inline">\(\theta\)</span> becomes more likely.</p>
<p>Recall that <span class="math inline">\(\Lambda(\theta) = -2 \ln \frac{L(\theta)}{L(\hat \theta)} \sim \chi^2(1)\)</span>, so we can use this to construct our p-values with <span class="math inline">\(P(\Lambda(Y) \ge \Lambda(y); H_0)\)</span>.</p>
<p>So for binomial data <span class="math inline">\(Y \sim \mathrm{Binomial}(n, \theta)\)</span>, <span class="math inline">\(L(\theta) = \theta^y(1 - \theta)^{n - 1}\)</span>. So <span class="math inline">\(-2 \ln \frac{\theta^y(1 - \theta)^{n - 1}}{\hat \theta^y(1 - \hat \theta)^{n - 1}} \sim \chi^2(1)\)</span>.</p>
<p>Recall that we also have the asymptotic normal statistic with <span class="math inline">\(2(1 - P(Z \le ))\)</span>. Both approximations are used in this course. ;wip: slide before binomial example</p>
<h2 id="multivariate-data">Multivariate data</h2>
<p>Suppose we take a sample of 30 students from this term's STAT231 and their final grades in STAT231 and STAT230, and we want to look at the relationship between a student's STAT230 mark and their STAT231 mark. This is clearly bivariate data, the units are students, and the variates are final grades in STAT230 and STAT231. The study population might be those students taking STAT231 now, the sample is those 30 students we got data for, and the target population might be all STAT230/STAT231 students now and into the future. Some of the summaries we care about are sample correlation coefficient, scatter plots, and relative risk.</p>
<p>The explanatry variate here is the STAT230 grade, and the response variate is the STAT231 grade. To model the data, we'll need a new model we haven't looked at yet. We're mainly interested in answering questions like, &quot;how do I predict my STAT231 grade given my STAT230 grade?&quot;.</p>
<h1 id="section-25">10/3/17</h1>
<p>Machine learning is just statistics. Our datasets become &quot;training data&quot;, predictive experiments become &quot;regressions&quot;, and fitting models becomes &quot;training models&quot;.</p>
<p>Recall that the sample correlation is defined as <span class="math inline">\(S_{x, y} = \sum (x_i - \overline x)(y_i - \overline y)\)</span>. It represents the degree of the linear relationship between two variables.</p>
<p>The <strong>method of least squares</strong> is used to draw a line of best fit. Essentially, we're trying to minimise the sum of squares of the difference between each point and our line, on the Y axis - given a bivariate sample <span class="math inline">\(d = \tup{x_1, y_1}, \ldots, \tup{x_n, y_n}\)</span> and a line <span class="math inline">\(\beta n + \alpha\)</span>, we want to minimise <span class="math inline">\(\sum_{\tup{x_i, y_i} \in d} \left( y_i - (\beta x_i + \alpha) \right)^2\)</span>. Note that this is not the same thing as minimizing the sum of the absolute value of the differences - we might get a different line if we do that.</p>
<p>Note that if you swap <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>, you can potentially get a different answer - the method of least squares depends on the order of the variates. <span class="math inline">\(x_i\)</span> should be the explanatory variate, and <span class="math inline">\(y_i\)</span> should be the response variate - the line of best fit is used to predict how <span class="math inline">\(y_i\)</span> should behave given <span class="math inline">\(x_i\)</span>.</p>
<p>To minimize this sum of squares of vertical distances, we can set its derivative with respect to <span class="math inline">\(\alpha\)</span> to 0 and solve for <span class="math inline">\(\alpha\)</span>, do the same for <span class="math inline">\(\beta\)</span> and solve for <span class="math inline">\(\beta\)</span>, then use the solution as critical points test them to find minima. Turns out, if we solve for this, we get <span class="math inline">\(\hat \beta = \frac{S_{x, y}}{S_{x, x}}\)</span> and <span class="math inline">\(\hat \alpha = \overline y - \hat beta \overline x\)</span>.</p>
<p>Recall that the ;wip: slide relating r to alpha and beta</p>
<p>We now have the line of best fit <span class="math inline">\(\hat \beta n + \hat \alpha\)</span>. Clearly, this lets us predict <span class="math inline">\(y_i\)</span> given <span class="math inline">\(x_i\)</span>, just by evaluating the line at a certain <span class="math inline">\(x\)</span> value. But how good of a prediction does this line make? So far, we haven't defined a model, so we don't have anything to evaluate abstractly.</p>
<p>We need a statistical model to get an interval estimate for each prediction the line makes.</p>
<p>Consider the students who got exactly 75 in STAT230 as a population. If we let <span class="math inline">\(Y\)</span> be a random variable representing the STAT231 grade of those students. Clearly, we could use a Gaussian model to represent <span class="math inline">\(Y\)</span>, so <span class="math inline">\(Y \sim G(\mu, \sigma)\)</span>. If we generalize this to all students, we might say that the mean <span class="math inline">\(\mu\)</span> of that Gaussian distribution could be represented as a linear function <span class="math inline">\(\mu(x) = \alpha + \beta x\)</span>, and <span class="math inline">\(\sigma\)</span> is just a constant. In other words, we have <span class="math inline">\(Y_i \sim \mathrm{Gaussian}(\alpha + \beta x_i, \sigma)\)</span>, all independently.</p>
<h1 id="section-26">13/3/17</h1>
<p>The line of best fit with the Gaussian model is known as <strong>simple linear regression model</strong>, because it's the simplest way to model and predict bivariate data. We're assuming that the variance is the same for each <span class="math inline">\(x_i\)</span>, so the entire model has only the unknown parameters <span class="math inline">\(\alpha, \beta, \sigma\)</span>.</p>
<p>Our goal for using Gaussian models is to be able to construct confidence intervals for the model predictions.</p>
<p>We could also assume that the variance is a function of <span class="math inline">\(x_i\)</span>, but this would be a lot harder to work with. We'll now look at graphical ways to assess whether it's reasonable to assume that the variance is constant.</p>
<p>Consider the parameter <span class="math inline">\(\mu(x) = \alpha + \beta x\)</span>. <span class="math inline">\(\beta\)</span> represents the change in mean STAT231 mark given a one mark increase in STAT230 mark. <span class="math inline">\(\alpha\)</span> represents the STAT231 mark for students that got a STAT230 mark of 0 (in this case this parameter is not really of interest, because it's not possible to get a 0 in STAT230). <span class="math inline">\(\sigma\)</span> represents the variability in the response variate <span class="math inline">\(Y_i\)</span> for each explanatory variable <span class="math inline">\(X_i\)</span>.</p>
<p>Since it's a Gaussian, we get the likelihood function <span class="math inline">\(L(\alpha, \beta; \tup{x_1, y_1}, \ldots, \tup{x_n, y_n}) = \prod \exp\left(-\frac{1}{2\sigma^2} (y_i - \alpha - \beta x_i)\right)^2 = \exp\left(-\frac{1}{2\sigma^2} \sum (y_i - \alpha - \beta x_i)^2\right)\)</span>, assuming <span class="math inline">\(\sigma\)</span> is known.</p>
<p>Maximizing the likelihood function is equivalent to minimizing <span class="math inline">\(\sum (y_i - \alpha - \beta x_i)^2\)</span>, because maximizing <span class="math inline">\(\exp(-f(x))\)</span> is equivalent to minimizing <span class="math inline">\(f(x)\)</span>. In other words, <strong>the line of least squared error is also the maximum likelihood estimate for the line</strong>.</p>
<p>So the values <span class="math inline">\(\hat \beta = \frac{S_{x, y}}{S_{x, x}}, \hat \alpha = \overline y - \hat \beta \overline x\)</span> we got above, assuming the Gaussian models are reasonable.</p>
<p>We have the point estimate <span class="math inline">\(\hat \beta\)</span>, but what's the distribution of the point estimator?</p>
<p>Clearly, <span class="math inline">\(\widetilde \beta = \frac{S_{X, Y}}{S_{X, X}} = \frac{1}{S_{x, x}} \sum (x_i - \overline x)(Y - \overline Y) = \frac{1}{S_{x, x}} \sum \frac{x - \overline x}{S_{x, x}} Y_i\)</span>.</p>
<p>Clearly, <span class="math inline">\(S_{X, Y} = \sum (x_i - \overline x)(Y - \overline Y) = \sum (x - \overline x) Y_i - \overline Y \sum (x_i - \overline x) = \sum (x - \overline x) Y_i - \overline Y \times 0 = \sum (x - \overline x) Y_i\)</span>.</p>
<p>Since <span class="math inline">\(\sum (x_i - \overline x) = \sum x_i - n\overline x = 0\)</span>, <span class="math inline">\(\sum (x_i - \overline x)(Y - \overline Y) = \sum (x - \overline x) Y_i\)</span>.</p>
<p>So <span class="math inline">\(\widetilde \beta = \sum \frac{x_i - \overline x}{S_{x, x}} Y_i\)</span>, which is a linear combination of Gaussians. We know that a linear combination of Guassian variables is still a Gaussian variable, where the means and variances add up. So <span class="math inline">\(\widetilde \beta \sim G(E(\widetilde \beta), \sigma(\widetilde \beta))\)</span>.</p>
<p>Clearly, <span class="math inline">\(E(\widetilde \beta) = \sum \frac{x_i - \overline x}{S_{x, x}} E(Y_i)\)</span>. Since <span class="math inline">\(E(Y_i) = \alpha + \beta x_i\)</span>, <span class="math inline">\(E(\widetilde \beta) = \frac{1}{S_{x, x}} \left(\alpha \sum (x_i - \overline x) + \beta \sum (x_i - \overline x) x_i\right) = \frac{1}{S_{x, x}} \left(\beta \sum (x_i - \overline x) x_i\right) = \beta\)</span> ;wip: what's that last part</p>
<h1 id="section-27">15/3/17</h1>
<p>Clearly, <span class="math inline">\(\mathrm{Var}(\widetilde \beta) = \mathrm{Var}(\sum \frac{x_i - \overline x}{S_{x, x}} Y_i) = \sum \frac{(x_i - \overline x)^2}{S_{x, x}^2} \mathrm{Var}(Y_i) = \sum \frac{\sigma^2}{(S_{x, x})^2} \sum (x_i - \overline x)^2\)</span>. Since <span class="math inline">\(\sum (x_i - \overline x)^2 = S_{x, x}\)</span>, we get <span class="math inline">\(\mathrm{Var}(\widetilde \beta) = \frac{\sigma^2}{S_{x, x}}\)</span>.</p>
<p>So knowing <span class="math inline">\(\mu\)</span> and<span class="math inline">\(\sigma\)</span>, we now know that <span class="math inline">\(\widetilde \beta \sim G(\beta, \frac{\sigma}{\sqrt{S_{x, x}}})\)</span>.</p>
<p>Since <span class="math inline">\(Y_i \sim G(\mu, \sigma)\)</span>, <span class="math inline">\(\widetilde \mu = \overline Y \sim G(\mu, \frac{\sigma}{\sqrt n})\)</span>.</p>
<p>We want to use <span class="math inline">\(\frac{\widetilde \beta - \beta}{\sigma / \sqrt{S_{x, x}}}\)</span> as a pivotal quantity. However, we still don't know the distribution of <span class="math inline">\(\sigma\)</span> - we need an estimator for <span class="math inline">\(\sigma\)</span>.</p>
<p>One approach is to use the MLE of the variance - <span class="math inline">\(\hat \sigma^2 = \frac{1}{n} \sum (y_i - \hat \alpha - \hat \beta x_i)\)</span>. However, this is a biased estimate. Instead, we'll use <span class="math inline">\(s_e^2 = \frac{1}{n - 2} \sum(y_i - \hat \alpha - \hat \beta x_i)\)</span> - an unbiased estimate of the variance (i.e., <span class="math inline">\(E(S_e^2) = \sigma^2\)</span>). There's a proof for this, but it's out of scope for this course.</p>
<p>Now consider what happens when we replace <span class="math inline">\(\sigma\)</span> with its unbiased estimate: <span class="math inline">\(\frac{\widetilde \beta - \beta}{s_e / \sqrt{S_{x, x}}} \sim t(n - 2)\)</span>. This is a <strong>pivotal quantity for <span class="math inline">\(\beta\)</span></strong>, which we can use to construct confidence intervals for <span class="math inline">\(\beta\)</span> and test hypotheses like <span class="math inline">\(\beta = k\)</span>.</p>
<p>The <span class="math inline">\(100p\)</span> percent confidence interval for <span class="math inline">\(\beta\)</span> is <span class="math inline">\(\hat \beta \pm as_e \frac{1}{\sqrt{S_{x, x}}}\)</span>, where <span class="math inline">\(P(T \le a) = \frac{1 + p}{2}\)</span>. Notice that because the pivotal quantity is the student-t distribution, which is symmetric, the confidence interval is centred at <span class="math inline">\(\hat \beta\)</span>.</p>
<p>Usually we're interested in proving or disproving the null hypothesis: &quot;there is no relationship between these two variables&quot;. To do this, we use the null hypothesis <span class="math inline">\(\beta = 0\)</span> - this means that the explanatory variable doesn't affect the response variable at all. Because it's so important, the hypothesis <span class="math inline">\(\beta = 0\)</span> is known as the &quot;hypothesis of no relationship&quot;.</p>
<p>If there's a very small p-value, we have very strong evidence against the hypothesis of no relationship, which implies that there is a relationship between the two variables.</p>
<h1 id="section-28">17/3/17</h1>
<p>Recall that simple linear regression has <span class="math inline">\(x\)</span>, the explanatory variate, and <span class="math inline">\(y\)</span>, the response variate, where <span class="math inline">\(x\)</span> is assumed to be known and <span class="math inline">\(y\)</span> is the experimental result. In the simple linear regression model, we assume that each <span class="math inline">\(y\)</span> value has a Gaussian distribution based on constant values of <span class="math inline">\(x\)</span>.</p>
<p>We now have a confidence interval for <span class="math inline">\(\beta\)</span>. Using the invariance property of MLEs, we can now easily find a point estimate for <span class="math inline">\(\mu(x) = \alpha + \beta x\)</span>, as <span class="math inline">\(\hat \mu = \alpha + \hat \beta x\)</span>. However, how do we get a confidence interval?</p>
<p>First, we need the sampling distribution of <span class="math inline">\(\widetilde \mu(x) = \widetilde \alpha + \beta x\)</span>. Turns out, we can write this as <span class="math inline">\(\widetilde \mu(x) = \sum (\frac 1 n + (x_i - \overline x \frac{x_i - \overline x}{S_{X, X}}))Y_i\)</span>. Turns out that this is a sum of Gaussian variables, and if we do the algebra, we get <span class="math inline">\(\widetilde \mu(x) \sim G(\mu(x), \sigma \sqrt{\frac 1 n + \frac{(x - \overline x)^2}{S_{x, x}}})\)</span>.</p>
<p>If we know <span class="math inline">\(\sigma\)</span>, then <span class="math inline">\(\widetilde \mu(x) = \sigma + \widetilde \beta x\)</span>, and <span class="math inline">\(\hat a \pm a s_e \sqrt{\frac 1 n + \frac{(x - \overline x)^2}{S_{X, X}}}\)</span> is a confidence interval for <span class="math inline">\(\mu(x)\)</span>. If we don't, then <span class="math inline">\(\widetilde \mu(x) = \widetilde \sigma + \widetilde \beta x\)</span>, and <span class="math inline">\(\hat a \pm a \sigma \sqrt{\frac 1 n + \frac{(x - \overline x)^2}{S_{X, X}}}\)</span> is a confidence interval for <span class="math inline">\(\mu(x)\)</span>.</p>
<p>For <span class="math inline">\(\alpha\)</span>, we likewise have the confidence interval <span class="math inline">\(\hat a \pm a s_e \sqrt{\frac 1 n + \frac{(x - \overline x)^2}{S_{X, X}}}\)</span>.</p>
<p>Note that the confidence interval for <span class="math inline">\(\mu(x)\)</span> is the confidence interval for the general population - the average for an unknown constant <span class="math inline">\(x\)</span>. However, suppose we wanted ;wip: what's the distinction between invididual vs aggregate mu?</p>
<p>;wip: prediction intervals</p>
<h1 id="section-29">20/3/17</h1>
<p>The simple linear regression model we looked at, where the response variates have Gaussian distributions that are functions of the explanatory variate, are in general called Gaussian response models.</p>
<p>A <strong>Gaussian response model</strong> is a model where <span class="math inline">\(Y_i \sim G(\mu(x_i), \sigma)\)</span> for <span class="math inline">\(1 \le i \le n\)</span>. In this model, <span class="math inline">\(x_i\)</span> is assumed to be a known constant for all <span class="math inline">\(i\)</span>, and <span class="math inline">\(E(Y_i) = \mu(x_i)\)</span>, while <span class="math inline">\(\sqrt{\mathrm{Var}(Y_i)} = \sigma\)</span> doesn't depend on any <span class="math inline">\(x_i\)</span>.</p>
<p>We can also write <span class="math inline">\(Y_i = \mu(x_i) + R_i\)</span>, where <span class="math inline">\(R_i \sim G(0, \sigma)\)</span>. Here <span class="math inline">\(x_i\)</span> is the deterministic component, while <span class="math inline">\(R_i\)</span> is the stochastic/error component. For model checking we mostly care about the error component.</p>
<p><strong>Linear regression models</strong> are Gaussian response models where <span class="math inline">\(\mu(x_i)\)</span> has the form <span class="math inline">\(\beta_0 + \sum \beta_i x_i\)</span>. Here, the values of <span class="math inline">\(\beta_i\)</span> are <strong>regression coefficients</strong>, and the explanatory variates <span class="math inline">\(x_i\)</span> are also known as covariates.</p>
<p>Linear regression models assume that:</p>
<ol style="list-style-type: decimal">
<li>The Gaussian model is a good fit for the response variates.</li>
<li><span class="math inline">\(E(Y_i) = \mu(x)\)</span> is a linear function of their corresponding <span class="math inline">\(x_i\)</span> values.</li>
<li>Each <span class="math inline">\(Y_i\)</span> has a Gaussian distribution with a standard deviation that doesn't depend on the covariates <span class="math inline">\(x_i\)</span> (assumption of constant variance).</li>
</ol>
<p>We must always check the model assumptions. We can check property 2 using something like a scatter plot, but what about the other two? For these we can use <strong>residuals</strong> - <span class="math inline">\(\hat r_i = y_i - \hat \mu_i\)</span>, where <span class="math inline">\(\hat \mu_i = \hat \alpha + \hat \beta_i x_i\)</span></p>
<p>The idea behind residuals is that they're observations drawn from <span class="math inline">\(R_i\)</span>, the error variables. These residuals are supposed to behave like a random variable <span class="math inline">\(R_i \sim G(0, \sigma)\)</span>. Therefore, the sum of the residuals should be 0.</p>
<p>Additionally, a <strong>residual plot</strong> containing <span class="math inline">\(\hat r_i\)</span> with respect to <span class="math inline">\(x_i\)</span>. If the model assumptions hold, we should see a random scattering of points in a band around the horizontal <span class="math inline">\(\hat r_i = 0\)</span> line, with no obvious pattern. This lets us check if assumption 2 and 3 holds.</p>
<p>Another type of residual plot, also used to check property 2 and 3, is to scatter plot <span class="math inline">\(\hat r_i, \hat \mu_i\)</span> together. We should also see a similar belt around the horizontal <span class="math inline">\(\hat r_i = 0\)</span> line, with no obvious pattern.</p>
<p>A third type of residual plot is a QQ plot showing the sample quantiles versus the Gaussian quantiles. This lets us check if assumption 1 holds.</p>
<p>If there is a pattern in the Y-axis location on the plot, it suggests that the second assumption, that <span class="math inline">\(E(Y_i) = \mu(x_i)\)</span>, might not be appropriate. For example, if the residuals plot curves upward, a quadratic regression might be more appropriate.</p>
<h1 id="section-30">22/3/17</h1>
<p>If there is a pattern in the spread in the Y-axis on the plot, it suggests that the third assumption, that the variance is constant, might not be appropriate. For example, if the residuals plot fans out toward the right, it seems like the variance increases as the X-axis value increases.</p>
<p>(In-class bean counting experiment)</p>
<p>Suppose we want to study whether hand spans of female students enrolled in STAT231 in Fall 2016 is different on average from the hand spans of male students in that same class.</p>
<p>First, we need a model. Let <span class="math inline">\(Y_{1, i}\)</span> be the handspan of the <span class="math inline">\(i\)</span>th male student, and <span class="math inline">\(Y_{2, i}\)</span> the handspan of the <span class="math inline">\(i\)</span> female student. Based on the given histograms and QQ plots, a Gaussian model seems to be reasonable for these - <span class="math inline">\(Y_{1, i} \sim G(\mu_1, \sigma)\)</span> and <span class="math inline">\(Y_{2, i} \sim G(\mu_2, \sigma)\)</span>.</p>
<p>This is called a two-sample Gaussian problem - two datasets where each data point independently has a Gaussian distribution, with two different means for each dataset, and we're assuming that the variance is the same for both. This is a special case of the Gaussian response model.</p>
<p>A point estimator for the difference in average hand span is <span class="math inline">\(\widetilde \mu_1 - \widetilde \mu_2 = \overline Y_1 - \overline Y_2\)</span>. We can also take the sample variance for one of the populations to get an estimate for <span class="math inline">\(\sigma^2\)</span> (and check our model assumptions by comparing it to the sample variance for the other population); we'd probably pick the bigger population since that would give a more precise estimate.</p>
<p>A <strong>pooled estimator</strong> for <span class="math inline">\(\sigma^2\)</span> in the two-sample Gaussian problem is simply a weighted average of the estimators for each population in the problem - a weighted average of the sample variances, weighted by the number of observations in each sample.</p>
<h1 id="section-31">24/3/17</h1>
<p>Note that <span class="math inline">\(S_p^2 = \frac{1}{n_1 + n_2 - 2}\left((n_1 - 1)S_1^2 + (n_2 - 1)S_2^2\right)\)</span>, where <span class="math inline">\(S_1 = \frac 1 {n_1 - 2} \sum (Y_{1, i} - \overline Y_1)^2\)</span> and <span class="math inline">\(S_2 = \frac 1 {n_2 - 2} \sum (Y_{2, i} - \overline Y_2)^2\)</span>. This is a pooled estimator for <span class="math inline">\(\sigma^2\)</span> for the two-sample Gaussian model.</p>
<p>Note that the <span class="math inline">\(\frac{\overline Y_1 - \overline Y_2 - (\mu_1 - \mu_2)}{\sigma \sqrt{\frac 1 {n_1} + \frac 1 {n_2}}} \sim G(0, 1)\)</span>, because we have a linear combination of Gaussian random variables.</p>
<p>Since we don't know <span class="math inline">\(\sigma\)</span>, we can replace it with an unbiased estimator to get <span class="math inline">\(\frac{\overline Y_1 - \overline Y_2 - (\mu_1 - \mu_2)}{S_p \sqrt{\frac 1 {n_1} + \frac 1 {n_2}}} \sim t(n_1 + n_2 - 2)\)</span>. We can use this as a pivotal quantity to construct a confidence interval. ;wip: why is this a pivotal quantity? it's not even a function of sigma</p>
<p>Since <span class="math inline">\(S_p^2 = \frac{1}{n_1 + n_2 - 2}\left((n_1 - 1)S_1^2 + (n_2 - 1)S_2^2\right)\)</span>, we can find a pivotal quantity for <span class="math inline">\(\sigma^2\)</span> as <span class="math inline">\(\frac{(n_1 + n_2 - 2)S_p^2}{\sigma^2} \sim \chi^2(n_1 + n_2 - 2)\)</span>.</p>
<p>Using the same approach as for previous pivotal quantities, and the fact that, a <span class="math inline">\(100p\)</span> percent confidence interval is <span class="math inline">\(\overline y_1 + \overline y_2 \pm a s_p \sqrt{\frac 1 {n_1} + \frac 1 {n_2}}\)</span>, where <span class="math inline">\(P(T &lt; a) = \frac{1 + p}{2}\)</span> and <span class="math inline">\(T \sim t(n_1 + n_2 - 2)\)</span>.</p>
<p>If <span class="math inline">\(n_1\)</span> and <span class="math inline">\(n_2\)</span> are large enough (at least 30 entries each), the student-t distribution approaches the Gaussian distribution, so we can approximately say that <span class="math inline">\(\frac{\overline Y_1 - \overline Y_2 - (\mu_1 - \mu_2)}{S_p \sqrt{\frac 1 {n_1} + \frac 1 {n_2}}} \sim G(0, 1)\)</span>.</p>
<p>The bean experiment from yesterday is a <strong>paired experiment</strong>, because observations were deliberately paired up to eliminate factors like finger size, agility, and so on (each left hand observation was paired with the right hand observation from the same person). This reduces the variance (compared to, say, mixing all of the observations up) whenever the covariance is positive between the two samples. We don't do this if the covariance is negative, because then they're negatively correlated, so the resulting variance would actually be higher.</p>
<p>For two-sample models, we often care about <span class="math inline">\(Y_1 - Y_2\)</span>. Since <span class="math inline">\(Y_1 - Y_2\)</span> is a single sample, we can then apply single-sample model techniques. For our bean experiment, since <span class="math inline">\(Y_1 \sim G(\mu_1, \sigma)\)</span> and <span class="math inline">\(Y_2 \sim G(\mu_2, \sigma)\)</span>, <span class="math inline">\(\overline Y_1 - \overline Y_2 \sim G(\mu_1 + \mu_2, \sigma \sqrt{\frac 1 {n_1} + \frac 1 {n_2}})\)</span>.</p>
<p>The result of the experiment showed that there's strong evidence against the null hypothesis that the true value of the average bean counts are the same between both hands. This difference might be of practical significance if dealing with things like playing an instrument, designing hand tools, and so on.</p>
<h1 id="section-32">27/3/17</h1>
<p>Paired experiments can only be used in situations in observational experiments, because then the order in which the observations are taken would affect the result.</p>
<p>Suppose we want to determine if the distribution of the colour of Smarties is uniform. We buy hundreds of boxes of Smarties, and then count their colors. How do we determine how well the resulting data confirms/disconfirms the hypothesis that the colour distribution is uniform?</p>
<p>Clearly, the number each colour can be represented as <span class="math inline">\(Y_i \sim \mathrm{Binomial}(n, \theta_i)\)</span>. All together, they form a multinomial distribution <span class="math inline">\(Y_1, \ldots, Y_k \sim \mathrm{Multinomial}(n; \theta_1, \ldots, \theta_k)\)</span>.</p>
<p>So the likelihood function is <span class="math inline">\(L(\theta_1, \ldots, \theta_k; y_1, \ldots, y_k) = \theta_1^{y_1} \cdots \theta_k^{y_k}\)</span>, with all the constant factors removed. Note that <span class="math inline">\(0 \le \theta \le 1\)</span> and <span class="math inline">\(\sum \theta_i = 1\)</span>. If we take the maximum likelihood estimate, we get <span class="math inline">\(\theta_i = \frac{y_i}{n}\)</span>.</p>
<p>Our null hypothesis is <span class="math inline">\(\theta_1 = \ldots = \theta_k = \frac 1 n\)</span>, or <span class="math inline">\(\theta = \theta_0\)</span> where <span class="math inline">\(\theta_0 = \tup{\theta_1, \ldots, \theta_k}\)</span>.</p>
<p>To test how good this hypothesis is, we now need a test statistic. For this, we can use the likelihood ratio test statistic, <span class="math inline">\(A = -2 \log \frac{L(\theta_0)}{L(\widetilde \theta)} \sim \chi^2(k - 1)\)</span> (we have <span class="math inline">\(k\)</span> variables <span class="math inline">\(\theta_1, \ldots, \theta_k\)</span>, but since <span class="math inline">\(\sum \theta_i = 1\)</span>, there's actually only <span class="math inline">\(k - 1\)</span> unknown values). If we expand <span class="math inline">\(\frac{L(\theta_0)}{L(\widetilde \theta)}\)</span>, we get <span class="math inline">\(\left(\frac{E_i}{Y_i}\right)^{Y_i}\)</span>, where <span class="math inline">\(E_i = \frac{;wip: get this from slides}\)</span> is the expected frequency of successes for <span class="math inline">\(i\)</span>. So <span class="math inline">\(A = 2 \sum Y_i \ln \frac{Y_i}{E_i}\)</span>.</p>
<p>This test statistic makes sense because it has large values when the hypothesis isn't supported, and small values when it is. Using this test statistic, we have the pivotal quantity <span class="math inline">\(\lambda = 2 \sum y_i \ln \frac{Y_i}{E_i} \sim \chi^2(1)\)</span>. We can now generate a confidence interval or p-value from this.</p>
<p>Before the likelihood ratio statistic was around, there was also the Pearson Chi-squared Goodness of Fit Statistic, <span class="math inline">\(D = \sum \frac{(Y_i - E_i)^2}{E_i}\)</span>, where <span class="math inline">\(E_i = n \theta_i\)</span> - the expected frequencies.</p>
<h1 id="section-33">29/3/17</h1>
<p>Alpha particle example.</p>
<p>For an approximate p-value, the p-value is around <span class="math inline">\(P(W \ge \lambda)\)</span>, where <span class="math inline">\(W \sim \chi^2(k - 1 - p)\)</span>, where <span class="math inline">\(p\)</span> is the number of parameters being estimated assuming the null hypothesis is true.</p>
<p>For bivariate categorical data, we used relative risk to summarize this data.</p>
<h1 id="section-34">31/3/17</h1>
<p>Exam will include a 2-way table question.</p>
<p>The generalized two-way table has the model <span class="math inline">\(Y_{1, 1}, Y_{1, 2}, \ldots, Y_{a, b} \sim \mathrm{Multinomial}(n; \theta_{1, 1}, \theta_{1, 2}, \ldots, \theta_{a, b})\)</span></p>
<p>In the two-way table, the null hypothesis is usually that the variables are independent.</p>
<p>Each <span class="math inline">\(e_{i,j} = \frac{r_i c_i}{n}\)</span>, where <span class="math inline">\(r_i\)</span> is the row value and <span class="math inline">\(c_i\)</span> is the column value.</p>
<p>For a two-way table, the number of degrees of freedom is <span class="math inline">\(ab - 1 - (a - 1 + b - 1)\)</span>.</p>
<h2 id="causal-relationships">Causal Relationships</h2>
<p>What does it means to say that X causes Y? We usually mean that if we change X, we cause a change in Y, but how do we determine when this is the case - how do we know smoking causes lung cancer, versus lung cancer causing smoking? Does your final STAT230 grade cause your final STAT231 grade? In a lot of cases, causation is hard to define.</p>
<p>One definition of a causal relationship is: a variate <span class="math inline">\(x\)</span>, has a causal effect on a random variable <span class="math inline">\(Y\)</span> if and only if changing X gets a change in Y, all other factors being held constant. However, a change in X might only change the distribution of Y, and this should still be considered a causal relationship.</p>
<p>A better definition is to say that changing the variate <span class="math inline">\(x\)</span> should result in a change in some property of the random variable <span class="math inline">\(Y\)</span>, like <span class="math inline">\(E(Y)\)</span> or <span class="math inline">\(P(Y \ge c)\)</span>.</p>
<p>It's usually not possible to hold all other factors constant as per the definition, but we should design studies so that all of the alternative explanations we can think of for <span class="math inline">\(Y\)</span> changing are ruled out. Generally, we can only do this using experimental studies, so we can actually change the explanatory variate <span class="math inline">\(x\)</span>.</p>
<h1 id="section-35">3/4/17</h1>
<p>Actually, there are ways to detect causal relationships in observational studies. Usually we can't do this because there are multiple other reasons we could have gotten the relationship we saw:</p>
<ul>
<li>The explanatory variate causes the response variate (e.g., eating food causes lowering of hunger).</li>
<li>The response variate is causing changes in the explanatory variate (e.g., hotel occupancy rates increase advertising spend).</li>
<li>The explanatory variate is contributing but not the cause of the response variate (e.g., dietary choices affect but don't solely cause cancer).</li>
<li>Both variates are changing over time (e.g., number of pirates vs. global temperature increase).</li>
<li>The variates are coincidentally the way they are (e.g., thinking about a song and it suddenly coming on the radio).</li>
<li>There are confounding variates (e.g. people who eat healthy vs. people who exercise vs. risk of cancer - the effects of diet and exercise are inextricable from each other).</li>
<li>The variates might be changing due to another, common cause, a lurking variate (e.g., tobacco companies saying smoking and lung cancer are both caused by genetic predispositions).</li>
</ul>
<p>If we can do an observational study where we control all of these confounding and lurking variates, we can establish a causal relationship. However, it's often not ethical to do so. To correct for these factors, we can instead use randomization.</p>
<p>If we have a large enough group, we can assume that the levels of confounding variates are about the same between the groups. We need to ensure the following hold:</p>
<ul>
<li>The association between the variates needs to be visible in many different types of studies in many different groups (e.g., smoking and lung cancer are related in all countries and in second hand smoking groups as well).</li>
<li>The association must hold even when we account for plausible confounding variables (e.g., genetic factors causing smoking and lung cancer are ruled out).</li>
<li>There is a plausible scientific explanation for the causal relation - it must fit into our existing theoretical frameworks (e.g., carcinogens causing DNA mutations).</li>
<li>The association must be consistent - the variates must always have the same relationship in all situations (e.g., smoking consistently increases cancer rates).</li>
</ul>
<hr />
<p>My previous STAT231 notes follow:</p>
<hr />
<h1 id="section-36">2/5/16</h1>
<p>There are 3 tutorial tests, 2 midterms, and a final exam.</p>
<p>The Challenger space shuttle disaster occurred due to a failed O-ring. However, the right data analysis could have presented this tragedy - there was already enough data available to find this problem, but it wasn't looked at in the right way. Statistics can help prevent this sort of error.</p>
<p>Kansas weather reports, in a study, were found to be accurate about 85% of the time in predicting rain. However, if they just said &quot;it's not going to rain&quot;, they would be right 90% of the time. Likewise, ESPN pundits were found to correctly predict the results of games about 48% of the time. Statistics can help us evaluate which analysts and predictors are believable.</p>
<p>Statistics also allows us to evaluate relationships between two things. For example, does smoking cause lung cancer, or does lung cancer cause smoking? Are they related at all?</p>
<h2 id="statistical-data">Statistical data</h2>
<p>There are two main types of data:</p>
<ul>
<li><strong>Numeric data</strong>, like a grade or a width. Numerical data can be:
<ul>
<li><strong>Discrete</strong> - an element of a countably large set. For example, the number of pennies in a set of coins.</li>
<li><strong>Continuous</strong> - a measure, like height or weight.</li>
</ul></li>
<li><strong>Categorical data</strong> (area of study, name, etc.). Categorical data can be:
<ul>
<li><strong>Binary</strong> - falling into two categories.</li>
<li><strong>Ordinal</strong> - there are categories, but there is an underlying order to the data. For example, colors are categorical data, but they have an underlying order on the EM spectrum.</li>
</ul></li>
</ul>
<p>A <strong>transformation</strong> is a function over a variable. A <strong>linear transformation</strong> is one of the form <span class="math inline">\(y = mx + b\)</span>. A linear transformation is also known as an <strong>affine transformation</strong>.</p>
<p>A <strong>coding</strong> is a transformation that converts categorical data to numerical data. For example, colors can be assigned numbers, like 0 for red, 1 for orange, and so on.</p>
<h2 id="summaries">Summaries</h2>
<p>We often want to extract important information about a data set to find its properties. When we do so, we extract <strong>data summaries</strong>. We can do this <strong>numerically</strong> (like finding the mean, stddev, median, etc.) or <strong>graphically</strong> (like making a scatter plot). Numerical summaries tell us about certain fundamental properties of data sets, while graphical summaries tell us the shape of the data.</p>
<p>Suppose we have $100 in a bank account, with 4% interest the first year, 8% the second, and 12% the third. What is the average interest rate?</p>
<blockquote>
<p>Although it would seem at first glance to be 8%, the arithmetic mean doesn't represent the actual average here. The average interest rate is the single rate such that, after those 3 years, we would have the same interest gains as we did with this bank account.<br />
Let's denote this rate as <span class="math inline">\(x\)</span>. Then according to the interest formula, <span class="math inline">\(100(1 + x)^3 = 100 \cdot 1.04 \cdot 1.08 \cdot 1.12\)</span>.<br />
So <span class="math inline">\((1 + x)^3 = 1.257984\)</span> and <span class="math inline">\(x = 0.0795059\)</span>, or about 0.795%.<br />
As it turns out, this is basically the same thing as the geometric mean of the interest rates. In this case, we use the geometric mean since the interest is defined in terms of an enponential function of <span class="math inline">\(n\)</span>.</p>
</blockquote>
<p>A car drives from A to B at 40 km/h, and immediately drives back at 60 km/h. What is the average speed of the car?</p>
<blockquote>
<p>Although it would seem at first glance to be 50 km/h, this is not the case, since the car takes less time driving back since it's going faster.<br />
Let's denote the distance between A and B as <span class="math inline">\(x\)</span>, in km. Then the trip from A to B took <span class="math inline">\(\frac{x}{40}\)</span> hours, and the trip back took <span class="math inline">\(\frac{x}{60}\)</span> hours.<br />
Since the total time taken is <span class="math inline">\(\frac{x}{40} + \frac{x}{60}\)</span>, the average speed is <span class="math inline">\(\frac{40\frac{x}{40} + 60\frac{x}{60}}{\frac{x}{40} + \frac{x}{60}} = \frac{2}{\frac{1}{40} + \frac{1}{60}}\)</span>, or 48 km/h.<br />
As it turns out, this is the same thing as the harmonic mean of the speeds.</p>
</blockquote>
<h1 id="section-37">4/5/16</h1>
<p>When we talk about distributions, we oftne picture them as bumps or hills - a high area in the middle, and low areas on the sides. In this picture, the <strong>tails</strong> of a distribution are the low areas on the sides. The concept of tails doesn't really generalize that well, so we'll avoid it for actual formal explanations.</p>
<h3 id="centrality">Centrality</h3>
<p>To find a percentile <span class="math inline">\(\beta\)</span>, we sort the observations from low to high, and then take the value at the 1-indexed position <span class="math inline">\(\beta(n + 1)\)</span> (or the sample mean of the two values closest to the 1-indexed position <span class="math inline">\(\beta(n + 1)\)</span>).</p>
<p>The average/central tendency isn't always the only thing we care about. Suppose you flip a coin <span class="math inline">\(n\)</span> times before a heads appears. If you are paid <span class="math inline">\(2^n\)</span> dollars for doing so, at what price would you pay to play this game?</p>
<blockquote>
<p>Note that a head comes up on the first trial with 1/2 probability, paying out 2 dollars, on the second trial with 1/4 probability, paying out 4 dollars, and so on.<br />
So the expected value is <span class="math inline">\(\sum_{i = 1}^\infty i \frac 1 i = \infty\)</span> - the expected value is infinity!<br />
However, most people wouldn't even pay 40 dollars to play - it seems like risk/variability is also an important factor in deciding whether to play. This is called St. Peter's Paradox.</p>
</blockquote>
<p>Suppose we have a dataset <span class="math inline">\(x = \set{x_1, \ldots, x_n}\)</span>. Suppose we know one observation with value <span class="math inline">\(k\)</span> is unreliable, and we want to discard it from <span class="math inline">\(x\)</span>. Can we find the new mean and variance?</p>
<blockquote>
<p>Let <span class="math inline">\(y = \set{y_1, \ldots, y_{n - 1}}\)</span> be the dataset with the unreliable observation removed.<br />
Clearly, <span class="math inline">\(\sum y_i = \sum x_i - k\)</span>, since we removed only that one element, and since <span class="math inline">\(\sum x_i = n \overline x\)</span>, <span class="math inline">\(\sum y_i = \sum x_i - k\)</span>.<br />
So the sample mean is <span class="math inline">\(\overline y = \frac{\sum x_i - k}{n - 1}\)</span>.<br />
To find the variance, we can use the alternate form of the variance formula, so <span class="math inline">\(s_y^2 = \frac 1 {(n - 1) - 1} \left(\sum y_i^2 - (n - 1)(\overline y)^2\right)\)</span>.<br />
Since <span class="math inline">\(\sum y_i = \sum x_i - k\)</span>, <span class="math inline">\(\sum y_i^2 = \sum x_i^2 - k^2\)</span>.<br />
Since we already know <span class="math inline">\(\overline y\)</span>, <span class="math inline">\(s_y^2 = \frac{1}{n - 2} \left(\sum x_i^2 - k^2\right) - \frac{\left(\sum x_i - k\right)^2}{n - 1}\)</span>.<br />
This same technique can be used to calculate the mean and variance of a dataset after adding or removing any number of items to it, without looking at the existing elements of the dataset.</p>
</blockquote>
<p>When you have a dataset with sample mean <span class="math inline">\(\overline x\)</span> and remove an item with value <span class="math inline">\(\overline x\)</span>, the mean stays the same, while the variance increases or stays the same.</p>
<p>The batting champion of a baseball season is the player with the highest batting average (probability of hitting the ball in each attempt, times 1000). However, although there have been 13 people with a batting average above 400 before 1941, there have been none after that year. Why does this happen?</p>
<p>In every sport, players have gotten better, absolutely speaking, over time - it doesn't seem like batters are actually getting worse. However, better pitchers, fielders, and managers can make a batter's job a lot harder.</p>
<p>We can actually test this by looking at the batting average for individual average batters over their careers. As it turns out, the average batter's batting average stays about the same over their career. However, the variance in the average player's ball hitting rate shrinks over their careers. Since the average is lower than 400, the shrinking variances mean that the tails in the batting average distribution shrink as well (causing fewer 400+ batting averages). When we become more consistent at hitting the ball at the average rate, that means there are fewer low-performers, but also fewer high-performers.</p>
<h1 id="section-38">9/5/16</h1>
<h3 id="association">Association</h3>
<p>Suppose we have a dataset <span class="math inline">\(D = \set{\tup{x_1, y_1}, \ldots, \tup{x_n, y_n}}\)</span> with two variables. How do we measure how much these two variables are associated (how is one variable affected by the other)?</p>
<p>For example, suppose we have categorical variables, <span class="math inline">\(x = \set{\text{smoker}, \text{non-smoker}}\)</span> and <span class="math inline">\(y = \set{\text{lung cancer}, \text{no lung cancer}}\)</span>. How do we determine the relationship between these variables, given a dataset of samples?</p>
<p><strong>Relative risk</strong> is a measure of association between two categorical variables. Basically, relative risk is <span class="math inline">\(\frac{P(A \mid B)}{P(A \mid \neg B)}\)</span> - the probability of <span class="math inline">\(A\)</span> occurring given that <span class="math inline">\(B\)</span> occurrs over the probability of <span class="math inline">\(A\)</span> occurring given that <span class="math inline">\(B\)</span> does not occur. For independent variables, relative risk is 1. The farther the relative risk is from 1, the more strongly it implies that the variables are associated.</p>
<p>For two categorical Boolean variables, relative risk is <span class="math inline">\(\frac{\frac{\abs{x \wedge y}}{\abs{x}}}{\frac{\abs{\neg x \wedge y}}{\abs{\neg x}}}\)</span>. Expanded into a more useful form, it becomes <span class="math inline">\(\frac{\frac{\abs{x \wedge y}}{\abs{x \wedge y} + \abs{x \wedge \neg y}}}{\frac{\abs{\neg x \wedge y}}{\abs{\neg x \wedge y} + \abs{\neg x \wedge \neg y}}}\)</span>.</p>
<p>The sign of <span class="math inline">\(r_{xy}\)</span> (specifically, the sign of the numerator) tells us the direction of the association (positive means variables tend to increase each other, while negative means one tends to increase when the other decreases, and vice versa), and the magnitude of <span class="math inline">\(r\)</span> tells us the strength of the association (0 meaning no association). The denominator ensures that <span class="math inline">\(-1 \le r_{x, y} \le 1\)</span> for any dataset.</p>
<p>For non-linear relationships, the sample correlation coefficient doesn't work so well. For example, for two variables <span class="math inline">\(x, y\)</span> associated by <span class="math inline">\(y = x^2\)</span>, the sample correlation coefficient is 0, even though we defined them to be associated by a particular function. A general measure of association for any function (not just linear ones) wouldn't really be meaningful since it's always possible to construct a function to fit a finite dataset. Therefore, all measures of association must be against a particularly chosen class of functions.</p>
<p>For a perfect linear relationship between variables <span class="math inline">\(x, y\)</span>, like <span class="math inline">\(y = ax + b\)</span>, <span class="math inline">\(r_{xy} = 1\)</span> if <span class="math inline">\(b &gt; 0\)</span>, and <span class="math inline">\(r_{xy} = -1\)</span> if <span class="math inline">\(b &lt; 0\)</span>. If <span class="math inline">\(b = 0\)</span>, then <span class="math inline">\(r_{xy} = 0\)</span>.</p>
<p>Since we're assuming our dataset is a sample of the population rather than the population itself, we can only find evidence of associations rather than actual associations themselves. Even if the evidence is very strong, it doesn't say for sure that the association is there. Basically, correlation doesn't imply causation.</p>
<h1 id="section-39">11/5/16</h1>
<p>The reason we have summaries is to figure out the shape of a dataset, and see if we can identify it as following a certain distribution. obtaining the distribution allows us to make predictions about future observations and other useful statistical things.</p>
<p>The <strong>five number summary</strong> of a dataset is a common set of summaries: minimum, first quartile, median, third quartile, and, maximum. Basically, it gives 5 equally spaced points on the histogram.</p>
<p>Since <span class="math inline">\(\sum (x_i - \overline x)(y_i - \overline y) = \sum x_i y_i - n \overline x \overline y\)</span> and <span class="math inline">\(\sum (x_i - \overline x)\)</span>, we can write the formula for the correlation coefficient more simply as <span class="math inline">\(\frac{\sum x_i y_i - n \overline x \overline y}{s_x s_y}\)</span>.</p>
<h3 id="graphical-summaries-1">Graphical Summaries</h3>
<p>One of the most commonly used graphical summaries is the histogram. This is generally used when we have data that can be grouped/binned (organized into disjoint, ordered sets). By &quot;histogram&quot;, most people actually mean a <strong>frequency histogram</strong> - a bar plot where the X axis is the bins, and the Y axis is the frequency of observations in each bin.</p>
<p>In contrast, statisticians usually mean <strong>density histograms</strong> when talking about histograms. This is the same thing, except the Y axis is the density of the bin.</p>
<p>The bins in a density histogram don't all have to all have the same range - one can encompass 30 elements, while another might encompass 10. On the plot, the bars don't all have to have the same width. The height of each bar is chosen such that the area of the bar (width of the bin times the height) is equal to the relative frequency of observations that fall into its corresponding bin. The relative frequency is simply <span class="math inline">\(\frac{\text{number of observations in the bin}}{\text{total number of observations}}\)</span>.</p>
<p>Note that the total area underneath the density histogram is always 1. Since the total area under a probability density function is also 1, the density histogram is very useful for comparing data with known probability density functions, which we do very often when trying to figure out if a dataset follows a certain distribution. We can't use the usual frequency histogram for this because the total area of the bars doesn't add up to 1.</p>
<p>The <strong>empirical cumulative distribution function</strong> (empirical CDF) for discrete numerical data is a plot where the X axis is the range of possible observation values, and the Y axis is the number of observations that are less than or equal to that value. So for a dataset <span class="math inline">\(x = \set{x_1, \ldots, x_n}\)</span>, and <span class="math inline">\(F(x_i) = \abs{\set{v \in x \middle| v \le x_i}}\)</span>, the empirical CDF is a plot of <span class="math inline">\((x_i, F(x_i))\)</span>.</p>
<p>The advantage of the empirical CDF is that the percentile values are directly shown on the plot - to find the <span class="math inline">\(n\)</span>th percentile, we find the X axis value such that the Y axis is <span class="math inline">\(n\)</span> percent of the maximum Y axis value. For example, to find the median, we would find the Y axis . Also, the mode is the bar that has the largest increase in height compared to the bar on its left.</p>
<p>The Q-Q plot is used for checking if a dataset resembles a normal distribution. This is a plot of the <span class="math inline">\(\alpha\)</span>th quantile of the dataset and the <span class="math inline">\(\alpha\)</span>th quantile of the Z distribution <span class="math inline">\(N(0, 1)\)</span> (normal distribution with mean 0 and variance 0), for all <span class="math inline">\(\alpha\)</span>.</p>
<p>So the median of the dataset is plotted at the X axis value that is the median of the Z distribution, the 95th percentile of the dataset is plotted at the X axis value that is the 95th percentile of the Z distribution, and so on. Basically, this is a plot of <span class="math inline">\((p \text{th percentile of the Z distribution}, p \text{th percentile of the dataset})\)</span> for all <span class="math inline">\(0 \le p \le 100\)</span>. The X axis goes on infinitely in both directions.</p>
<p>If the Q-Q plot resembles a straight line, the dataset resembles a normal distribution. For example, for a normally distributed dataset, it would always be the case that the <span class="math inline">\(p\)</span>th percentile of the dataset be a linear function of the <span class="math inline">\(p\)</span>th percentile of the Z distribution, so all <span class="math inline">\((p \text{th percentile of the Z distribution}, p \text{th percentile of the dataset})\)</span> would lie along the same line. For a uniform dataset, we would have a sigmoid-like shape, because the uniform distribution has no tails. For an exponential distribution, we would have a U-shaped upward curve, because the exponential distribution has a thick right tail. A distribution with a thick left tail would have a Q-Q plot that looks like a U-shaped downward curve.</p>
<p>A <strong>scatter plot</strong> plots two variables against each other, where the X axis is one variable and the Y axis is the other, and points are plotted for each observation. The scatter plot is great for finding patterns in the data, like correlations, grouping, and so on.</p>
<h1 id="section-40">16/5/16</h1>
<p>Quiz about numerical and graphical summaries, I'm writing in DC1351 on Thursday at 3:30PM, for just under 1 hour.</p>
<h2 id="statistical-analysis">Statistical Analysis</h2>
<p>A common statistical problem is &quot;Given a population of observations, some of the characteristics of which are unknown, and a sample taken from that population, what can we say about the population by looking just at the sample?&quot;.</p>
<p>The PPDAC approach is a sort of template for solving this type of problem: problem, plan, daa, analysis, and conclusion. It's specific to statistics at Waterloo.</p>
<ul>
<li>The <strong>problem</strong> can be descriptive (what properties of the population are we interested in?), causative (does X cause Y? how are they related?), and predictive (what will be the result of doing X?).
<ul>
<li>We should also identify and explicitly state the population of interest. A <strong>unit</strong> is a member of the target population, and a <strong>variate</strong> is a characteristic of a unit. An <strong>attribute</strong> is a function over the variates of a collection of units.</li>
<li>For example, for finding the presidential approval rating, the target population is the voting population, a unit is any particular voter, a variate is whether they approve of the president, and the proportion of approvals is the attribute.</li>
</ul></li>
<li>The <strong>plan</strong> can be experimental (variables are chosen and controlled when making observations) or observational (variables can't be controlled when making observations).
<ul>
<li>We first choose a study population from which we draw the sample.</li>
<li>For the presidental approval rating example, we might choose the subset of voters that have a phone (so we can call them to ask whether they approve).</li>
<li>For a new drug being tested, the study population might be a collection of mice, while the target population is all humans - the study population doesn't have to be a subset of the target population.</li>
</ul></li>
<li>The <strong>analysis</strong> involves setting up a statistical model.
<ul>
<li>That means we assume (after obtaining sufficient evidence to back it up) that the data follows some known distribution, possibly with unknown parameters.</li>
<li>For the presidental approval rating example, since we can probably assume that the voters are independently making decisions, and that approval is a binary result, we can probably use a binomial distribution.</li>
<li>A <strong>bias</strong> is a systematic error in the data. Bias is often caused by measurement error, or selection bias (for example, people only ask for exam remarks if they got less than they feel they deserved, so marking errors might be systematically higher than the true value).</li>
<li>When distribution parameters are unknown, we often use Greek letters like <span class="math inline">\(\mu, \sigma, \pi\)</span>. When we have sample parameters, which we do know, we often use English letters like <span class="math inline">\(\overline y, s^2, p\)</span>, or Greek letters with hats, like <span class="math inline">\(\hat, \mu, \hat \sigma, \hat pi\)</span>.</li>
<li>The <strong>study error</strong> is the difference between the target population mean and the study population mean for the attribute under study: <span class="math inline">\(\mu_1 - \mu_2\)</span>.</li>
<li>The <strong>sampling error</strong> is the difference between the sample mean and the study population mean for the attribute under study: <span class="math inline">\(\overline y - \mu_2\)</span>.</li>
<li>When we're picking a sample, we really want it to represent the population - the properties of the population should be likely to be similar to the properties of the population, to reduce the sampling error as much as possible. We do this by doing <strong>random sampling</strong>. Correctly doing random sampling is critical in avoiding biases, but often depends on characteristics of the problem.</li>
</ul></li>
<li>The <strong>conclusion</strong> is a statement that should be understandable by non-experts.</li>
</ul>
<h3 id="estimation">Estimation</h3>
<p>The <strong>method of maximum likelihood</strong> is finding th most likely value of an unknown parameter of a statistical model, based on a sample. For example:</p>
<blockquote>
<p>We have a coin that either has a 25% chance of getting heads, or a 75% chance of getting heads.<br />
Suppose that we flipped the coin 100 times and got 30 heads. Obviously, we would say that it's most likely that the coin has a 25% chance of getting heads.<br />
Mentally, it seemed like we asked &quot;what is the probability of 30 heads occuring, if it were the case that the coin had a 25% chance of getting heads?&quot; and &quot;what would that probability be if it were the case that the coin had a 75% chance of getting heads instead?&quot;, and then chose the option that maximizes the probability of the event occuring.</p>
</blockquote>
<p>Essentially, the method of maximum likelihood is picking the parameter that makes our observed sample properties most likely.</p>
<p>Formally, given a discrete distribution <span class="math inline">\(Y\)</span> with probability function <span class="math inline">\(f\)</span> and unknown parameter <span class="math inline">\(\theta\)</span>, then the <strong>likelihood function</strong> is defined as <span class="math inline">\(L(\theta; y_1, \ldots, y_n) = P(Y_1 = y_1, \ldots, Y_n = y_n) \text{ for the given value of } \theta\)</span>.</p>
<p>We then find the value of <span class="math inline">\(\theta\)</span> that maximizes the likelihood function. This value is the <strong>maximum likelihood estimate</strong> (MLE), denoted <span class="math inline">\(\hat \theta\)</span>.</p>
<p>Suppose we have a variable <span class="math inline">\(X \sim \mathrm{Poisson}(\mu)\)</span>. What is the MLE of <span class="math inline">\(\mu\)</span>?</p>
<blockquote>
<p>Clearly, the probability of each sample observation is <span class="math inline">\(\frac{e^{-\mu} \mu^r}{r!}\)</span> where <span class="math inline">\(r\)</span> is the value of the observation, by the definition of the Poisson distribution.<br />
Therefore, <span class="math inline">\(P(Y_1 = y_1, \ldots, Y_n = y_n) = \frac{e^{-\mu} \mu^{x_1}}{x_1!} \cdots \frac{e^{-\mu} \mu^{x_n}}{x_n!}\)</span> (we can just multiply the probabilities together because all of the observations are independent, which we know is the case since we're using the Poisson distribution).<br />
We can find the value of <span class="math inline">\(\mu\)</span> that maximizes the likelihood function by taking the logarithm of the whole thing, then finding where the derivative of that is 0. This is a pretty common way to find the max.<br />
This value of <span class="math inline">\(\mu\)</span> is the maximum likelihood estimate.</p>
</blockquote>
<h1 id="section-41">18/5/16</h1>
<p>The midterm covers everything up to next week.</p>
<p>Most statistical inference problems start with parameter estimation. We generally start with a guess, and then try to refine it using samples.</p>
<p>A sample <span class="math inline">\(y\)</span> can be thought of as a a set of outcomes of a random variable <span class="math inline">\(Y\)</span>, rather than just a set of numbers. This random variable is the <strong>statistical model</strong>.</p>
<p>To review, lowercase Greek letters like <span class="math inline">\(\mu, \theta\)</span> are unknown parameters, lowercase Latin letters like <span class="math inline">\(x, y\)</span> are known/sample parameters, and uppercase Latin letters like <span class="math inline">\(X, Y\)</span> are random variables. These letters correspond: <span class="math inline">\(y_i\)</span> is a single outcome drawn from the random variable <span class="math inline">\(Y_i\)</span>.</p>
<p>Likewise, <span class="math inline">\(\overline y\)</span>, the sample mean can be thought of as a single outcome of a random variable <span class="math inline">\(\overline Y\)</span>. As a result, with multiple samples, we can get a whole distribution of different <span class="math inline">\(\overline y\)</span> values. The unknown population mean is <span class="math inline">\(\mu\)</span>, while <span class="math inline">\(\overline Y\)</span> is the random variable with the distribution of all the different <span class="math inline">\(\overline y\)</span> values.</p>
<p>These random variables, like <span class="math inline">\(\overline Y, S^2, \widetilde \pi\)</span> are <strong>estimators</strong>. These sample values, like <span class="math inline">\(\overline y, s^2, \hat \pi\)</span> are <strong>estimates</strong>.</p>
<p>Estimate the approval rating of the president:</p>
<blockquote>
<p>The approval rating is <span class="math inline">\(\pi\)</span>, a population parameter that we don't know. Suppose we interview 10 people and get <span class="math inline">\(\set{A, A, D, D, D, D, A, D, D, D}\)</span>, where <span class="math inline">\(A\)</span> means approval and <span class="math inline">\(D\)</span> means disapproval.<br />
We want to find <span class="math inline">\(\hat \pi\)</span>, the MLE of <span class="math inline">\(\pi\)</span>. Clearly, <span class="math inline">\(L(\pi; y_1, \ldots, y_n) = \pi \pi (1 - \pi) (1 - \pi) (1 - \pi) (1 - \pi) \pi (1 - \pi) (1 - \pi) (1 - \pi)\)</span>, because the probability of each person approving is just <span class="math inline">\(\pi\)</span>.<br />
So <span class="math inline">\(L(\pi; y_1, \ldots, y_n) = \pi^3 (1 - \pi)^7\)</span>, and the log likelihood function is <span class="math inline">\(l(\pi) = \ln(\pi^3 (1 - \pi)^7) = 3 \ln \pi + 7 \ln(1 - \pi)\)</span>.<br />
Clearly, the value of <span class="math inline">\(\pi\)</span> that maximizes <span class="math inline">\(l(\pi)\)</span> is <span class="math inline">\(\frac{3}{10}\)</span> (by taking the derivative of the log likelihood function). Therefore, the MLE for the approval rating is <span class="math inline">\(\frac{3}{10}\)</span>.</p>
</blockquote>
<p>Estimate the average website hits per hour, given a sample of visits over <span class="math inline">\(n\)</span> hours <span class="math inline">\(\set{y_1, \ldots, y_n}\)</span>:</p>
<blockquote>
<p>Assume that the data is from a Poisson distribution. Then we want to figure out the distribution parameter <span class="math inline">\(\mu\)</span>.<br />
Construct the likelihood function <span class="math inline">\(L(\mu; y_1, \ldots, y_n) = \frac{e^{-\mu} \mu^{y_1}}{y_1!} \cdot \ldots \cdot \frac{e^{-\mu} \mu^{y_n}}{y_n!}\)</span>.<br />
So <span class="math inline">\(L(\mu; y_1, \ldots, y_n) = \frac{e^{-n\mu} \mu^{\sum y_i}}{y_1! \ldots y_n!}\)</span>.<br />
So the log likelihood function is <span class="math inline">\(l(\mu) = -n \mu + \sum (y_i \ln \mu) - \ln(y_1! \ldots y_n!)\)</span>.<br />
Differentiating the log likelihood function with respect to <span class="math inline">\(\mu\)</span>, we get <span class="math inline">\(-n + \sum \frac{y_i}{\mu}\)</span>.<br />
By setting the derivative to 0 and solving for <span class="math inline">\(\mu\)</span>, we find the maximum value of the log likelihood function at <span class="math inline">\(\hat \mu = \frac 1 n \sum y_i = \overline y\)</span>.<br />
So if we have a Poisson distribution, the best guess for the parameter <span class="math inline">\(\mu\)</span> is <span class="math inline">\(\overline y\)</span>.</p>
</blockquote>
<p>Estimate the probability that a Canadian contestant wins Jeopardy, given a sample of shows Canadian contestants appeared in <span class="math inline">\(\set{y_1, \ldots, y_n}\)</span>:</p>
<blockquote>
<p>Clearly, if a contestant appears in <span class="math inline">\(n\)</span> shows, then they won <span class="math inline">\(n\)</span> in a row and lost in the <span class="math inline">\(n + 1\)</span>th show.<br />
We assume that shows are independent. Therefore, we have a geometric distribution, <span class="math inline">\(Y \sim \mathrm{Geo}(\pi)\)</span>.<br />
Recall that for a geometric distribution, <span class="math inline">\(P(Y = y) = \pi^{y - 1}(1 - \pi)\)</span>.<br />
So <span class="math inline">\(L(\pi; y_1, \ldots, y_n) = \pi^{y_1 - 1}(1 - \pi) \ldots \pi^{y_n - 1}(1 - \pi) = \pi^{\sum y_i - n} (1 - \pi)^n\)</span>.<br />
So <span class="math inline">\(l(\pi) = (\sum y_i - n) \ln x + n \ln (1 - x)\)</span>. Using the usual methods, we find the value of <span class="math inline">\(\pi\)</span> that maximizes <span class="math inline">\(l(\pi)\)</span> to get the MLE of <span class="math inline">\(\pi\)</span>.</p>
</blockquote>
<p>Estimate the proportion of left handers in this university:</p>
<blockquote>
<p>Strategy 1: Suppose we got our data by asking people from the population until we got a 10 left handers, and we needed to ask 100 before before there were 10.<br />
Strategy 2: Suppose we got our data by asking 100 people from the population, and 10 of them turned out to be left handers.<br />
Let <span class="math inline">\(\pi\)</span> be the proportion of left handers. In strategy 1, we have a negative binomial distribution.<br />
So in strategy 1, we asked 99 people and got either left or right handed, and the 100th person answered left handed. So the likelihood function is <span class="math inline">\(L(\pi) = {99 \choose 9} \pi^9 (1 - \pi)^{90} \times \pi\)</span>.<br />
So in strategy 2, we asked all 100 people, and got either left or right handed. So the likelihood function is <span class="math inline">\(L(\pi) = {100 \choose 10} \pi^{10} (1 - \pi)^{90}\)</span>.<br />
As it turns out, both strategies give the same MLE for <span class="math inline">\(\pi\)</span>, <span class="math inline">\(\frac{1}{10}\)</span>.</p>
</blockquote>
<p>Note that when we're maximising value of a parameter by taking the derivative, setting it to 0, and solving for the parameter, we have to make sure that the solution is actually the global maximum, not a local maximum or even a minimum. When there are multiple solutions, we must check each of them to find the one that results in the largest likelihood value.</p>
<p>Note that in this course, there will only ever be one solution to the derivative being equal to 0, so we won't have to worry about it.</p>
<p>When we have lots of independent observations, the likelihood function will look like <span class="math inline">\(P(Y_1 = y_1) \cdot \ldots \cdot P(Y_n = y_n)\)</span>. So if <span class="math inline">\(n\)</span> is large, we're multiplying a lot of numbers between 0 and 1, which means that the actual likelihood is going to be very small. To deal with this, we sometimes use the <strong>relative likelihood function</strong> <span class="math inline">\(R(\theta) = \frac{L(\theta)}{L(\hat \theta)}\)</span>, which always has a maximum of 1 at <span class="math inline">\(\hat \theta\)</span>.</p>
<p>Note that this only works for discrete distributions. For continuous ones, we have other tools.</p>
<h1 id="section-42">30/5/16</h1>
<p>While the likelihood interval results in a likelihood interval, the sampling distribution gives us a confidence interval.</p>
<p>The theory of estimation deals with problems of the form &quot;estimate an unknown population attribute <span class="math inline">\(\theta\)</span> given a sample drawn from the population, <span class="math inline">\(\set{y_1, \ldots, y_n}\)</span> to get the estimate, <span class="math inline">\(\hat \theta(y_1, \ldots, y_n)\)</span>&quot;. So far, we've looked at the maximum likelihood estimation technique for solving these for discrete distributions.</p>
<p>Another one is the <strong>method of least squares</strong>, where we find the value of <span class="math inline">\(\theta\)</span> that minimises some squared error function we define - minimizing <span class="math inline">\(\sum E(y_i, \theta)^2\)</span>. If <span class="math inline">\(\theta\)</span> is <span class="math inline">\(\mu\)</span> in a normal distribution, then this works out to just be the sample mean, since the error is <span class="math inline">\(y_i - \mu\)</span> and we're minizing (y_i - )^2$. This is not the best method, but it's very popular becuase it's so simple.</p>
<p>So far, when we're estimating we've been trying to get a single value as the estimate. However, it's often more useful to get an interval estimate instead - to find an interval <span class="math inline">\([A, B]\)</span> such that <span class="math inline">\(\theta\)</span> has at least a given probability of being within this interval. This type of interval problem often shows up as determining margins of error.</p>
<p>One way to obtain an interval estimate is via a relative likelihood function. Unlike the MLE method, where we find the most likely value, we're finding all values of a relative likelihood that exceed a certain probability threshold.</p>
<p>Basically, a <span class="math inline">\(p\)</span>-percent likelihood interval for an unknown parameter <span class="math inline">\(\theta\)</span> is <span class="math inline">\(\set{\theta \middle| R(\theta) \ge p}\)</span>, where <span class="math inline">\(R(\theta) = \frac{L(\theta)}{L(\hat \theta)}\)</span> is the relative likelihood function and <span class="math inline">\(\hat \theta\)</span> is the MLE. The lowest and highest values in the resulting set is then the likelihood interval. The likelihood interval itself, though, is a set of all values of <span class="math inline">\(\theta\)</span> that satisfy the criteria.</p>
<p>All <span class="math inline">\(p\)</span>-percent likelihood intervals are subsets of <span class="math inline">\(q\)</span>-percent intervals if <span class="math inline">\(q \le p\)</span> - smaller percentage likelihood intervals are subsets of higher percentage likelihood itnervals. The MLE belongs to all likelihood intervals. The higher the value of <span class="math inline">\(p\)</span>, the higher the probability that the true value doesn't fall into the interval. Likelihood intervals are not easy to interpret and don't have an intuitive meaning, so we generally want to use something else.</p>
<p>The <strong>method of sampling distributions</strong> is a better way to estimate intervals. An <span class="math inline">\(n\)</span>-percent confidence interval means that we're <span class="math inline">\(n\)</span> percent sure that the value falls withint this interval, which is a lot more intuitive. Given a sample <span class="math inline">\(\set{y_1, \ldots, y_n}\)</span> and a confidence threshold <span class="math inline">\(p\)</span>:</p>
<ol style="list-style-type: decimal">
<li>Identify the pivotal distribution, from the model.</li>
<li>Find the endpoints of the pivotal distribution that exceed the confidence threshold <span class="math inline">\(p\)</span>.</li>
<li>Rearrange the endpoints to construct a coverage interval.</li>
<li>Estimate the coverage interval using the sample to get a confidence interval.</li>
</ol>
<p>;wip: for a normal dist the coverage interval is <span class="math inline">\((\overline Y - Z^* \frac{\sigma}{\sqrt{n}}, \overline Y + Z^* \frac{\sigma}{\sqrt{n}})\)</span>, and the confidence interval is <span class="math inline">\((\overline y - Z^* \frac{\sigma}{\sqrt{n}}, \overline y + Z^* \frac{\sigma}{\sqrt{n}})\)</span></p>
<p>The confidence interval tells us the probability that the true value falls within a certain range. However, we can also use them to say &quot;the true value must be at most <span class="math inline">\(A\)</span> from our estimate&quot; by choosing our sample size <span class="math inline">\(n\)</span> such that <span class="math inline">\(A = \frac{Z^* \sigma}{\sqrt{n}}\)</span>.</p>
<p>The problem with confidence intervals is if we don't know <span class="math inline">\(\sigma\)</span> or if the population isn't normal.</p>
<p>From the central limit theorem, if <span class="math inline">\(n\)</span> is large, and <span class="math inline">\(Y_1, \ldots, Y_n\)</span> are arbitrary distributions with mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2\)</span>, then <span class="math inline">\(\overline Y \sim N(\mu, \frac{\sigma^2}{n})\)</span>.</p>
<h1 id="section-43">1/6/16</h1>
<p>;wip: missed first half due to interviews</p>
<p>Continuing with multiple examples of choosing sample sizes such that we have a certain margin of error - making the confidence interval at least a certain size. Interestingly, to get a margin of error of 3% in any population, all we need to do is ask around 1000 people, regardless of how large the population is.</p>
<h1 id="section-44">5/6/16</h1>
<p>I dropped this course due to my injuries, as it became infeasible to continue going to classes.</p>
<div class="license">
  <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-nc-sa/4.0/80x15.png" /></a> This work by <a xmlns:cc="http://creativecommons.org/ns#" href="https://uberi.github.io/" property="cc:attributionName" rel="cc:attributionURL">Anthony Zhang</a> is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>.
  Copyright 2013-2017 Anthony Zhang.
</div>
<script type="text/javascript">
MathJax.Hub.Config({
  jax: ["input/TeX","output/HTML-CSS"],
  extensions: ["tex2jax.js","MathMenu.js","MathZoom.js"],
  TeX: {
    extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"]
  }
});
</script>
</body>
</html>
